{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "467e1ba1890047d39af21b4c92f4da6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44e101b887f24fc0988dfec3d2f776e2",
              "IPY_MODEL_bd89473f5b1d4f5f81cac947f4aaab46",
              "IPY_MODEL_97ca5224e4eb4c0a98813be18bdf53aa"
            ],
            "layout": "IPY_MODEL_70ad825310d5493fbce673d29569547a"
          }
        },
        "44e101b887f24fc0988dfec3d2f776e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ec84bc9a9945a3b4d09237e0217eb1",
            "placeholder": "​",
            "style": "IPY_MODEL_2c03622b1da24b1e9f4b84e74b88ed99",
            "value": ""
          }
        },
        "bd89473f5b1d4f5f81cac947f4aaab46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea6866704f794922b148cb14bc2ae59a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb22ec64d4044652925e4a055e75fed3",
            "value": 0
          }
        },
        "97ca5224e4eb4c0a98813be18bdf53aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85074396305e47639eea7809e3300578",
            "placeholder": "​",
            "style": "IPY_MODEL_07668326f05647188745375222b7caa8",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "70ad825310d5493fbce673d29569547a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ec84bc9a9945a3b4d09237e0217eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c03622b1da24b1e9f4b84e74b88ed99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6866704f794922b148cb14bc2ae59a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "eb22ec64d4044652925e4a055e75fed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85074396305e47639eea7809e3300578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07668326f05647188745375222b7caa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac5a2206d6ff482aac5980eb7ee8e3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86b0df5bff6b41d5a31f345b8b682a65",
              "IPY_MODEL_3af8d59818ee4810a0ad0aeeb3c0aeb5",
              "IPY_MODEL_bc9ebc3db81443728db92cffa5610138"
            ],
            "layout": "IPY_MODEL_88895776595c46d29dba4aa8207b332d"
          }
        },
        "86b0df5bff6b41d5a31f345b8b682a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_593291182278446e8adfb96a0b8851cc",
            "placeholder": "​",
            "style": "IPY_MODEL_8fed8e0e6e2d4b93ab613b1ee2705dcd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3af8d59818ee4810a0ad0aeeb3c0aeb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919ee68707ed4ff0bde722fbbe19b25e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b40f211a4fe04b0fbbed94890f638c73",
            "value": 3
          }
        },
        "bc9ebc3db81443728db92cffa5610138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1646260a7884a25a81850fe5acca24b",
            "placeholder": "​",
            "style": "IPY_MODEL_817cc1bef2ae44b094da17dd905f30d7",
            "value": " 3/3 [00:07&lt;00:00,  2.20s/it]"
          }
        },
        "88895776595c46d29dba4aa8207b332d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593291182278446e8adfb96a0b8851cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fed8e0e6e2d4b93ab613b1ee2705dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "919ee68707ed4ff0bde722fbbe19b25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b40f211a4fe04b0fbbed94890f638c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1646260a7884a25a81850fe5acca24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817cc1bef2ae44b094da17dd905f30d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1311d7765b3b4e35b13a3060ff3e0899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28a3539d510243a8a235dd0c7cbeea9a",
              "IPY_MODEL_f83bd28976cb4446b7329aa9c073778e",
              "IPY_MODEL_2866579d69a9419fa77a46bf86bcbfdf"
            ],
            "layout": "IPY_MODEL_dbc0836c279a4d50bbd6953988085070"
          }
        },
        "28a3539d510243a8a235dd0c7cbeea9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97cb2d5cbf714395b88e5c48e4401283",
            "placeholder": "​",
            "style": "IPY_MODEL_2b5977e1f9f148c6a6569cb957125747",
            "value": "Batches: 100%"
          }
        },
        "f83bd28976cb4446b7329aa9c073778e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58fb68561984413db2ceaeccc9fa3128",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9204a336ca8f4b9a99ad74229617b067",
            "value": 15
          }
        },
        "2866579d69a9419fa77a46bf86bcbfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baacd1c1c7064663ab99b5b381959b65",
            "placeholder": "​",
            "style": "IPY_MODEL_44f875c8cd2a43d7bfa406a6b68bb1eb",
            "value": " 15/15 [00:00&lt;00:00, 41.60it/s]"
          }
        },
        "dbc0836c279a4d50bbd6953988085070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cb2d5cbf714395b88e5c48e4401283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5977e1f9f148c6a6569cb957125747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58fb68561984413db2ceaeccc9fa3128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9204a336ca8f4b9a99ad74229617b067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baacd1c1c7064663ab99b5b381959b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f875c8cd2a43d7bfa406a6b68bb1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cca516b24b5b4b92b53b251ca648171e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11244acecb8547c4b998be89b8b43ea3",
              "IPY_MODEL_bc6663412b5243a1a481682917b42cb0",
              "IPY_MODEL_40dea74936784ec6aa545f97ad75bd4f"
            ],
            "layout": "IPY_MODEL_c4a4bee89891423d851dbdcf427ad20f"
          }
        },
        "11244acecb8547c4b998be89b8b43ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63b18a103d6d4cc5a342d73a744a6e4c",
            "placeholder": "​",
            "style": "IPY_MODEL_15b49d63f6844421a880a68eb9410d42",
            "value": "Batches: 100%"
          }
        },
        "bc6663412b5243a1a481682917b42cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b92bc5aa544bf296c0c901794b7ffc",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3341a3c77184f8b93aae9d22456cbc4",
            "value": 7
          }
        },
        "40dea74936784ec6aa545f97ad75bd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d046ece6ad6d411c9564504fa1f59dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_8f0eb3ed2b6e402c8fbb82dcbe631188",
            "value": " 7/7 [00:00&lt;00:00, 85.95it/s]"
          }
        },
        "c4a4bee89891423d851dbdcf427ad20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b18a103d6d4cc5a342d73a744a6e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b49d63f6844421a880a68eb9410d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12b92bc5aa544bf296c0c901794b7ffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3341a3c77184f8b93aae9d22456cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d046ece6ad6d411c9564504fa1f59dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f0eb3ed2b6e402c8fbb82dcbe631188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Setup & Dependencies**"
      ],
      "metadata": {
        "id": "wxMtlT9Des1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. install gdown\n",
        "!pip install -q gdown\n",
        "\n",
        "# 1. download your HW2_task2 folder\n",
        "!gdown --folder \\\n",
        "  'https://drive.google.com/drive/folders/1C7IN3IaNFRbLcny9AWfC1gkuEhznEDxC?usp=sharing' \\\n",
        "  -O /content/Hw2_task2\n",
        "\n",
        "# 2. download your pages folder\n",
        "!gdown --folder \\\n",
        "  'https://drive.google.com/drive/folders/14OcRVpXzTy7wAVWvBj_HWYpryCcsts1R?usp=sharing' \\\n",
        "  -O /content/pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcIQQFWVZcKi",
        "outputId": "6b639867-2145-4e29-b2b0-43d9af7e5629"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1jfU68zWURLbsI_qkpVIiuX8-liqFfOhx  r13922a22.csv\n",
            "Processing file 1Rd17o0cIMmGFvTAQonn7H24u-UF8L3Pi AI.pdf\n",
            "Processing file 16yGstZB-QvqyGX-Btioe-5JiuK5bZch4 HW2_query.csv\n",
            "Processing file 1Escae79Tmyob7zY6zLYR_fZpc_1G93eJ HW2_template.csv\n",
            "Processing file 1uGZFsBZptrdfDf31LaCuSKYrdNVWhcyJ submission_1.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jfU68zWURLbsI_qkpVIiuX8-liqFfOhx\n",
            "To: /content/Hw2_task2/ r13922a22.csv\n",
            "100% 1.46k/1.46k [00:00<00:00, 7.38MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rd17o0cIMmGFvTAQonn7H24u-UF8L3Pi\n",
            "To: /content/Hw2_task2/AI.pdf\n",
            "100% 90.4M/90.4M [00:00<00:00, 179MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16yGstZB-QvqyGX-Btioe-5JiuK5bZch4\n",
            "To: /content/Hw2_task2/HW2_query.csv\n",
            "100% 44.7k/44.7k [00:00<00:00, 92.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Escae79Tmyob7zY6zLYR_fZpc_1G93eJ\n",
            "To: /content/Hw2_task2/HW2_template.csv\n",
            "100% 1.50k/1.50k [00:00<00:00, 9.26MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uGZFsBZptrdfDf31LaCuSKYrdNVWhcyJ\n",
            "To: /content/Hw2_task2/submission_1.csv\n",
            "100% 1.45k/1.45k [00:00<00:00, 9.25MB/s]\n",
            "Download completed\n",
            "Retrieving folder contents\n",
            "Processing file 1P3jVGNjbZ3vUMqXeCUyXV4DeemE8pzB5 page_001.png\n",
            "Processing file 1dqwz_27woZDWvtiYxLqBzIMWMlERgomH page_002.png\n",
            "Processing file 1C1qcdMm9KSqFHDl3-Hf1YerUivnRRTxW page_003.png\n",
            "Processing file 1Y44OFLsIv84-Qf6406vp0R0m61BFWyNt page_004.png\n",
            "Processing file 1-7j7i7rg7yqeMwejsDCjfsge6s4Gdpug page_005.png\n",
            "Processing file 1cT-SCdMU_89OesYy0NePv89aX42VzZ6M page_006.png\n",
            "Processing file 1WE0JpoS8Soa5Uk3NBDe9bFF8KDn_P69L page_007.png\n",
            "Processing file 1V2cdMlH3Upg3KrmM5z9Pqjs7YNxoWQpu page_008.png\n",
            "Processing file 1Mgb1L280YIL9Uq_WDkl-jp75hKkOWpA2 page_009.png\n",
            "Processing file 1ytR1NaNOgdUhV7bYN80v-cy8o3bPyBMz page_010.png\n",
            "Processing file 1JTnMDHVDkCuDluQk4fjI4QCflFz1NV7Z page_011.png\n",
            "Processing file 1xS9e9JtBuEFaLNPtMSDASiE9AyvuLJgp page_012.png\n",
            "Processing file 1aLvBteiMO4Nx0YiFlxzg_ZvpWKjE0Wbt page_013.png\n",
            "Processing file 1NU3O0G9vX1fPkmCcY1XLNKjmuLtdaKT_ page_014.png\n",
            "Processing file 1oqErTSnwMNpMsVU8kdModvnGQ74M1K4y page_015.png\n",
            "Processing file 1jkOOGEutCeL4lsbgFAMv8bIR8nDPIRg2 page_016.png\n",
            "Processing file 1u121w8sf1-NVt6PC1YI4jl5GLyBZgKSf page_017.png\n",
            "Processing file 1VBkgqqId_7f_JacqxAglr71lQi2WDmyu page_018.png\n",
            "Processing file 1bEKfVDwOaZunx-JunJqQnZnCMFfuWlUW page_019.png\n",
            "Processing file 11yw-3674u1xuhOEGnj9iw1zB-NFfqDyO page_020.png\n",
            "Processing file 13ys3nYZ0fb52kmJ0IgRC_Aq28PKqSeqa page_021.png\n",
            "Processing file 1SH0hYuSr3C0upFWflTjc6Qt7fqd8dsbP page_022.png\n",
            "Processing file 1oDKfL4dcyBl9n17gKBp8x0k2wV51Fj9T page_023.png\n",
            "Processing file 1hPYxo5Pu7Co9Vy6fkKzc6SuXcmlYuA1v page_024.png\n",
            "Processing file 1iqfIedG3y5KKnq71v_reUkMCwco9jlyU page_025.png\n",
            "Processing file 1ZCiPGcZ4vOCI1RdQsVo5u6b_bYzHh6D7 page_026.png\n",
            "Processing file 1zKFMyDNZJJpQnefN2Om1fCrZESU1Z7QO page_027.png\n",
            "Processing file 1u_sOToIox0qWBAhlYh-2GgnxVaF1MTT3 page_028.png\n",
            "Processing file 14Soo6Qv1MJFRbFEvZIU4sU5_oR10RnW8 page_029.png\n",
            "Processing file 1H6BeHsCMBZQXE8grDQ9qNZspx4W5v5tG page_030.png\n",
            "Processing file 1zzvBHphRHCNTJg84DvW7FGlv1DciQVm6 page_031.png\n",
            "Processing file 17GoNXH2Iw2-9_kpBouNt7FQZTX_YP7zG page_032.png\n",
            "Processing file 1UZS5KAyMKwrqMbj7KQ2rV2D1KSez9VMB page_033.png\n",
            "Processing file 1gH-98-ML1UKARuM1viajshHP88xRzboD page_034.png\n",
            "Processing file 1osUaj0XjGjKt0KoUKpWNJC8dFHW4HcZz page_035.png\n",
            "Processing file 1uRcepXeQrVu0AKMUJLDd_fWuUZintbBk page_036.png\n",
            "Processing file 1wMUegCOn3rHHAWiQjD623UBs0Zl4gesp page_037.png\n",
            "Processing file 1fF8dpvq4ZTMwsPRY7LJcRlIVza9-XO4j page_038.png\n",
            "Processing file 1gRC1gQbY9Sbk5WmAm74k0MsmxyP-sqOj page_039.png\n",
            "Processing file 1l25SHxMOHE2j0rJEZer_6CzTH-nIrJte page_040.png\n",
            "Processing file 1t4B5W1Tg2yVJpwfPdcJxVjHA_zmKvKNY page_041.png\n",
            "Processing file 1O-_b7YCvFgNiAM5hwYZjZ9ReJefQ2-NY page_042.png\n",
            "Processing file 15TmUh5GJYN3-B3MlZ4kr5utpEbqHoWaZ page_043.png\n",
            "Processing file 1RZ5OcydLZ-PEKfk5Nfx4Q4_haaYDRf-u page_044.png\n",
            "Processing file 1iUQNF3caLNN-MrgvwNoX7bcMtNWHRMFq page_045.png\n",
            "Processing file 1B_hNIKf8ep9Hc9jqcxM8ge8JoZBeN0Ey page_046.png\n",
            "Processing file 1I3O8eUPf67cytwyi50WuhKkVimLVh0gg page_047.png\n",
            "Processing file 16J5IKiFJjRuDahYlXa7fC9HcaeOMKZMl page_048.png\n",
            "Processing file 1eRcOqn1506ADRbUYGd3Gyr23oVC_g0UI page_049.png\n",
            "Processing file 11cg_H6LVQhhKht-JAj1W0wUZcXs_ux-w page_050.png\n",
            "Failed to retrieve folder contents:\n",
            "\n",
            "\tThe gdrive folder with url: https://drive.google.com/drive/folders/14O\n",
            "\tcRVpXzTy7wAVWvBj_HWYpryCcsts1R?usp=sharing&hl=en has more than 50\n",
            "\tfiles, gdrive can't download more than this limit.\n",
            "\n",
            "You can use `--remaining-ok` option to ignore this error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPue1RIQeLWN",
        "outputId": "5ccfa3af-c568-4eb5-b58f-a63559e4be40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: transformers==4.48.2 in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.2) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Fetched 384 kB in 1s (362 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# install needed libraries\n",
        "!pip install pdf2image torch accelerate transformers==4.48.2 sentence-transformers chromadb pandas langchain faiss-cpu pillow pytesseract\n",
        "\n",
        "# system utilities\n",
        "!apt-get update && apt-get install -y poppler-utils tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVz1v2IsHt1R",
        "outputId": "8d62328a-be49-4204-e482-a7be441906ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xphlc4OIi7Y",
        "outputId": "2d435187-779b-47fd-8d1a-9f03ab7c438c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install backoff evaluate rouge_score datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Q9fYrBcJAy",
        "outputId": "9e5c71c5-ac52-4160-d529-dacf8276326b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=de242d9bfd16924ce0bfbee4017716f5c30f713ec8502ca6fe764f20bf3681f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, fsspec, dill, rouge_score, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 rouge_score-0.1.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "from transformers import pipeline, AutoProcessor, AutoModelForVision2Seq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "467e1ba1890047d39af21b4c92f4da6c",
            "44e101b887f24fc0988dfec3d2f776e2",
            "bd89473f5b1d4f5f81cac947f4aaab46",
            "97ca5224e4eb4c0a98813be18bdf53aa",
            "70ad825310d5493fbce673d29569547a",
            "e6ec84bc9a9945a3b4d09237e0217eb1",
            "2c03622b1da24b1e9f4b84e74b88ed99",
            "ea6866704f794922b148cb14bc2ae59a",
            "eb22ec64d4044652925e4a055e75fed3",
            "85074396305e47639eea7809e3300578",
            "07668326f05647188745375222b7caa8"
          ]
        },
        "id": "ZYsz3PNPgf1P",
        "outputId": "25cd0674-cf50-4400-fd8b-80aa92ca372b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "467e1ba1890047d39af21b4c92f4da6c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Mount Drive & Download Data**"
      ],
      "metadata": {
        "id": "Pxs5TfTeetS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DouJzqleUQf",
        "outputId": "9d4cc8b3-1e5e-4222-fb72-561a84e7b316"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. PDF → Page Images**"
      ],
      "metadata": {
        "id": "leBosUpHeho_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "BASE = '/content/Hw2_task2'\n",
        "PDF_PATH = os.path.join(BASE, 'AI.pdf')\n",
        "QUERY_CSV= os.path.join(BASE, 'HW2_query.csv')\n",
        "TEMPLATE = os.path.join(BASE, 'HW2_template.csv')\n",
        "\n",
        "IMG_DIR  = '/content/pages'\n",
        "\n",
        "os.makedirs(IMG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "AwpNb25Aa4Jv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each page to a PNG\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "pages = convert_from_path(PDF_PATH, dpi=150, fmt='png',thread_count=12)\n",
        "for i, page in enumerate(pages, start=1):\n",
        "    page.save(f\"{IMG_DIR}/page_{i:03d}.png\", \"PNG\")\n",
        "print(f\"Converted {len(pages)} pages.\")"
      ],
      "metadata": {
        "id": "wYXiDt9za3IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Caption Each Page**"
      ],
      "metadata": {
        "id": "23mYU7qHedsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Tesseract OCR + Φ-4 Captioner"
      ],
      "metadata": {
        "id": "G87QoNj91zBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from transformers import pipeline\n",
        "\n",
        "# Support Chinese: 'chi_tra'／'eng+chi_tra'\n",
        "# point to tesseract binary (usually /usr/bin/tesseract)\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "OCR_LANG = 'eng'\n",
        "OCR_PSM  = 3   # Full Page Detection"
      ],
      "metadata": {
        "id": "GdC3uQrezn-u"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Φ-4 multimodal-instruct"
      ],
      "metadata": {
        "id": "prVrT2gJ169V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
        "\n",
        "model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "# Load processor and model (set to GPU with full precision)\n",
        "processor_phi = AutoProcessor.from_pretrained(model_path, trust_remote_code=True, use_fast=True)\n",
        "\n",
        "model_phi = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    _attn_implementation='eager',\n",
        ").cuda()\n",
        "\n",
        "\n",
        "# Load generation configuration and define prompt tokens as in the reference code.\n",
        "generation_config = GenerationConfig.from_pretrained(model_path)\n",
        "user_prompt = '<|user|>'\n",
        "assistant_prompt = '<|assistant|>'\n",
        "prompt_suffix = '<|end|>'\n",
        "\n",
        "def get_phi4_caption(image):\n",
        "    \"\"\"\n",
        "    Generate an image caption using Phi‑4.\n",
        "\n",
        "    Constructs a prompt as per the reference code and returns the generated caption.\n",
        "    \"\"\"\n",
        "    # Construct the prompt for image captioning\n",
        "    prompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\n",
        "    inputs = processor_phi(text=prompt, images=image, return_tensors='pt').to('cuda:0')\n",
        "\n",
        "    generate_ids = model_phi.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2000,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    # Remove the prompt tokens from the generated output\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    caption = processor_phi.batch_decode(\n",
        "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    return caption\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "ac5a2206d6ff482aac5980eb7ee8e3d5",
            "86b0df5bff6b41d5a31f345b8b682a65",
            "3af8d59818ee4810a0ad0aeeb3c0aeb5",
            "bc9ebc3db81443728db92cffa5610138",
            "88895776595c46d29dba4aa8207b332d",
            "593291182278446e8adfb96a0b8851cc",
            "8fed8e0e6e2d4b93ab613b1ee2705dcd",
            "919ee68707ed4ff0bde722fbbe19b25e",
            "b40f211a4fe04b0fbbed94890f638c73",
            "e1646260a7884a25a81850fe5acca24b",
            "817cc1bef2ae44b094da17dd905f30d7"
          ]
        },
        "id": "xPsI5llxLNPI",
        "outputId": "1103086f-d229-4f78-f2e5-00d9f1f5c10c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/0af439b3adb8c23fda473c4f86001dbf9a226021/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
            "  lambda i: encoder_checkpoint_wrapper(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5a2206d6ff482aac5980eb7ee8e3d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_texts = {}\n",
        "for img_path in sorted(glob.glob(f\"{IMG_DIR}/*.png\")):\n",
        "    page_num = int(os.path.basename(img_path).split('_')[1].split('.')[0])\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # 1 OCR\n",
        "    ocr_text = pytesseract.image_to_string(\n",
        "        image,\n",
        "        lang=OCR_LANG,\n",
        "        config=f'--psm {OCR_PSM}'\n",
        "    ).strip()\n",
        "\n",
        "    # 2 Φ-4 Captioning\n",
        "    caption = get_phi4_caption(image)\n",
        "    print(f\"Caption for {img_path}-- OCR: {ocr_text}; Caption: {caption}\")\n",
        "\n",
        "    # 3 Combine\n",
        "    combined = (\n",
        "        \"=== OCR TEXT ===\\n\" +\n",
        "        ocr_text +\n",
        "        \"\\n\\n=== Φ-4 CAPTION ===\\n\" +\n",
        "        caption\n",
        "    )\n",
        "    # 4. Extract page number and save\n",
        "    pg = int(os.path.basename(img_path).split(\"_\")[1].split(\".\")[0])\n",
        "    page_texts[page_num] = combined\n",
        "\n",
        "# quick sanity check\n",
        "print(\"Page 333 snippet:\\n\", page_texts[333][:500])\n",
        "print(\"Page 445 snippet:\\n\", page_texts[445][:200])\n",
        "print(\"Page 463 snippet:\\n\", page_texts[463][:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jqndcTm1ghV",
        "outputId": "29b16e94-6263-4b26-b937-29e00233ee19"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_001.png-- OCR: Preliminaries and Al Agent Basics\n",
            "\n",
            "Wen-Huang Cheng (2832 2)\n",
            "\n",
            "National Taiwan University\n",
            "\n",
            " \n",
            "\n",
            "wenhuang@csie.ntu.edu.tw; Caption: The image displays a presentation slide with a blue background. At the top, there is a title \"Artificial Intelligence\" followed by a subtitle \"Preliminaries and AI Agent Basics\". Below the subtitles, there is a logo of National Taiwan University and the name \"Wen-Huang Cheng (郭文皇)\" with an email address \"wenhuang@csie.ntu.edu.tw\". On the right side, there is a graphic of a human head with a circuit pattern and the letters \"AI\" inside it. The slide number \"1\" is visible at the bottom left corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_002.png-- OCR: Ne. <\n",
            "\n",
            "SkillMimic: Learning Reusable Basketball Skills from Demonstrations\n",
            "\n",
            "aes)\n",
            "\n",
            " \n",
            "\n",
            "Yinhuai Wang*, Qihan Zhao*, Runyi Yu*, Ailing Zeng, Jing lin, Zhengyi Luo, Hok Wai Tsui, Jiwen Yu,\n",
            "Xiu Li, Qifeng Chen, Jian Zhang, Lei Zhang, Ping Tan\n",
            "Carnegie\n",
            "\n",
            "I) ete fc ness or uNITREE Fi8 Ge AF (MFA idea= Tencent “&) \\idion\n",
            "\n",
            "SCIENCE AND TECHNOLOGY PEKING UNeRsITY ~S ghua University — paki OR PASATR RAR << University; Caption: The image shows a basketball court with a green humanoid figure playing basketball.\n",
            "Caption for /content/drive/MyDrive/pages/page_003.png-- OCR: Al Weekly\n",
            "\n",
            "Agent Robotics: Learning-from-observation (LfQ)\n",
            "—\n",
            "\n",
            "Task model\n",
            "(Minsky’s frame)\n",
            "\n",
            "   \n",
            "\n",
            "How-te-da\n",
            "\n",
            "  \n",
            "\n",
            "Source: https://multimodalagentai.github.io/files/Katsushilkeuchi_Talk3_CVPR2024_Tutorial_GeneralistAgentAl.pdf; Caption: The image is a slide from a presentation titled 'AI Weekly' focusing on 'Agent Robotics: Learning-from-observation (LfO)'. It features two photographs of humanoid robots, one labeled 'Teaching mode' and the other 'Execution mode'. The slide explains the concept of 'Task model (Minsky’s frame)' which is divided into 'Encoding' (What-to-do, Where-to-do) and 'Skill agents' (How-to-do). It emphasizes that this is not direct mimicking but involves a process of learning from observation. The source of the information is provided at the bottom of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_004.png-- OCR: Ne <\n",
            "\n",
            "Basic principle of deep reinforcement learning\n",
            "\n",
            "Agent\n",
            "\n",
            "State s Action a\n",
            "\n",
            " \n",
            "\n",
            "Action\n",
            "\n",
            " \n",
            "\n",
            "Environment; Caption: The image shows a diagram explaining the basic principle of deep reinforcement learning. It includes an agent with a deep neural network, a policy π, and an environment. The agent receives a state s, processes it through the neural network to determine an action a, and then interacts with the environment. The environment provides a reward based on the action taken.\n",
            "Caption for /content/drive/MyDrive/pages/page_005.png-- OCR: SkillMimic: Concept\n",
            "\n",
            "aoa?\n",
            "i)\n",
            "\n",
            "SS eal —\n",
            "Mation SkillMimic\n",
            "— ie Capture\n",
            "\n",
            "   \n",
            "\n",
            "Real-World Skill\n",
            "\n",
            "lO) Mim er ie)\n",
            "\n",
            "Unified learning for all basketball skills; Caption: The image depicts a concept for SkillMimic, which appears to be a system for capturing and simulating basketball skills. On the left, there is a real-world scenario where a person is performing a basketball skill, likely dribbling, in front of a building. In the center, there is a representation of motion capture data, indicated by a blue figure with motion lines, suggesting the tracking of the basketball player's movements. On the right, there is a green figure in a simulated environment, which represents the SkillMimic system in action, mimicking the basketball skill in a virtual setting. The caption 'Unified learning for all basketball skills' suggests that the system is designed to learn and replicate various basketball skills in a unified manner.\n",
            "Caption for /content/drive/MyDrive/pages/page_006.png-- OCR: SkillMimic: Skill Learning Results\n",
            "\n",
            "Dribble left Dribble turn Dribble back\n",
            "\n",
            " \n",
            "\n",
            "Layup mp shot Turnaround layup\n",
            "\n",
            " \n",
            "\n",
            "Rebound Pickup Pass\n",
            "\n",
            " \n",
            "\n",
            "Learning diverse basketball skills within a single policy; Caption: The image shows a basketball court with a green figure demonstrating various basketball skills. The skills are labeled as Dribble left, Dribble turn, Dribble back, Layup, Jump shot, Turnaround layup, Rebound, Pickup, and Pass.\n",
            "Caption for /content/drive/MyDrive/pages/page_007.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Preliminaries\n",
            "\n",
            "m Generative Models\n",
            "= Transformers\n",
            "= Diffusion Models\n",
            "\n",
            "= Multimodal Embedding Spaces\n",
            "= Al Agent Basics\n",
            "e Acting\n",
            "e Tool Use\n",
            "e Memory; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is structured with bullet points and sub-bullet points, indicating a list of topics to be covered in the lecture. The main topics are 'Preliminaries,' 'Generative Models,' 'Multimodal Embedding Spaces,' and 'AI Agent Basics.' Under 'Generative Models,' there are two subtopics: 'Transformers' and 'Diffusion Models.' Under 'AI Agent Basics,' there are three subtopics: 'Acting,' 'Tool Use,' and 'Memory.' The slide number '7' is visible at the bottom, suggesting this is the seventh slide in the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_008.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Preliminaries\n",
            "\n",
            "m Generative Models\n",
            "= Transformers; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is structured with bullet points and sub-bullet points, indicating a list of topics to be covered in the lecture. The main topics are 'Preliminaries,' 'Generative Models,' 'Multimodal Embedding Spaces,' and 'AI Agent Basics.' Under 'Generative Models,' there are two subtopics: 'Transformers' and 'Diffusion Models.' Under 'AI Agent Basics,' there are three subtopics: 'Acting,' 'Tool Use,' and 'Memory.' The slide number '8' is visible at the bottom, suggesting this is the eighth slide in the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_009.png-- OCR: Sequence to Sequence Generation\n",
            "with Transformer Based Models\n",
            "Input: Sequence x,, ..., X7\n",
            "* Output: Sequence y;, ..., yr\n",
            "\n",
            "¢ Transformer is an encoder-decoder architecture:\n",
            "¢ An encoder (neural network) analyzes the input and builds an\n",
            "intermediate representation (a.k.a. context vector) of it;\n",
            "¢ Adecoder (neural network) analyzes the intermediate\n",
            "representation and creates an output.\n",
            "\n",
            "The fy Lacasaeé\n",
            "is red rossa\n",
            "\n",
            "Vector\n",
            "\n",
            " \n",
            "\n",
            "Source: https://huggingface.co/blog/encoder-decoder; Caption: The image is a slide from a presentation titled 'Sequence to Sequence Generation with Transformer Based Models'. It explains the process of sequence to sequence generation using Transformer models. The slide lists the input and output sequences, describes the Transformer as an encoder-decoder architecture, and details the roles of the encoder and decoder. The encoder analyzes the input and builds an intermediate representation called a context vector, while the decoder analyzes this representation and creates an output. The slide includes a diagram illustrating the flow from input to output through the encoder and decoder, with a context vector in between. The source of the information is cited at the bottom of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_010.png-- OCR: YJ Transformer Based Models C\n",
            "ys\n",
            "\n",
            "* Transformer is an encoder-decoder architecture with a core\n",
            "component — attention mechanism.\n",
            "« Attention allows the model to focus on the most relevant parts of the\n",
            "input sequence when generating the output.\n",
            "\n",
            "VN 4 ™\n",
            "\n",
            " \n",
            "\n",
            "La casa é\n",
            "rossa\n",
            "\n",
            "  \n",
            "\n",
            "%\n",
            "\n",
            "The house is red\n",
            "—_—_——— Context Vectors\n",
            "\n",
            "Source: https://huggingface.co/blog/encoder-decoder; Caption: The image is a slide from a presentation on Transformer Based Models. It features a diagram illustrating the architecture of a Transformer model, which includes an Encoder and a Decoder. The Encoder is depicted as a blue rectangle, and the Decoder as an orange rectangle. There are arrows indicating the flow of information from the Encoder to the Decoder. Additionally, there are context vectors shown as small circles with arrows pointing towards the Decoder. The slide also includes text explaining that the Transformer model has an attention mechanism that allows it to focus on the most relevant parts of the input sequence when generating the output. The slide is sourced from a blog post on huggingface.co.\n",
            "Caption for /content/drive/MyDrive/pages/page_011.png-- OCR: A Example\n",
            "—_ Language Translation with Transformers\n",
            "\n",
            "Transformers utilize an autoregressive\n",
            "approach for output generation.\n",
            "\n",
            "   \n",
            "  \n",
            "\n",
            "MEENA\n",
            "\n",
            "     \n",
            "\n",
            "      \n",
            "\n",
            "fot ft t ft +t f toot (ft (ft ot cf\n",
            "\n",
            "| | | |\n",
            "ee ib |f jf jt uti\n",
            "xi=I x»=want x3=to x,=buy X5=a Xg=—Car x;=EOS yo=BOS yictch yew yeoein vento van vechos\n",
            "\n",
            "Source: https://huggingface.co/blog/encoder-decoder; Caption: The image is a diagram illustrating the process of language translation using transformers.\n",
            "Caption for /content/drive/MyDrive/pages/page_012.png-- OCR: YJ Example:\n",
            "—_ Language Translation with Transformers\n",
            "\n",
            "Autoregressive generation is the inference-time\n",
            "\n",
            "Step 1 procedure of iteratively calling a model with its Step 2\n",
            "own generated outputs, given a few initial inputs.\n",
            "\n",
            "  \n",
            "\n",
            "i i\n",
            "i, 4h ab\n",
            "EE (ost nnd) HS Gas) Gas)\n",
            "+ , t +\n",
            "t t\n",
            "[Decoder Block) => Decoder Block)\n",
            "t t t\n",
            "t tf t t t ft f t tt\n",
            "re ee es | i i i\n",
            "x:=I  x.=want x3=to x,=buy X5=a Xg=Car x;=EOS yo=BOS yo=BOS yi=Ich\n",
            "\n",
            "12 Source: https://huggingface.co/blog/encoder-decoder; Caption: The image is a slide from a presentation, titled 'Example: Language Translation with Transformers'. It explains the concept of autoregressive generation in the context of language translation using transformers. The slide is divided into two main steps, each illustrated with diagrams. Step 1 shows a sequence of encoder blocks receiving inputs and passing them to decoder blocks. Step 2 depicts the decoder blocks generating outputs based on the inputs and their own generated outputs. The slide also includes a list of example inputs and outputs, such as 'x1=I' and 'y0=BOS', and a source link to a blog post on the Hugging Face website.\n",
            "Caption for /content/drive/MyDrive/pages/page_013.png-- OCR: YJ Example:\n",
            "—_ Image Captioning with Transformers\n",
            "\n",
            "¢ Image Captioning is the task of describing the content\n",
            "of an image in words.\n",
            "\n",
            " \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Captioning Model\n",
            "\n",
            "A happy dog is standing in the ocean\n",
            "\n",
            "13 Source: https://eman-lotfy-elrefai.medium.com/5-models-to-use-for-image-captioning-task-59143fb2df6d; Caption: The image shows a golden retriever standing in the ocean with a red frisbee in its mouth.\n",
            "Caption for /content/drive/MyDrive/pages/page_014.png-- OCR: YJ Example:\n",
            "—_ Image Captioning with Transformers\n",
            "\n",
            "   \n",
            "  \n",
            "\n",
            "Input: Image |\n",
            "Output: Sequence y = y,, y,,.--; Y\n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "[START] person wearing hat\n",
            "\n",
            "Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram explaining the process of image captioning with transformers. It includes a visual representation of the input image, the transformer encoder and decoder, and the output sequence of captions.\n",
            "Caption for /content/drive/MyDrive/pages/page_015.png-- OCR: YJ The Transformer Encoder Block (1/5) C\n",
            "mm\n",
            "Lseo| | lLeee| ~ [Ba\n",
            "\n",
            "Made up of N encoder blocks.\n",
            "\n",
            "Transformer encoder\n",
            "\n",
            " \n",
            "\n",
            "15 Source: https://cs23‘n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Transformer Encoder Block, which is part of a larger Transformer Encoder. The diagram is labeled as 'The Transformer Encoder Block (1/5)', indicating that this is the first part of a five-part series. The Transformer Encoder is depicted as a series of N encoder blocks stacked vertically, with each block containing a matrix labeled 'C' with dimensions that vary from block to block. The output of each block is shown as a matrix labeled 'Z' with dimensions that also vary. The diagram is sourced from a Stanford University lecture, as indicated by the URL at the bottom of the image.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_016.png-- OCR: Shi The Transformer Encoder Block (2/5) C\n",
            "\n",
            "Let's dive into one encoder block\n",
            "\n",
            "Transformer encoder\n",
            "\n",
            "~—_\n",
            "\n",
            "Eee oe fee\n",
            "\n",
            "Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Transformer Encoder Block, which is part of a larger presentation on AI. The diagram is divided into two main sections, with the left side showing the input sequence and the right side showing the output sequence. The input sequence is represented by a series of 'x' characters, indicating a variable number of elements, and the output sequence is represented by a series of 'Z' characters, also indicating a variable number of elements. The Transformer Encoder Block is depicted as a series of layers, with each layer consisting of a matrix multiplication step followed by a residual connection. The matrices are labeled with 'C' for the input and 'Z' for the output, and the residual connections are indicated by arrows pointing from the output of the matrix multiplication to the input of the next layer. The slide is labeled as '16' and includes a source URL at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_017.png-- OCR: The Transformer Encoder Block (3/5) C\n",
            "\n",
            "Ay\n",
            "nS ;\n",
            "\n",
            "  \n",
            "\n",
            "Transformer encoder\n",
            "\n",
            "——__ aan Add positional encoding\n",
            "\n",
            "Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Transformer Encoder Block, which is part of a larger presentation on AI.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_018.png-- OCR: Positional Encoding\n",
            "\n",
            " \n",
            "\n",
            "Concatenate special positional\n",
            "encoding Pp, to each input vector x,\n",
            "\n",
            "We use a function pos: N —R®\n",
            "\n",
            "to process the position j of the\n",
            "\n",
            "vector into a d-dimensional vector\n",
            "\n",
            "Source: “Attention is all you need,” NeurlPS, 2017.\n",
            "\n",
            "18 So, p, = pos()); Caption: The image is a slide from a presentation, titled 'Positional Encoding'. It features a diagram illustrating the process of positional encoding in the context of self-attention in neural networks. The diagram is divided into three main sections, each represented by a colored box with text labels. The top section, in a light blue box, is labeled 'self-attention' and shows three vectors labeled y0, y1, and y2. The middle section, in a purple box, is labeled 'position encoding' and shows three vectors labeled p0, p1, and p2. The bottom section, in a darker purple box, shows three vectors labeled x0, x1, and x2. Arrows indicate the flow from the self-attention section to the position encoding section, and then to the x vectors. Below the diagram, there is a text explanation that reads 'Concatenate special positional encoding p1 to each input vector xj. We use a function pos: N -> Rd to process the position j of the vector into a d-dimensional vector. So, pj = pos(j)'. The source of the content is cited as 'Attention is all you need,' NeurIPS, 2017.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_019.png-- OCR: 19\n",
            "\n",
            "Positional Encoding\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo | Ys || Yo\n",
            "\n",
            "self-attention\n",
            "\n",
            "ae\n",
            "\n",
            "position encoding\n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "Concatenate special positional\n",
            "encoding P, to each input vector x,\n",
            "\n",
            "We use a function pos: N —R®\n",
            "to process the position j of the\n",
            "vector into a d-dimensional vector\n",
            "\n",
            "So, p, = pos())\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Desiderata of pos(.) :\n",
            "\n",
            "1.\n",
            "\n",
            "2.\n",
            "\n",
            "3.\n",
            "\n",
            "It should output a unique encoding for each time-step\n",
            "(word’s position in a sentence)\n",
            "\n",
            "Distance between any two time-steps should be\n",
            "consistent across sentences with different lengths.\n",
            "Our model should generalize to longer sentences\n",
            "without any efforts. Its values should be bounded.\n",
            "\n",
            "It must be deterministic.\n",
            "\n",
            "Source: “Attention is all you need,” NeurlPS, 2017.; Caption: The image is a slide from a presentation, titled 'Positional Encoding'. It features a diagram illustrating the concept of positional encoding in the context of self-attention in neural networks. The diagram is divided into three main sections: 'self-attention', 'position encoding', and a list of desiderata for the function 'pos(.)'. The 'self-attention' section shows three vectors labeled y0, y1, and y2, which are inputs to a self-attention mechanism. The 'position encoding' section shows three vectors labeled x0, x1, and x2, which are inputs to a function 'pos: N →Rd' that processes the position of a vector into a d-dimensional vector. The list of desiderata for 'pos(.)' includes four points: it should output a unique encoding for each time-step, the distance between any two time-steps should be consistent across sentences with different lengths, the model should generalize to longer sentences without any efforts, and it must be deterministic. The source of the content is cited as 'Attention is all you need,' NeurIPS, 2017.\n",
            "Caption for /content/drive/MyDrive/pages/page_020.png-- OCR: 20\n",
            "\n",
            "Positional Encoding\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "self-attention\n",
            "\n",
            "ae\n",
            "\n",
            "position encoding\n",
            "\n",
            "  \n",
            "\n",
            "   \n",
            "\n",
            "Concatenate special positional\n",
            "encoding P, to each input vector x,\n",
            "\n",
            "We use a function pos: N —R®\n",
            "to process the position j of the\n",
            "vector into a d-dimensional vector\n",
            "\n",
            "So, p, = pos())\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Options for pos(.)\n",
            "\n",
            "1. Learn a lookup table:\n",
            "\n",
            "© Learn parameters to use for pos(t) for t € [0, T)\n",
            "© Lookup table contains T x d parameters.\n",
            "\n",
            "Desiderata of pos(.) :\n",
            "\n",
            "1.\n",
            "\n",
            "2.\n",
            "\n",
            "3.\n",
            "\n",
            "It should output a unique encoding for each time-step\n",
            "(word’s position in a sentence)\n",
            "\n",
            "Distance between any two time-steps should be\n",
            "consistent across sentences with different lengths.\n",
            "Our model should generalize to longer sentences\n",
            "without any efforts. Its values should be bounded.\n",
            "\n",
            "It must be deterministic.\n",
            "\n",
            "Source: “Attention is all you need,” NeurlPS, 2017.; Caption: The image is a slide from a presentation on 'Positional Encoding' in the context of AI. It features a diagram on the left side and a bulleted list of options and considerations for positional encoding on the right side.\n",
            "Caption for /content/drive/MyDrive/pages/page_021.png-- OCR: Positional Encoding\n",
            "\n",
            "  \n",
            "\n",
            "Options for pos(.)\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "; 1. Learn a lookup table:\n",
            "Seo ELS NLS © Learn parameters to use for pos(t) for t € [0, T)\n",
            "\n",
            "© Lookup table contains T x d parameters.\n",
            "| | | | 2. Design a fixed function with the desiderata\n",
            "\n",
            "position encoding\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "sin(w.t)\n",
            "\n",
            " \n",
            "\n",
            "cos(w}.t)\n",
            "sin(w». t)\n",
            "Concatenate special positional _ | 608(w2-t)\n",
            "encoding p, to each input vector x, p(t) =\n",
            "We use a function pos: N —R®\n",
            "to process the position j of the sin(wg/2. t) _\n",
            "vector into a d-dimensional vector cos(wa/2-t) | 4 where « 10000*/4\n",
            "\n",
            "Source: “Attention is all you need,” NeurlPS, 2017.\n",
            "\n",
            "21 So, p, = pos()); Caption: The image is a slide from a presentation on Positional Encoding. It includes a diagram illustrating the process of positional encoding in a neural network, with a focus on self-attention and position encoding. The diagram shows input vectors x0, x1, x2, and their corresponding position encodings p0, p1, p2, which are then used in self-attention to produce output vectors y0, y1, y2. The slide also lists two options for implementing positional encoding: learning a lookup table or designing a fixed function with the deserializer. Additionally, there is a mathematical representation of the positional encoding function p(t), which includes sine and cosine functions with specific frequencies. The source of the content is cited as 'Attention is all you need,' NeurIPS, 2017.\n",
            "Caption for /content/drive/MyDrive/pages/page_022.png-- OCR: Positional Encoding Visualization\n",
            "\n",
            " \n",
            "\n",
            "sin(w.t)\n",
            "cos(w;.t)\n",
            "\n",
            "sin(w». t)\n",
            "cos(w. t)\n",
            "\n",
            "p(t) =\n",
            "\n",
            " \n",
            "\n",
            "sin(wqj2-t)\n",
            "\n",
            "cos(wa/2.t) é\n",
            "\n",
            "h _ 1\n",
            "wnere eT opote\n",
            "\n",
            " \n",
            "\n",
            "Source: https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers; Caption: Slide 22\n",
            "Caption for /content/drive/MyDrive/pages/page_023.png-- OCR: YJ The Transformer Encoder Block (4/5) C\n",
            "mm\n",
            "\n",
            "Attention attends over\n",
            "all the vectors\n",
            "Add positional encoding\n",
            "\n",
            "Transformer encoder\n",
            "\n",
            " \n",
            "\n",
            "23 Source: https://cs23‘n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Transformer Encoder Block, which is part of a larger presentation on AI.\n",
            "Caption for /content/drive/MyDrive/pages/page_024.png-- OCR: 24\n",
            "\n",
            "Self-Attention Layer\n",
            "\n",
            " \n",
            "\n",
            "Outputs:\n",
            "Context vectors: y (shape: N x D)\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "self-attention\n",
            "\n",
            "  \n",
            "\n",
            "Inputs:\n",
            "Input vectors: x (shape: N x D)\n",
            "Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image displays a diagram of a Self-Attention Layer, which is a component of a neural network architecture. The diagram illustrates the process of generating context vectors from input vectors using self-attention. The inputs are represented by 'x' with a shape of N x D, and the outputs are context vectors 'y' with a shape of N x D. The self-attention mechanism is depicted as a layer that takes the input vectors and produces the output context vectors.\n",
            "Caption for /content/drive/MyDrive/pages/page_025.png-- OCR: yN Self-Attention Layer\n",
            "mm\n",
            "\n",
            "25\n",
            "\n",
            "Input vectors\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Outputs:\n",
            "Context vectors: y (shape: N x D)\n",
            "\n",
            "Operations:\n",
            "Query vectors: q = xW,\n",
            "\n",
            "Inputs:\n",
            "Input vectors: x (shape: N x D)\n",
            "Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image is a slide from a presentation, specifically slide number 25, discussing the Self-Attention Layer. It includes a diagram illustrating the flow of data through the self-attention mechanism, with input vectors and query vectors, and the resulting context vectors. The slide also lists the outputs and operations involved in the self-attention layer.\n",
            "Caption for /content/drive/MyDrive/pages/page_026.png-- OCR: yN Self-Attention Layer\n",
            "mm\n",
            "\n",
            "Outputs:\n",
            "Context vectors: y (shape: N x D)\n",
            "\n",
            "Attention\n",
            "\n",
            "Operations:\n",
            "\n",
            "Query vectors: gq = xW,\n",
            "Key vectors: k = xW\n",
            "Value vectors: v = x\\V\\\n",
            "Alignment: e;; = g;° ki / VD\n",
            "Attention: a = softmax(e)\n",
            "Output: y; = >i aij ° Vv)\n",
            "\n",
            "Input vectors\n",
            "Alignment\n",
            "\n",
            "Inputs:\n",
            "Input vectors: x (shape: N x D)\n",
            "\n",
            "26 Source: https://cs23‘n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image is a slide from a presentation, specifically slide number 26, which explains the Self-Attention Layer in the context of neural networks. The slide is divided into two main sections: a diagram on the left and text on the right. The diagram illustrates the components and flow of the self-attention mechanism, including input vectors, query vectors, key vectors, value vectors, and the resulting context vectors. The text on the right side of the slide provides a detailed explanation of the operations involved in the self-attention layer, such as the calculation of query, key, and value vectors, alignment, attention, and the final output. The slide also includes a source link at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_027.png-- OCR: yN Self-Attention Layer\n",
            "mm\n",
            "\n",
            "Permutation equivariant\n",
            "\n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "self-attention self-attention self-attention\n",
            "\n",
            "        \n",
            "\n",
            "Self-attention layer doesn’t care about the orders of the inputs!\n",
            "Problem: How can we encode ordered sequences like language or spatially ordered image features?\n",
            "\n",
            "— That’s why we need positional encoding!\n",
            "\n",
            "27 Source: https://cs23‘n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Self-Attention Layer in a neural network, with three different permutations of input sequences and their corresponding self-attention outputs.\n",
            "Caption for /content/drive/MyDrive/pages/page_028.png-- OCR: YJ Multi-Head Self-Attention Layer\n",
            "\n",
            "  \n",
            "\n",
            "¢ Multiple self-attention heads in parallel\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo || 4 || Yo\n",
            "\n",
            " \n",
            "\n",
            "| Concatenate\n",
            "\n",
            " \n",
            "\n",
            "Self-attention of Head, Self-attention of Head, —Self-attention of Head, ...\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "28 Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image displays a diagram of a Multi-Head Self-Attention Layer used in artificial intelligence.\n",
            "Caption for /content/drive/MyDrive/pages/page_029.png-- OCR: YJ The Transformer Encoder Block (5/5) C\n",
            "mm\n",
            "\n",
            "MLP: Multilayer Perceptron\n",
            "\n",
            "Attention attends over\n",
            "all the vectors\n",
            "Add positional encoding\n",
            "\n",
            "Transformer encoder\n",
            "\n",
            "  \n",
            "\n",
            "29 Source: https://cs23‘n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image shows a diagram of a Transformer Encoder Block, which is part of a larger presentation on AI.\n",
            "Caption for /content/drive/MyDrive/pages/page_030.png-- OCR: YJ The Transformer Decoder Block (1) C\n",
            "mm\n",
            "\n",
            "person wearing hat [END]\n",
            "\n",
            "  \n",
            "\n",
            "Made up of N decoder blocks.\n",
            "\n",
            "JBpPOISP JOWJOJSUBI |\n",
            "\n",
            " \n",
            "\n",
            "30 [START] person wearing hat Source: https://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf; Caption: The image displays a diagram of a Transformer Decoder Block, which is part of a neural network architecture used in natural language processing. The diagram is labeled 'The Transformer Decoder Block (1)' and is divided into two main sections. The top section shows a sequence of words '[START] person wearing hat [END]' with corresponding token representations y0, y1, y2, and y3. The bottom section illustrates the structure of the decoder block, which is made up of N decoder blocks, each represented by a green rectangle. The decoder blocks are connected to a series of matrices labeled C0,0, C0,1, C0,2, and so on, which are part of the attention mechanism within the transformer model. The source of the image is provided at the bottom as a URL from Stanford University's course slides.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_031.png-- OCR: person wearing hat [END]\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Let's dive into one\n",
            "decoder block\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "y, Y2\n",
            "\n",
            " \n",
            "\n",
            "Yo\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "31 [START] person wearing hat; Caption: The image is a slide from a presentation, specifically slide number 31, titled 'The Transformer Decoder Block (2)'. It features a diagram illustrating the structure of a Transformer decoder block. The diagram is divided into two main sections, with the left side showing the input sequence 'person wearing hat' and the right side showing the output sequence 'y0 y1 y2 y3'. The Transformer decoder block is represented by a large green rectangle, which is connected to a series of smaller rectangles labeled 'C0,0', 'C0,1', 'C0,2', and so on, indicating the presence of multiple attention heads within the block. Each of these attention heads is connected to a corresponding output 'y0', 'y1', 'y2', 'y3'. The diagram also includes a smaller rectangle labeled 'FC' at the top, which stands for fully connected layer, and is connected to the output sequence. The slide text 'Let's dive into one decoder block' suggests that the slide is an introduction to the detailed workings of a single decoder block within the Transformer architecture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_032.png-- OCR: FC\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "  \n",
            "    \n",
            " \n",
            "   \n",
            "\n",
            " \n",
            " \n",
            "  \n",
            " \n",
            "  \n",
            "  \n",
            " \n",
            " \n",
            "\n",
            "Yo\n",
            "\n",
            "person wearing hat [END]\n",
            "@)\n",
            "\n",
            "G)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo Ms Yo Ya\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Most of the network is\n",
            "the same the\n",
            "transformer encoder.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Layer norm\n",
            "C)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Masked Self-attention\n",
            "only interacts with\n",
            "\n",
            "past inputs.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo y Yo Y3\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "[START] person wearing hat; Caption: The image is a slide from a presentation, specifically slide number 32, titled 'The Transformer Decoder Block (3)'. It features a diagram of a transformer decoder block, which is part of a neural network architecture used in machine learning. The diagram is divided into two main sections: the Transformer decoder on the left and the masked multi-head self-attention mechanism on the right. The Transformer decoder section shows a sequence of inputs labeled from y0 to y3, with a start token '[START]' and an end token '[END]'. The inputs are processed through a series of layers, each with a layer normalization step followed by a multi-head self-attention mechanism and a MLP (multi-layer perceptron). The masked multi-head self-attention mechanism is highlighted in orange and is noted to only interact with past inputs. The diagram also includes a note stating that most of the network is the same as the transformer encoder, with the exception of the masked self-attention.\n",
            "Caption for /content/drive/MyDrive/pages/page_033.png-- OCR: YJ Masked Self-Attention Layer C\n",
            "mm\n",
            "\n",
            "Output\n",
            "Probabilities\n",
            "\n",
            "¢ Masked self-attention is used\n",
            "to ensure that the model\n",
            "\n",
            "doesn’t attend to some of the\n",
            "\n",
            "tokens in the input sequence\n",
            "\n",
            "Nx during training or generation.\n",
            "\n",
            "     \n",
            "\n",
            "The architecture of\n",
            "Vision Transformer (ViT)\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "   \n",
            " \n",
            "  \n",
            "\n",
            "  \n",
            "  \n",
            " \n",
            "\n",
            "Add & Norm\n",
            "\n",
            "(Add & Norm J =\n",
            "a Multi-Head\n",
            "Feed Attention\n",
            "E dD\n",
            "Add & Norm\n",
            "LAdd & Norm J\n",
            "Add & Norm hosed\n",
            "Multi-Head Multi-Head\n",
            "Attention Attention\n",
            "Xt\n",
            "\n",
            "a\n",
            "Positional\n",
            "\n",
            "CG © @ Encoding\n",
            "\n",
            "Input Output\n",
            "Embedding Embedding\n",
            "\n",
            "33 Inputs Outputs Source: “Attention is All You Need,” NIPS. 2017.\n",
            "(shifted right)\n",
            "\n",
            "  \n",
            "    \n",
            "   \n",
            "  \n",
            "\n",
            "    \n",
            "   \n",
            " \n",
            "   \n",
            " \n",
            " \n",
            "\n",
            "Positional\n",
            "Encoding; Caption: The image displays the architecture of Vision Transformer (ViT) with a focus on the Masked Self-Attention Layer. It includes a diagram illustrating the flow of data through various components such as Add & Norm, Feed Forward, Multi-Head Attention, and Masked Multi-Head Attention. The diagram also shows the input and output embeddings, positional encoding, and the final output probabilities.\n",
            "Caption for /content/drive/MyDrive/pages/page_034.png-- OCR: YJ Masked Self-Attention Layer C\n",
            "mm\n",
            "\n",
            "   \n",
            "\n",
            "¢ Prevent vectors from looking\n",
            "at future vectors\n",
            "\n",
            "¢ ‘Future’ alignment scores are\n",
            "set to -infinity\n",
            "\n",
            "mul(—) + add (1)\n",
            "\n",
            "   \n",
            "\n",
            "fe\n",
            "Attention\n",
            "\n",
            "tT\n",
            "CE\n",
            "---\n",
            "\n",
            "an\n",
            "=\n",
            "@\n",
            "x<\n",
            "—~\n",
            "S\n",
            "~— |\n",
            "\n",
            "Input vectors\n",
            "\n",
            "<\n",
            "o\n",
            "E\n",
            "Cc\n",
            "D>\n",
            "<\n",
            "\n",
            "[= Je |\n",
            "[--\n",
            "\n",
            "34 Source: “Attention is All You Need,” NIPS. 2017.; Caption: The image displays a diagram of a Masked Self-Attention Layer, which is a component of a neural network architecture. The diagram is divided into two main sections: the left side shows the input vectors and the right side illustrates the attention mechanism. The input vectors are labeled as V0, V1, and V2, and the attention mechanism includes a series of operations such as 'mul' and 'add' leading to a 'softmax' function. The attention scores are represented in a matrix with some values masked out. The diagram also includes annotations explaining the purpose of the self-attention layer, such as preventing vectors from looking at future vectors and setting future alignment scores to negative infinity. The source of the diagram is cited as \"Attention is All You Need,\" NIPS. 2017.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_035.png-- OCR: 35\n",
            "\n",
            "person wearing\n",
            "\n",
            "hat\n",
            "\n",
            " \n",
            "\n",
            "Yo Ms\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Ye\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "5) JOWJOJSUCL |\n",
            "\n",
            "  \n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo Y\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Yo\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "[START] person\n",
            "\n",
            "wearing\n",
            "\n",
            "hat\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "FC\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            "Yo\n",
            "G)\n",
            "[| _Layernorm _|\n",
            "\n",
            "Layer norm\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Layer norm\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            " \n",
            "\n",
            "Positional encoding\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            " \n",
            "\n",
            "Multi-head attention\n",
            "block is NOT self-\n",
            "attention. It attends\n",
            "over encoder outputs.\n",
            "\n",
            "For image captions,\n",
            "this is how we inject\n",
            "image features into\n",
            "the decoder.; Caption: The image is a slide from a presentation, specifically slide number 35, titled 'The Transformer Decoder Block (4)'. It features a diagram of a transformer decoder block, which is a component of a transformer model used in machine learning for tasks such as language translation and image captioning. The diagram is divided into two main sections: the left side shows the transformer decoder itself, and the right side illustrates the multi-head attention mechanism within the decoder block. The transformer decoder is depicted as a series of layers with inputs labeled 'y0' to 'y3' and outputs labeled 'y0' to 'y3'. The multi-head attention mechanism is shown with components labeled 'MLP', 'Layer norm', 'Multi-head attention k', 'v', 'q', and 'Masked Multi-head self-attention'. The slide text explains that the multi-head attention block is not self-attention and that it attends over encoder outputs. It also mentions that for image captions, this is how image features are injected into the decoder.\n",
            "Caption for /content/drive/MyDrive/pages/page_036.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Preliminaries\n",
            "\n",
            "m Generative Models\n",
            "= Transformers\n",
            "= Diffusion Models\n",
            "\n",
            "36; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is structured with bullet points and sub-bullet points, indicating a list of topics to be covered in the lecture. The main topics are 'Preliminaries,' 'Generative Models,' 'Multimodal Embedding Spaces,' and 'AI Agent Basics.' Under 'Generative Models,' there are two subtopics: 'Transformers' and 'Diffusion Models.' Under 'AI Agent Basics,' there are three subtopics: 'Acting,' 'Tool Use,' and 'Memory.' The slide number '36' is visible at the bottom, suggesting this is part of a larger presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_037.png-- OCR: m Text to Image Generation\n",
            "e Generating realistic images from text descriptions\n",
            "\n",
            "by OpenAl’s DALL-E 2\n",
            "\n",
            " \n",
            "\n",
            ". a propaganda poster depicting a cat\n",
            "a dolphin in an astronaut a teddy bear on a skateboard\n",
            "dressed as french emperor\n",
            "suit on saturn, artstation . . in times square\n",
            "napoleon holding a piece of cheese\n",
            "\n",
            "37 Source: \"Hierarchical Text-Conditional Image Generation with CLIP Latents,” arXiv, 2022.; Caption: The image displays a slide from a presentation on 'Text to Image Generation with Diffusion Based Models'. It includes three examples of generated images: a dolphin in an astronaut suit on Saturn, a cat dressed as a French emperor Napoleon holding a piece of cheese, and a teddy bear on a skateboard in Times Square. The slide also mentions OpenAI's DALL-E 2 as the source of these images and cites a paper titled 'Hierarchical Text-Conditional Image Generation with CLIP Latents' from arXiv 2022.\n",
            "Caption for /content/drive/MyDrive/pages/page_038.png-- OCR: = Diffusion Process EF Ree hm)\n",
            "e Diffusion models are inspired by non-equilibrium thermodynamics.\n",
            "\n",
            "e For asmall fraction of the time, it is difficult to determine whether particles\n",
            "are moving in the direction of mixing or in the opposite direction.\n",
            "\n",
            "=)\n",
            "%\n",
            "\n",
            " \n",
            "\n",
            "38 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a series of three beakers with red and blue particles, illustrating the diffusion process.\n",
            "Caption for /content/drive/MyDrive/pages/page_039.png-- OCR: / YJ Text to Image Generation C\n",
            "i with Diffusion Based Models\n",
            "\n",
            "m Diffusion Process\n",
            "e If we look at the movement of a single molecule on a very short time scale,\n",
            "\n",
            "it follows a Gaussian distribution.\n",
            "e Since the direction of mixing and the opposite direction are the same in a\n",
            "very short time, the opposite direction also follows a Gaussian distribution.\n",
            "\n",
            "“s MNGi)\n",
            "oS\n",
            "\n",
            "39 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a diagram of a molecule with four atoms connected in a chain, with arrows indicating the direction of movement. There is also a mathematical expression 'm_t ~ N(μ, σ)' next to the diagram.\n",
            "Caption for /content/drive/MyDrive/pages/page_040.png-- OCR: / YJ Text to Image Generation C\n",
            "i with Diffusion Based Models\n",
            "\n",
            "= Diffusion Process\n",
            "e Just as we viewed the molecule’s motion as a Gaussian-distributed noise\n",
            "\n",
            "AIA, °\n",
            "e + ZS =\n",
            "Molecule Noise Movement\n",
            "movement\n",
            "e We add a Gaussian-distributed noise to the pixel\n",
            "e + Cc = e\n",
            "Pixel Noise Pixel\n",
            "\n",
            "40 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a slide from a presentation on 'Text to Image Generation with Diffusion Based Models'. It includes a title, a section on the diffusion process, and two diagrams illustrating the process. The first diagram shows a molecule with arrows pointing outward, representing noise, which leads to a movement. The second diagram shows a pixel with Gaussian-distributed noise added to it, resulting in a pixel with movement. The source of the content is provided at the bottom of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_041.png-- OCR: =m Denoising diffusion models consist of two processes:\n",
            "e Forward diffusion process that gradually adds noise to input\n",
            "e Reverse denoising process that learns to generate data by denoising\n",
            "\n",
            "Forward diffusion process (fixed)\n",
            "\n",
            "Data Noise\n",
            "\n",
            " \n",
            "\n",
            "Reverse denoising process (generative)\n",
            "\n",
            "41 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a cat.\n",
            "Caption for /content/drive/MyDrive/pages/page_042.png-- OCR: YJ Denoising Diffusion Models C\n",
            "yi\n",
            "\n",
            "Sampling\n",
            "eS\n",
            "(Unconditional generation)\n",
            "\n",
            "- =|\n",
            "\n",
            "(unknown) data distribution known distribution\n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "42 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a diagram explaining Denoising Diffusion Models. It includes a representation of an unknown data distribution on the left, a red dot indicating a specific data point, and a known distribution on the right. There are arrows indicating the process of sampling (unconditional generation) from the unknown to the known distribution.\n",
            "Caption for /content/drive/MyDrive/pages/page_043.png-- OCR: 43\n",
            "\n",
            " \n",
            "\n",
            "Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image is a collage of various scenes and images, including a bear with a magnifying glass, a surreal landscape with a face, a person walking on a bridge, a cyclist on a road, a mountain landscape, a spider web, a lion, a person in a forest, a tent with a guitar, a car under a bridge, a beach with stones, and an eagle. There is also a small image of a colorful pattern on the left side, and a large image of a colorful pattern on the right side. The text 'Diffusion' is written above the small image, and 'Denoising Diffusion Models' is written at the top of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_044.png-- OCR: YJ Denoising Diffusion Models C\n",
            "yi\n",
            "\n",
            "= Forward diffusion process: Introducing noise\n",
            "\n",
            "e In the forward diffusion process, the small Gaussian noise is incrementally\n",
            "added to the distribution over T steps, resulting in a series of increasingly\n",
            "noisy samples.\n",
            "\n",
            "a> Ao, q(r+|t+-1) _— So,\n",
            "( [ ( \\ [ \\\n",
            "\\ a x sa — Uy 1 }____4| — >\" _ LT\n",
            "\n",
            "Yay i —\n",
            "\n",
            "a(X+|X+t-1) = N (xe; V1— B:xt-1, 6,1)\n",
            "\n",
            "mean yu variance o2\n",
            "\n",
            " \n",
            "\n",
            "44 Source: https://lilianweng.github.io/posts/2021-07-11 -diffusion-models/; Caption: The image shows a slide from a presentation on Denoising Diffusion Models. It includes a title, a bullet point explaining the forward diffusion process, a diagram illustrating the process, and a mathematical equation. The slide also contains a source URL at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_045.png-- OCR: YJ Denoising Diffusion Models C\n",
            "yi\n",
            "\n",
            "m Reverse denoising process: Removing noise\n",
            "\n",
            "e We can’t directly calculate q(x,_,|x,) because it involves complex data-\n",
            "related calculations. Instead, we model the reverse process with a neural\n",
            "network to estimate q(x,_4|X;).\n",
            "\n",
            "Po(Xt—1|Xz)\n",
            "@. -@=@, —,\n",
            "\n",
            "~oSae\n",
            "\n",
            " \n",
            "\n",
            "= N (at iio(ae, t), Xo (zz, t))\n",
            "\n",
            "Model\n",
            "\n",
            " \n",
            "\n",
            "45 Source: https://ilianweng.github.io/posts/2021-07-11-diffusion-models/; Caption: The image is a slide from a presentation on Denoising Diffusion Models. It includes a title, a bullet point explaining the reverse denoising process, and a diagram illustrating the process of removing noise from data. The diagram shows a sequence of images from a noisy image (XT) to a clean image (X0), with intermediate steps showing the denoising process. There is also a mathematical equation representing the reverse denoising process. The source of the content is provided at the bottom of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_046.png-- OCR: / YJ Text to Image Generation C\n",
            "i with Diffusion Based Models\n",
            "m™ Generating photorealistic images given any text input\n",
            "(Conditional generation)\n",
            "\n",
            " \n",
            "\n",
            "prccccccce\n",
            "hae manana =\n",
            "\n",
            "!\n",
            "!\n",
            "!\n",
            "1\n",
            "1\n",
            "1\n",
            "'\n",
            "4\n",
            "\n",
            " \n",
            "\n",
            "46 Source: https://sgvr.kaist.ac.kr/~sungeui/CG/Slides/slides_24/CS380_Talk3_Diffusion.pdf; Caption: The image shows a diagram illustrating the process of generating photorealistic images from text input using diffusion-based models.\n",
            "Caption for /content/drive/MyDrive/pages/page_047.png-- OCR: 47\n",
            "\n",
            "aN Example: Stable Diffusion De\n",
            "\n",
            "Latent Space\n",
            "\n",
            "Diffusion Process\n",
            "\n",
            "     \n",
            "\n",
            "denoising step crossattention switch = skip connection concat\n",
            "\n",
            "Source: “High-Resolution Image Synthesis with Latent Diffusion Models,” CVPR, 2022.; Caption: This image is a diagram illustrating the process of Stable Diffusion, a type of generative model used in artificial intelligence.\n",
            "Caption for /content/drive/MyDrive/pages/page_048.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Preliminaries\n",
            "\n",
            "m Generative Models\n",
            "= Transformers\n",
            "= Diffusion Models\n",
            "\n",
            "= Multimodal Embedding Spaces\n",
            "\n",
            "48; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is divided into three main sections, each with bullet points. The first section is titled 'Preliminaries' and includes two sub-points: 'Generative Models' and 'Multimodal Embedding Spaces.' Under 'Generative Models,' there are two further sub-points: 'Transformers' and 'Diffusion Models.' The second section is titled 'AI Agent Basics' and lists three sub-points: 'Acting,' 'Tool Use,' and 'Memory.' The slide number '48' is visible at the bottom. The slide has a simple design with a white background and black text, with the title in a larger font size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_049.png-- OCR: 49\n",
            "\n",
            " \n",
            "\n",
            "Multimodal embedding spaces are typically learned by\n",
            "collecting large amounts of paired multimodal data from the\n",
            "Internet, often images and text, and learning encoders for\n",
            "each modality using a pairwise similarity objective.\n",
            "\n",
            "PV ee) ew ae “aes | ee menos in the\n",
            "WY CC eee LU “| &..\n",
            "\n",
            " \n",
            "\n",
            "MODAL DATASETS\n",
            "\n",
            "Lael Theat ne lars; Caption: The image shows a slide from a presentation with the title 'Multimodal Embedding Spaces'. It includes a brief explanation of multimodal embedding spaces, stating that they are learned by collecting large amounts of paired multimodal data from the Internet, often images and text, and learning encoders for each modality using a pairwise similarity objective. Below the text, there are two images: one of a young boy playing basketball and another of two dogs playing in the grass. The slide also references a dataset called 'LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL Datasets' by Romain Beaumont, dated 10 Oct, 2022. The dataset is described as having 5.85 billion CLIP-filtered image-text pairs, which is 14 times larger than LAION-400M, the previously biggest openly accessible image-text dataset in the world. The authors of the dataset are listed as Christoph Schuhmann, Richard Vencu, Romain Beaumont, Thea Coombes, Cade Gordon, Aarush Katka, Robert Kaczmarczyk, and Jenia Jitsev.\n",
            "Caption for /content/drive/MyDrive/pages/page_050.png-- OCR: \"an elephant in the\n",
            "\n",
            "     \n",
            "\n",
            "So\n",
            "\n",
            "erengeti\n",
            "\n",
            "'\n",
            "\n",
            "; 11.24, 0.76, ...J,\n",
            "\n",
            "> Lt, 2.81,\n",
            "\n",
            "(2.4, -.03, ...J\n",
            "\n",
            "Oo\n",
            "11.21, 0.91, ...J,\n",
            "\n",
            "> Lt, 2.75, J,\n",
            "\n",
            "12.04, -.i, 1\n",
            "\n",
            "   \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "“two dogs cunning across\n",
            "\n",
            "aa O° oO << a Frosty Field\"\n",
            "\n",
            "The multi-modal nature of MLLMs is powered by\n",
            "two encoder models (Text encoder and Image\n",
            "encoder) trained to “speak the same language”.\n",
            "\n",
            "50; Caption: The image shows a slide from a presentation on 'Multimodal Embedding Spaces' with a focus on 'Multimodal learning'. It includes a diagram of a text encoder and an image encoder, with examples of text inputs and corresponding image outputs. The slide also contains a visual representation of a multimodal embedding space with text and image embeddings connected by lines.\n",
            "Caption for /content/drive/MyDrive/pages/page_051.png-- OCR: Multimodal Embedding Spaces\n",
            "\n",
            "  \n",
            "\n",
            "¢ Multimodal learning: Generalization across any-to-any modalities\n",
            "RGB modalities Edge modalities [Neu rlPS 2024]\n",
            "\n",
            "RGB Color palette SAMedges Canny edges\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Geometric modalities) al i Pa a Text modalities\n",
            "3D human = 4 aoe oe\n",
            "‘poses Caption ‘Web text\n",
            "4 Intratonal\n",
            "Airport serves as\n",
            "7; ~~. “2 Cen oy or © ‘era\n",
            "faba tttttt Oy | Sees\n",
            "Norco” [vst |\n",
            "encoder decoder\n",
            "Semantic modalities Metadata modalities\n",
            "Bounding Semantic SAM Semantic Geometric Image\n",
            "boxes segmentation —_ instances. metadata © metadata © metadata\n",
            "Humans: 7 ceomerie. | foreres:staxsr2\n",
            "ojctnes: eave | | complenn: 559% | |Contact asm\n",
            "Feature map modalites Global feature modalities [qiiieol. “935, 2596 Sattraton: 2056\n",
            "ImageBind poEova ImageBind ~ 7 ~\n",
            "Cee ee features features\n",
            "(dense) (dense) (dense) ante (global),\n",
            "\n",
            " \n",
            "\n",
            "ae\n",
            "\n",
            "51 Source: “4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities,” NeurlPS, 2024.; Caption: Multimodal Embedding Spaces\n",
            "Caption for /content/drive/MyDrive/pages/page_052.png-- OCR: A q Observations [Google Research 2023] C\n",
            "mm\n",
            "\n",
            "= Open-web datasets used to train open text-to-image models\n",
            "suffer from significant issues.\n",
            "\n",
            "e To be specific, the captions in the LAION dataset, used to train vision-\n",
            "language models, come from alt HTML tags (Alttext) and often describes\n",
            "only a narrow aspect of the image, neglecting significant visual details.\n",
            "\n",
            "e For example, an image of a person can have as Alttext the name of the\n",
            "person and the name of the photographer, but not a description of their\n",
            "appearance, their clothes, their position, or the background.\n",
            "\n",
            "Source: “A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation,” arXiv, 2023.\n",
            "52; Caption: The image displays a slide from a presentation with the title 'Observations' and a reference to 'Google Research 2023'. It contains text discussing the issues with open-web datasets used to train open text-to-image models, specifically mentioning the LAION dataset and the limitations of alt HTML tags (Alttext) in providing detailed descriptions of images. The source of the information is cited as 'A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation,' arXiv, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_053.png-- OCR: 53\n",
            "\n",
            "Alttext\n",
            "\n",
            "2013 ducati monster 1100 evo diesel motorcycle\n",
            "photos and specifications. Black Bedroom Furniture\n",
            "Sets. Home Design Ideas\n",
            "\n",
            "Me\n",
            "\n",
            " \n",
            "\n",
            "Cover of Oregon Wine Press February 2019; Caption: The image shows a slide from a presentation with the title 'Alttext Examples (1/2)'. It features two examples of alt text. On the left, there is a photo of a 2013 Ducati Monster 1100 EVO diesel motorcycle with a caption describing it as a motorcycle with photos and specifications, black bedroom furniture sets, and home design ideas. On the right, there is a cover of the Oregon Wine Press from February 2019 with a caption describing it as a magazine cover with Indian fare and fine wine, a heavenly hot pursuit. The slide number is 53.\n",
            "Caption for /content/drive/MyDrive/pages/page_054.png-- OCR: 54\n",
            "\n",
            "Alttext\n",
            "\n",
            "You Want To Learn About What Human Anatomy The\n",
            "Skeletal System\n",
            "\n",
            " \n",
            "\n",
            "GraphicRiver Toucan Pattern 7750027; Caption: The image displays two examples of Alttext. On the left, there is a photograph of a child drawing a human skeleton on a cardboard box. On the right, there is a colorful image of a toucan pattern.\n",
            "Caption for /content/drive/MyDrive/pages/page_055.png-- OCR: YJ The RECAP Framework\n",
            "\n",
            "=\n",
            "\n",
            "= This work proposes to relabel the corpus with improved captions that are\n",
            "auto-generated by a custom I2T (image-to-text) model, so as to increase\n",
            "\n",
            "sample efficiency and allow a vision-language model to better understand the\n",
            "relations between captions and images.\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "   \n",
            "       \n",
            "      \n",
            "\n",
            "3. Train Image\n",
            "Generation Model\n",
            "\n",
            "  \n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "1a. Manually 2. Generate\n",
            "caption a captions\n",
            "sample\n",
            "\n",
            "1b. Finetune\n",
            "Captioning Model\n",
            "\n",
            "Figure 2. Schematic diagram of our method RECAP. In steps (1a) and (1b) we fine-tune an image-to-text captioning model on a small set of\n",
            "detailed human captions. In step (2) we use this fine-tuned model to recaption the images in the training dataset of a text-to-image model,\n",
            "and with this dataset, in step (3) we train an image generation model with the recaptioned dataset.\n",
            "\n",
            "55; Caption: The image is a slide from a presentation titled 'The RECAP Framework'. It includes a schematic diagram and a brief description of a method for improving image captions. The diagram illustrates a three-step process involving images, a manually captioned sample, and two models: a captioning model and an image generation model. The description explains that the method aims to enhance the corpus with auto-generated captions to increase sample efficiency and improve the vision-language model's understanding of captions and images.\n",
            "Caption for /content/drive/MyDrive/pages/page_056.png-- OCR: 56\n",
            "\n",
            "(given by human)\n",
            "\n",
            "Short Caption\n",
            "\n",
            "Long Caption\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "A blue willys gasser car\n",
            "\n",
            "Awillys gasser car in blue color. it is placed\n",
            "near the car shed on the floor. in the back, there\n",
            "is aman seated in the chair\n",
            "\n",
            "Aglass of iced tea\n",
            "\n",
            "Aglass of iced tea placed on a saucer decorated\n",
            "with mint leaves. it is located on the wood table; Caption: The image displays two examples of captions. On the left, there is a photo of a blue Willys Gasser car with a short caption and a long caption below it. On the right, there is a photo of a glass of iced tea with a short caption and a long caption below it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_057.png-- OCR: AN\n",
            "\n",
            "57\n",
            "\n",
            "Alttext\n",
            "\n",
            "RECAP Short\n",
            "\n",
            "RECAP Long\n",
            "\n",
            "Dera alg\n",
            "002s Sy\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "2013 ducati monster 1100 evo diesel motorcycle\n",
            "photos and specifications. Black Bedroom Furniture\n",
            "Sets. Home Design Ideas\n",
            "\n",
            "Agreen benelli motorcycle\n",
            "A modern motorcycle with a combination of black,\n",
            "\n",
            "grey, and green colors. it is placed on a black\n",
            "background\n",
            "\n",
            "Cover of Oregon Wine Press February 2019\n",
            "\n",
            "A poster of food served on plates\n",
            "\n",
            "A black plate filled with a variety of indian food\n",
            "items along with a glass of white wine. the\n",
            "caption reads \"spice it up\"; Caption: The image is a slide from a presentation, featuring two photographs and their corresponding descriptions. On the left, there is a black and white image of a Ducati Monster 1100 Evo diesel motorcycle, with a caption that reads '2013 ducati monster 1100 evo diesel motorcycle photos and specifications. Black Bedroom Furniture Sets. Home Design Ideas.' Below the image, there are two sections labeled 'RECAP Short' and 'RECAP Long,' with brief descriptions of the motorcycle. On the right, there is a colorful image of a plate of Indian food with a glass of white wine, and a caption that reads 'Cover of Oregon Wine Press February 2019. A poster of food served on plates. A black plate filled with a variety of indian food items along with a glass of white wine. the caption reads \"spice it up\".' Below the image, there are two sections labeled 'RECAP Short' and 'RECAP Long,' with brief descriptions of the poster. The slide number '57' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_058.png-- OCR: AN\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Alttext You Want To Learn About What Human Anatomy The GraphicRiver Toucan Pattern 7750027\n",
            "Skeletal System\n",
            "\n",
            "RECAP Short A girl making a human skeleton model Floral and toucans seamless pattern\n",
            "\n",
            "RECAP Long A girl who is kneeling down and drawing a human Aseamless vector pattern with toucan birds and\n",
            "skeleton model. the skeleton model is made of tropical flowers. the background color is beige\n",
            "cardboard\n",
            "\n",
            "58; Caption: The image shows a girl making a human skeleton model out of cardboard.\n",
            "Caption for /content/drive/MyDrive/pages/page_059.png-- OCR: 59\n",
            "\n",
            "A pizza near\n",
            "\n",
            "a pineapple\n",
            "\n",
            "A person made\n",
            "of cutlery\n",
            "\n",
            "Two flowers,\n",
            "one ic blue\n",
            "and the other\n",
            "i¢ green\n",
            "\n",
            " \n",
            "\n",
            "Base Model; Caption: The image displays a collection of photographs arranged in a grid format, each representing different concepts. The top row shows a pizza near a pineapple, the middle row depicts a person made of cutlery, and the bottom row illustrates two flowers, one blue and the other green. Each row has two images, one labeled 'Base Model' and the other 'RECAP'. The images are clear and well-lit, showcasing the subjects from various angles.\n",
            "Caption for /content/drive/MyDrive/pages/page_060.png-- OCR: A red bench\n",
            "anda\n",
            "yellow clock\n",
            "\n",
            "A car driving\n",
            "through a hole\n",
            "in a huge tree\n",
            "\n",
            "A kitchen with\n",
            "a wooden table\n",
            "and a white\n",
            "refrigerator\n",
            "\n",
            " \n",
            "\n",
            "RECAP\n",
            "\n",
            "Base Model\n",
            "\n",
            "60; Caption: This image is a collage of various pictures.\n",
            "Caption for /content/drive/MyDrive/pages/page_061.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Preliminaries\n",
            "\n",
            "m Generative Models\n",
            "= Transformers\n",
            "= Diffusion Models\n",
            "\n",
            "= Multimodal Embedding Spaces\n",
            "= Al Agent Basics\n",
            "e Acting\n",
            "e Tool Use\n",
            "e Memory\n",
            "\n",
            "61; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is structured with bullet points and sub-bullet points, indicating a list of topics to be covered in the lecture. The main topics are 'Preliminaries,' 'Generative Models,' 'Multimodal Embedding Spaces,' and 'AI Agent Basics.' Under 'Generative Models,' there are two subtopics: 'Transformers' and 'Diffusion Models.' Under 'AI Agent Basics,' there are three subtopics: 'Acting,' 'Tool Use,' and 'Memory.' The slide number '61' is visible at the bottom, suggesting this is part of a larger presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_062.png-- OCR: YJ MLLMs as Agent Models C\n",
            "mA\n",
            "\n",
            "Level 2: Using MLLM to reason to act Level 1: Using MLLM to act\n",
            "(reasoning) (acting)\n",
            "\n",
            " \n",
            "  \n",
            " \n",
            "\n",
            "Action €L\n",
            "\n",
            "nn &\n",
            "- Observation € L\n",
            "\n",
            "t\n",
            "om xX Tol Use\n",
            "\n",
            "62 Source: https://rdi.berkeley.edu/lIm-agents/f24\n",
            "\n",
            " \n",
            "\n",
            "Explored in the later lectures\n",
            "\n",
            "Memory; Caption: The image is a slide from a presentation titled 'MLLMs as Agent Models'. It is divided into two levels, Level 1 and Level 2, with Level 1 being 'Using MLLM to act (acting)' and Level 2 being 'Using MLLM to reason to act (reasoning)'. There is a robot icon in the center, with arrows pointing to and from it, indicating a flow of action and observation. Below the robot, there are two icons representing 'Memory' and 'Tool Use'. The slide also includes a source link at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_063.png-- OCR: YJ Agent Architecture C\n",
            "mm\n",
            "\n",
            "Level 2: Using MLLM to reason to act Level 1: Using MLLM to act\n",
            "(reasoning) (acting)\n",
            "\n",
            "Action €L\n",
            "\n",
            "  \n",
            "\n",
            "OO\n",
            "onan\n",
            "\n",
            "<<\n",
            "Observation € L\n",
            "\n",
            "$\n",
            "y 4 Tool Use\n",
            "\n",
            "63 Source: https://rdi.berkeley.edu/lIm-agents/f24; Caption: The image is a slide from a presentation on Agent Architecture, specifically focusing on Level 2: Using MLLM to reason to act (reasoning) and Level 1: Using MLLM to act (acting). It includes a diagram with a robot, a brain, and a wrench, representing memory and tool use, and arrows indicating the flow of action and observation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_064.png-- OCR: YJ MLLMs as Agent Models C\n",
            "mA\n",
            "\n",
            "¢ MLLMs predict model-specific tokens\n",
            "(W1,W2,...,We) — West\n",
            "¢ Al Agents predict observation-action sequences\n",
            "¢ Policy model: (01, 41,02, a2, ...,0¢) — a\n",
            "¢ World model: (01, a1, 02, Az, .-, Op, Ap) — O¢44; Caption: The image is a slide from a presentation titled 'MLLMs as Agent Models'. It contains text explaining the concept of MLLMs (Multimodal Language Models) as agent models. The slide is divided into two main sections. The first section discusses how MLLMs predict model-specific tokens, represented by a mathematical expression (w1, w2, ..., wt) → wt+1. The second section explains how AI Agents predict observation-action sequences, with two sub-sections detailing the policy model and the world model. The policy model is represented by the expression (o1, a1, o2, a2, ..., ot, at) → at, and the world model by (o1, a1, o2, a2, ..., ot, at) → ot+1. The slide number '64' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_065.png-- OCR: ¢ The model can drive in the simulator with a driving\n",
            "plan predicted entirely from imagination.\n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "o” Punto\n",
            "| v { i\n",
            "i I\n",
            "\n",
            "RGB input (time t) Ground truth BEV (time t) Pred. BEV (time t)\n",
            "\n",
            "RGB input Pred. acceleration; —_______ ==} 0,34\n",
            ": . Pred. steering: —SSS 0.68\n",
            "When the RGB input becomes sepia-coloured,\n",
            "\n",
            "the model is driving in imagination. Ground truth bird's-eye view Predicted bird’s-eye view\n",
            "\n",
            "semantic segmentation segmentation\n",
            "65; Caption: The image shows a simulation of a car driving on a road with buildings on the sides.\n",
            "Caption for /content/drive/MyDrive/pages/page_066.png-- OCR: YJ MLLMs as Agent Models C\n",
            "mA\n",
            "\n",
            "¢ MLLMs predict model-specific tokens\n",
            "(W1,W2,...,We) — West\n",
            "¢ Al Agents predict observation-action sequences\n",
            "¢ Policy model: (01, 41,02, a2, ...,0¢) — a\n",
            "¢ World model: (01, a1, 02, Az, .-, Op, Ap) — O¢44\n",
            "¢ MLLMs are agent models if we can\n",
            "¢ Tokenize observations\n",
            "¢ Tokenize actions; Caption: The image is a slide from a presentation titled 'MLLMs as Agent Models'. It contains text and mathematical notations explaining the concept of MLLMs (Multi-Layer Language Models) as agent models. The slide lists three bullet points: MLLMs predict model-specific tokens, AI Agents predict observation-action sequences, and MLLMs are agent models if we can tokenize observations and actions. There are also two mathematical notations provided for the policy model and world model. The slide number is 66.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_067.png-- OCR: A Example:\n",
            "= RT-2: Vision-Language-Action Models [PLMR 2023]\n",
            "© Google DeepMind\n",
            "\n",
            "Q: What should\n",
            "the robot do to\n",
            "<task>? At...\n",
            "\n",
            "  \n",
            "    \n",
            "\n",
            "Robot action\n",
            "\n",
            "67; Caption: The image displays a slide from a presentation, specifically slide number 67, which is part of a section titled 'Example: RT-2: Vision-Language-Action Models [PLMR 2023]'. The slide is related to Google DeepMind and illustrates a process involving a robot performing a task. The slide includes a diagram that represents the components of the RT-2 model, which is a large language model. The diagram shows the flow from a vision-language-action model (ViT) to the RT-2 model, and then to the robot action, which is represented by a set of numbers and angles. The slide also contains a question 'Q: What should the robot do to <task>?' followed by a series of arrows pointing to the robot action. Additionally, there are two sets of numbers and angles labeled 'A:' and 'Δ T = [0.1, -0.2, 0]' and 'Δ R = [10°, 25°, -7°]' respectively.\n",
            "Caption for /content/drive/MyDrive/pages/page_068.png-- OCR: ; Caption: A robot arm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_069.png-- OCR: A Example:\n",
            "= RT-2: Vision-Language-Action Models [PLMR 2023]\n",
            "\n",
            " \n",
            "     \n",
            "   \n",
            "   \n",
            " \n",
            "  \n",
            "      \n",
            "\n",
            "Transfer Web Knowledge to Robotic Control\n",
            "\n",
            "Internet-Scale V@A + Robot Action Data\n",
            "\n",
            "Co-Fine-Tune\n",
            "Q: What is happening\n",
            "in the image?\n",
            "\n",
            " \n",
            "\n",
            "Vision-Language-\n",
            "Action Models for\n",
            "\n",
            "Q: Que, pulse) faite Robot Control\n",
            "\n",
            "avec ces objets?\n",
            "RT-2\n",
            "\n",
            " \n",
            "\n",
            "Q: What should the\n",
            "robot do to <task>?\n",
            "\n",
            "A Translation = (0.1, -0.2, 0) Deploy\n",
            "\n",
            "4 Rotation = [10°, 25°, -7°]\n",
            "\n",
            " \n",
            "\n",
            "Closed-Loop Robot Control\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "Put the strawberry into\n",
            "the correct bowl\n",
            "\n",
            "Pick the nearly falling\n",
            "bog\n",
            "\n",
            "Pick object that is\n",
            "different\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "69; Caption: The image displays a slide from a presentation, specifically slide number 69, which is part of a section titled 'Example: RT-2: Vision-Language-Action Models [PLMR 2023]'. The slide is divided into two main sections. On the left, there is a series of images with corresponding questions and answers in multiple languages, demonstrating the application of vision-language-action models. The images depict various scenarios, such as a donkey walking down the street, a person making a cup of tea, and a robot performing tasks like picking an object from a table. On the right, there is a flowchart that outlines the process of transferring web knowledge to robotic control, with steps labeled 'Co-Fine-Tune' and 'Deploy'. The flowchart includes a robot icon and a box with text and coordinates, indicating the translation and rotation parameters for the robot's action. The slide also includes a section titled 'Closed-Loop Robot Control' with images of a robot performing tasks such as putting a strawberry into a bowl, picking a nearly falling bag, and picking an object that is different.\n",
            "Caption for /content/drive/MyDrive/pages/page_070.png-- OCR: YJ Agent Architecture C\n",
            "mm\n",
            "\n",
            "Level 2: Using MLLM to reason to act Level 1: Using MLLM to act\n",
            "(reasoning) (acting)\n",
            "\n",
            "Action €L\n",
            "\n",
            "  \n",
            "\n",
            "OO\n",
            "onan\n",
            "\n",
            "<<\n",
            "Observation € L\n",
            "\n",
            "$\n",
            "y 4 Tool Use\n",
            "\n",
            "70 Source: https://rdi.berkeley.edu/lIm-agents/f24; Caption: The image is a slide from a presentation on Agent Architecture, specifically focusing on Level 2: Using MLLM to reason to act (reasoning) and Level 1: Using MLLM to act (acting). It includes a diagram with a robot, a brain, a wrench, and a globe, representing the flow of action and observation in an agent's decision-making process. The source of the content is provided at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_071.png-- OCR: | CID\n",
            "mm\n",
            "\n",
            "¢ MLLMs predict model-specific tokens\n",
            "\n",
            "(Wy, W2, ,We) > Wey\n",
            "\n",
            "ez\n",
            "\n",
            "Sometimes, invoking ‘tool calls’ is necessary to\n",
            "accomplish the ‘prediction’ task.\n",
            "\n",
            "Special tokens can be exploited to invoke tool calls for\n",
            "¢ Search engine, calculator, etc.\n",
            "\n",
            "* Task-specific models (translation)\n",
            "* APIs\n",
            "\n",
            "71; Caption: The image is a slide from a presentation, titled 'Tool Use'. It discusses the use of Machine Learning Language Models (MLLMs) to predict model-specific tokens. The slide explains that sometimes, 'tool calls' are necessary to accomplish the 'prediction' task. It lists examples of special tokens that can be used to invoke tool calls, such as search engines, calculators, task-specific models like translation, and APIs. There is a red arrow pointing to the text 'Sometimes, invoking “tool calls” is necessary to accomplish the “prediction” task.' The slide number is 71.\n",
            "Caption for /content/drive/MyDrive/pages/page_072.png-- OCR: YJ Tool Use: Example\n",
            "mm\n",
            "\n",
            "72\n",
            "\n",
            "  \n",
            "\n",
            "Language Model\n",
            "~ input\n",
            "\n",
            "Tool Augmented Language Model\n",
            "\n",
            "| input — toolinput || toolresult —! output |\n",
            "call append\n",
            "\n",
            "external Lo wy J tool\n",
            "tool result An abstract task:\n",
            "\n",
            "task input text |¢ool-call tool input text |result tool out-\n",
            "put text |output task output text\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "A weather task:\n",
            "how hot will it get in NYC today? |weather lookup re-\n",
            "\n",
            "gion=NYC |result precipitation chance: 10, high temp:\n",
            "20c, low-temp: 12c |output today’s high will be 20C\n",
            "\n",
            " \n",
            "\n",
            "Source: “TALM: Tool Augmented Language Models,” arXiv, 2022.; Caption: The image displays a slide from a presentation, specifically slide number 72, which is titled 'Tool Use: Example'. It features a diagram comparing a 'Language Model' with a 'Tool Augmented Language Model'. The Language Model diagram shows a simple flow from 'input' to 'output'. The Tool Augmented Language Model diagram is more complex, showing an additional step where 'tool input' is processed by an 'external tool' to produce 'tool result', which then combines with the 'input' to produce the 'output'. Below the diagrams, there are two examples of tasks: 'An abstract task' and 'A weather task', each with a structured input and output format. The source of the content is cited as 'TALM: Tool Augmented Language Models,' from arXiv, 2022.\n",
            "Caption for /content/drive/MyDrive/pages/page_073.png-- OCR: YJ Toolformer: Language Models Can Teach\n",
            "mm\n",
            "\n",
            " \n",
            "\n",
            "Themselves to Use Tools [NeurlPS 2023]\n",
            "\n",
            "Key steps: Given an input text x, we first sample a position / and\n",
            "corresponding API call candidates c’;, c?;,..., cX. We then execute these\n",
            "API calls and filter out all calls which do not reduce the loss L; over the next\n",
            "tokens. All remaining API calls are interleaved with the original text,\n",
            "resulting in a new text x*.\n",
            "\n",
            "LM Dataset 1 2 3 LM Dataset\n",
            "Sample API Calls Execute API Calls Filter API Calls with API Calls\n",
            "X,.;., = Pittsburgh is c;* = What other name is r; = Steel City L{c;* — Steel City) x\" = Pittsburgh is\n",
            "~~ also known as Pittsburgh known by? < min(L.(c— ¢), L(e)) also known as\n",
            "a ‘ [QA(What ...?\n",
            "X;,, = the Steel City ¢2 = Which country is 7 = United States Lc? United States) — Steel City)]\n",
            "\n",
            "Pittsburgh in? > min(L,(c? — €), L,(e)) the Steel City.\n",
            "\n",
            "73; Caption: The image is a slide from a presentation titled 'Toolformer: Language Models Can Teach Themselves to Use Tools' at NeurIPS 2023. It outlines the key steps in a process where an input text is given, API calls are sampled and executed, and then filtered to result in a new text. The slide includes a flowchart with three main steps: 'Sample API Calls,' 'Execute API Calls,' and 'Filter API Calls,' leading to the final output of an 'LM Dataset with API Calls.' The slide also contains text explaining the process in detail and examples of API calls related to the input text.\n",
            "Caption for /content/drive/MyDrive/pages/page_074.png-- OCR: YJ Example Predictions of Toolformer\n",
            "mA\n",
            "\n",
            " \n",
            "\n",
            "The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The New England Journal\n",
            "of Medicine?”) — Massachusetts Medical Society] the MMS.\n",
            "\n",
            "Out of 1400 participants, 400 (or [Calculator(400 / 1400) — 0.29] 29%) passed the test.\n",
            "The name derives from “la tortuga’”, the Spanish word for [MT(“tortuga”) — turtle] turtle.\n",
            "The Brown Act is California's law [WikiSearch(“Brown Act”) — The Ralph M. Brown Act is an act of the California State\n",
            "\n",
            "Legislature that guarantees the public's right to attend and participate in meetings of local legislative bodies.] that\n",
            "requires legislative bodies, like city councils, to hold their meetings open to the public.\n",
            "\n",
            "74; Caption: The image displays a slide titled 'Example Predictions of Toolformer' with a list of statements and their corresponding predictions. The slide background is white with a blue header and footer. The content is organized in bullet points with some text highlighted in different colors.\n",
            "Caption for /content/drive/MyDrive/pages/page_075.png-- OCR: YJ Agent Architecture C\n",
            "mm\n",
            "\n",
            "Level 2: Using MLLM to reason to act Level 1: Using MLLM to act\n",
            "(reasoning) (acting)\n",
            "\n",
            "Action €L\n",
            "\n",
            "  \n",
            "\n",
            "OO\n",
            "onan\n",
            "\n",
            "<<\n",
            "Observation € L\n",
            "\n",
            "$\n",
            "y 4 Tool Use\n",
            "\n",
            "75 Source: https://rdi.berkeley.edu/lIm-agents/f24; Caption: The image is a slide from a presentation on Agent Architecture, specifically focusing on Level 2: Using MLLM to reason to act (reasoning) and Level 1: Using MLLM to act (acting). It includes a diagram with a robot, a brain, and a wrench, representing memory and tool use, and arrows indicating the flow of action and observation.\n",
            "Caption for /content/drive/MyDrive/pages/page_076.png-- OCR: AN\n",
            "\n",
            "  \n",
            "\n",
            "Write Action ¥\n",
            "> 5305\n",
            "_ .\n",
            "Read Observation\n",
            "A long-term memory\n",
            "\n",
            "* Stores experience,\n",
            "\n",
            "skills, knowledge... Instruction: ... A “short-term memory”\n",
            "* Persist over new Thought: ... ¢ Append-only\n",
            "experience Action: ... ¢ Limited context\n",
            "\n",
            "Obs: ... ¢ Limited attention\n",
            "Thought: ... * Do not persist over\n",
            "new tasks\n",
            "\n",
            " \n",
            "\n",
            "76; Caption: Page showing memory.\n",
            "Caption for /content/drive/MyDrive/pages/page_077.png-- OCR: iY Transforming Experience into Long-Term Memory\n",
            "=\n",
            "\n",
            "o\n",
            "©\n",
            "°7\" OH WOW!\n",
            "IGOT IT! O\n",
            "ce\n",
            "°o\n",
            "\n",
            " \n",
            "   \n",
            "\n",
            "     \n",
            "\n",
            "* Humans learn from prior mistakes\n",
            "to perform better on a task.\n",
            "\n",
            "- Self-reflection (ARRB) isa\n",
            "mechanism to build a long-term\n",
            "memory of self-reflective\n",
            "experiences which allows humans\n",
            "to identify their own errors and\n",
            "self-suggest lessons to learn from y\n",
            "\n",
            "their mistakes over time. SS\n",
            "\n",
            "THE TRAGEDY OF A THREE SECOND MEMORY\n",
            "\n",
            "     \n",
            "   \n",
            " \n",
            "\n",
            "  \n",
            "\n",
            "WHAT WAS I\n",
            "THINKING ABOUT?\n",
            "©\n",
            "\n",
            "  \n",
            "\n",
            "+O\n",
            "\n",
            "77; Caption: The image is a comic strip titled 'THE TRAGEDY OF A THREE SECOND MEMORY' from SPUDCOMICS.COM, created by LONNIE EASTERLING in 2010. It features a cartoon fish with a surprised expression in the first panel, followed by a panel with a complex mathematical equation, and the last panel shows the fish with a puzzled expression. The comic strip is used to illustrate the concept of transforming experiences into long-term memory, specifically highlighting the role of self-reflection in learning from mistakes.\n",
            "Caption for /content/drive/MyDrive/pages/page_078.png-- OCR: YJ Transforming into Long-Term Memory\n",
            "yi\n",
            "\n",
            "» re ee\n",
            "Read & —— Observation\n",
            "A long-term memory\n",
            "\n",
            "For agents, however, generating useful reflective feedback is\n",
            "\n",
            "challenging since it requires:\n",
            "\n",
            "* agood understanding of where the model made mistakes;\n",
            "\n",
            "¢ the ability to generate a summary containing actionable insights for\n",
            "improvement.\n",
            "\n",
            "78; Caption: Page showing information about long term memory.\n",
            "Caption for /content/drive/MyDrive/pages/page_079.png-- OCR: ‘YJ Reflexion [NeurlPS 2023] C\n",
            "mm\n",
            "\n",
            "79\n",
            "\n",
            "Reflexion uses verbal reinforcement to help agents learn from\n",
            "prior failings, i.e., converting binary (e.g., success/fail) or scalar\n",
            "feedback from the environment into verbal feedback in the form\n",
            "of a textual summary, which is then added as additional context\n",
            "for the MLLM agent in the next episode.\n",
            "\n",
            "Reflexion utilizes three distinct models:\n",
            "\n",
            "¢ Actor: generating text and actions\n",
            "\n",
            "¢ Evaluator: scoring the outputs produced by the Actor\n",
            "\n",
            "¢ Self-Reflection model: generating verbal reinforcement cues to\n",
            "assist the Actor in self-improvement.\n",
            "\n",
            "Source: “Reflexion: Language Agents with Verbal Reinforcement Learning,” NeurIPS, 2023.; Caption: The image is a slide from a presentation titled 'Reflexion [NeurIPS 2023]'. It contains text explaining the concept of Reflexion, which is a method that uses verbal reinforcement to help agents learn from prior failures. The slide describes how Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the MLLM agent in the next episode. The slide also lists three distinct models used in Reflexion: Actor, Evaluator, and Self-Reflection model. The source of the information is cited as 'Reflexion: Language Agents with Verbal Reinforcement Learning,' NeurIPS, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_080.png-- OCR: YJ The Reflexion process C\n",
            "mm\n",
            "\n",
            "¢ Initialization\n",
            "¢ The Actor produces a trajectory Tt, by interacting with the\n",
            "environment.\n",
            "¢ The Evaluator then produces a score 79 which is computed as T; =\n",
            "M.(t), where tT; is only a scalar reward for trial t.\n",
            "¢ The Self-Reflection model analyzes the set of {t9, 79} to produce a\n",
            "summary s7p which is stored in the memory mem, where s7; is a\n",
            "verbal experience feedback for trial t.\n",
            "¢ Repeat\n",
            "¢ The Actor, Evaluator, and Self-Reflection model work together\n",
            "through trials in a loop until the Evaluator deems sr; to be correct.\n",
            "Note that after each trial t, sr;, is appended mem.\n",
            "\n",
            "80; Caption: The image is a slide from a presentation, titled 'The Reflexion process'. It contains text explaining the initialization and repeat steps of a process involving an Actor, Evaluator, and Self-Reflection model. The slide is numbered 80 at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_081.png-- OCR: The Reflexion process\n",
            "\n",
            "External feedback\n",
            "Self-reflection (LM)\n",
            "Reflective\n",
            "\n",
            "Internal\n",
            "feedback text\n",
            "\n",
            "Evaluator (LM) Experience\n",
            "(long-term memory)\n",
            "tiemmemoy) | _Aaort)\n",
            "(short-term memory) Actor (LM)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "~ Obs / Reward Acton\n",
            "\n",
            "81; Caption: The image is a slide from a presentation titled 'The Reflexion process'. It features a flowchart that outlines the components and processes involved in an agent's reflexion process. The flowchart is divided into several sections with labeled boxes and arrows indicating the flow of information. The main components include 'Agent', 'External feedback', 'Internal feedback', 'Self-reflection (LM)', 'Reflective text', 'Evaluator (LM)', 'Experience (long-term memory)', 'Actor (LM)', 'Trajectory (short-term memory)', 'Obs / Reward', 'Environment', and 'Action'. The slide number '81' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_082.png-- OCR: 82\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "You are in the middle of a room. Looking quickly around you, you see a drawer 2,\n",
            "a shelf 5, a drawer 1, a shelf 4, a sidetable 1, a drawer 5, a shelf 6, a shelf\n",
            "1, a shelf 9, a cabinet 2, a sofa 1, a cabinet 1, a shelf 3, a cabinet 3, a\n",
            "drawer 3, a shelf 11, a shelf 2, a shelf 10, a dresser 1, a shelf 12, a\n",
            "garbagecan 1, a armchair 1, a cabinet 4, a shelf 7, a shelf 8, a safe 1, and a\n",
            "drawer 4.\n",
            "\n",
            "Your task is to: put some vase in safe.\n",
            "\n",
            "> go to shelf 6\n",
            "You arrive at loc 4. On the shelf 6, you see a vase 2.\n",
            "\n",
            "> take vase 2 from shelf 6\n",
            "You pick up the vase 2 from the shelf 6.\n",
            "\n",
            "> go to safe 1\n",
            "You arrive at loc 3. The safe 1 is closed.\n",
            "\n",
            "> open safe 1\n",
            "You open the safe 1. The safe 1 is open. In it, you see a keychain 3.\n",
            "\n",
            "> put vase 2 in/on safe 1\n",
            "You won!\n",
            "\n",
            " \n",
            "\n",
            "Source: https://alfworld.github.io/; Caption: The image shows a screenshot of a computer interface with a list of instructions for a task.\n",
            "Caption for /content/drive/MyDrive/pages/page_083.png-- OCR: AN\n",
            "\n",
            "83\n",
            "\n",
            " \n",
            "\n",
            "A decision-making example\n",
            "\n",
            " \n",
            "\n",
            "Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk\n",
            "2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a\n",
            "laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.\n",
            "\n",
            "Task: examine the mug with the desklamp.\n",
            "\n",
            "Task: examine the mug — with the desklamp; Caption: The image shows a slide with a title 'A decision-making example' and a task description. The task is to 'examine the mug with the desklamp.' There are three icons: a magnifying glass, a mug, and a desk lamp.\n",
            "Caption for /content/drive/MyDrive/pages/page_084.png-- OCR: Trial #1\n",
            "\n",
            "> think: To solve the task, I need to find and take a mug, then find and use a desklamp.\n",
            "\n",
            "> think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf\n",
            "(1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.\n",
            "\n",
            "> go to drawer 1\n",
            "\n",
            "The drawer | is closed.\n",
            "\n",
            "> open drawer 6\n",
            "\n",
            "You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.\n",
            "\n",
            "> go to desk 1\n",
            "\n",
            "On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\n",
            "\n",
            "> think: Now I find a mug (1). Next, I need to take it.\n",
            "\n",
            "> take mug 1 from desk 1\n",
            "\n",
            "You pick up the mug 1 from the desk 1.\n",
            "\n",
            "> think: Now I take a mug (1). Next, I need to find a desklamp. A desklamp is more likely to appear\n",
            "in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with\n",
            "desk 1.\n",
            "\n",
            "> go to desk 1\n",
            "\n",
            "Nothing happens.\n",
            "\n",
            "> go to desk 2\n",
            "\n",
            "On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2.\n",
            "\n",
            "> think: Now I find a desklamp (1). Next, I need to use it.\n",
            "\n",
            "> use desklamp 1\n",
            "\n",
            "Nothing happens.\n",
            "\n",
            "> use desklamp 1\n",
            "\n",
            "Nothing happens.\n",
            "\n",
            "Status: Fail\n",
            "\n",
            "Reflection: In this environment, my plan was to find a mug then find and use a desklamp. However,\n",
            "the task says to examine the mug with the desklamp. I should have looked for the desklamp first, then\n",
            "looked for the mug. I noticed that the desklamp was found on desk 1. In the next trial, I will go to\n",
            "desk 1, find the lamp, then look for the mug and examine it with the desklamp.; Caption: The image displays a digital interface with a 'GO' button at the top left corner, indicating the start of a task or game. Below the button, there is a list of instructions or steps, each preceded by a 'think:' prompt, suggesting a thought process or decision-making scenario. The steps involve searching for and using items such as a mug and a desklamp in a virtual environment. The text is accompanied by a red stamp that reads 'MISSION FAILED,' indicating that the task was not completed successfully. The background is white, and the text is primarily in black with some green highlights, possibly to emphasize certain actions or items. The overall layout is clean and organized, designed to guide the user through a series of actions.\n",
            "Caption for /content/drive/MyDrive/pages/page_085.png-- OCR: Trial #2\n",
            "\n",
            "> go to desk 1\n",
            "\n",
            "On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\n",
            "> think: To solve the task, I need to find and take a mug, then find and use a desklamp.\n",
            "\n",
            "> take mug 1 from desk 1\n",
            "\n",
            "You pick up the mug 1 from the desk 1.\n",
            "\n",
            "> think: To solve the task, I need to find and take a mug, then find and use a desklamp.\n",
            "\n",
            "> use desklamp 1\n",
            "\n",
            "You turn on the desklamp 1.\n",
            "\n",
            "Status: Success\n",
            "\n",
            "The agent is able to correct its reasoning trace and\n",
            "execute a sequence of actions in a concise manner!; Caption: The image displays a digital interface with a green 'GO' button at the top left corner, indicating the start of a process or trial. Below the button, there is a list of instructions for 'Trial #2' which involves going to desk 1, taking a mug, and using a desklamp. The instructions are followed by a series of actions and thoughts that lead to the successful completion of the task, as indicated by the 'MISSION ACCOMPLISHED' stamp in the bottom left corner. The text at the bottom right corner emphasizes the agent's ability to correct its reasoning trace and execute a sequence of actions concisely. The overall layout suggests a step-by-step guide or tutorial for completing a specific task.\n",
            "Caption for /content/drive/MyDrive/pages/page_086.png-- OCR: 86\n",
            "\n",
            "\\e More Reflection, Greater Success C\n",
            "\n",
            "ALFWorld Success Rate\n",
            "\n",
            "Proportion of Solved Environments\n",
            "\n",
            "°\n",
            "©\n",
            "\n",
            "°\n",
            "co\n",
            "\n",
            "°\n",
            "N\n",
            "\n",
            "°\n",
            "a\n",
            "\n",
            "°\n",
            "uw\n",
            "\n",
            "--- ReAct only\n",
            "—e ReAct + Reflexion (Heuristic)\n",
            "—— ReAct + Reflexion (GPT)\n",
            "\n",
            "0 2 4 6\n",
            "Trial Number\n",
            "\n",
            "oo\n",
            "\n",
            "   \n",
            "\n",
            "AlfWorld performance\n",
            "across 134 tasks showing\n",
            "cumulative proportions of\n",
            "solved tasks using self-\n",
            "evaluation techniques of\n",
            "(Heuristic) and (GPT) for\n",
            "binary classification.\n",
            "\n",
            "ReAct [ICLR 2023]; Caption: This image is a slide from a presentation, titled 'More Reflection, Greater Success'. It features a line graph titled 'ALFWorld Success Rate' that compares the performance of different AI techniques across 134 tasks. The graph shows three lines representing 'ReAct only', 'ReAct + Reflection (Heuristic)', and 'ReAct + Reflection (GPT)'. The x-axis is labeled 'Trial Number' and ranges from 0 to 10, while the y-axis is labeled 'Proportion of Solved Environments' and ranges from 0.5 to 1.0. The slide also includes a text box with a brief explanation of the graph and a citation '[ICLR 2023]'.\n",
            "Caption for /content/drive/MyDrive/pages/page_087.png-- OCR: YJ Voyager [TMLR 2024] C\n",
            "= A procedural memory of skills\n",
            "Objective:\n",
            "\n",
            "¢ Leveraging MLLMs (i.e., OpenAl’s APIs) for improved game\n",
            "mechanics understanding\n",
            "\n",
            "Innovative Features:\n",
            "¢ Introduction of automatic curriculum and skill library\n",
            "¢ Automatic curriculum proposes increasingly complex tasks\n",
            "¢ Skill library serves as a learning and evolution basis\n",
            "¢ Iterative prompting mechanism refines the program and\n",
            "incorporates it into the skill library as a new skill\n",
            "\n",
            "87 Source: “Voyager: An Open-Ended Embodied Agent with Large Language Models,” Transactions on Machine Learning Research (TMLR), 2024.; Caption: The image is a slide from a presentation, specifically slide number 87. It features a blue header with the word 'A' in white, followed by the title 'Voyager [TMLR 2024] A procedural memory of skills'. Below the title, there are two main sections: 'Objective' and 'Innovative Features'. The 'Objective' section states the aim to leverage MLLMs for improved game mechanics understanding. The 'Innovative Features' section lists three bullet points: the introduction of an automatic curriculum and skill library, the automatic curriculum proposing increasingly complex tasks, and the skill library serving as a learning and evolution basis. It also mentions an iterative prompting mechanism that refines the program and incorporates it into the skill library as a new skill. At the bottom, there is a source citation for the content.\n",
            "Caption for /content/drive/MyDrive/pages/page_088.png-- OCR: ; Caption: The image displays a scene from a video game, specifically a sandbox game like Minecraft. It features a large, open area with a clear blue sky. The terrain is composed of large, beige blocks that resemble stone or sand. There are various structures made of these blocks, including what appears to be a large staircase or ramp on the left side of the image. Additionally, there are smaller structures and possibly a small body of water or a pond in the center. The environment suggests a desert or arid landscape, typical of many sandbox games.\n",
            "Caption for /content/drive/MyDrive/pages/page_089.png-- OCR: YJ A Running Example (1/4) C\n",
            "mm\n",
            "| Afomatic Curcutum\n",
            "\n",
            "  \n",
            " \n",
            "\n",
            "  \n",
            " \n",
            "    \n",
            "\n",
            "Make Crafting Table\n",
            "\n",
            "New\n",
            "Combat\n",
            "Zombie\n",
            "\n",
            "Mine Wood Log\n",
            "\n",
            "Mine Diamond\n",
            "\n",
            " \n",
            "\n",
            "89; Caption: The image displays a circular flow diagram with four circular nodes connected by arrows, indicating a sequence of tasks. Each node contains a screenshot of a video game scene, suggesting that the tasks are related to gameplay. The top node shows a character in a snowy environment, the bottom left node shows a character in a forest, the bottom right node shows a character in a desert, and the top right node shows a character in a village. The arrows indicate a clockwise progression from one task to the next.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_090.png-- OCR: A Running Example (2/4)\n",
            "\n",
            "Automatic Curriculum\n",
            "\n",
            "/®:.\n",
            "\n",
            "Mine Wood Log\n",
            "\n",
            "90\n",
            "\n",
            " \n",
            "\n",
            "lake Crafting Table\n",
            "\n",
            "Combat\n",
            "Zombie\n",
            "\n",
            "Q\n",
            "\n",
            "Mine Diamond\n",
            "\n",
            " \n",
            "\n",
            "Iterative Prompting Mechanism\n",
            "\n",
            "async function combatZombie(bot) {\n",
            "// Equip a weapon\n",
            "const sword = bot.inventory.findInventoryItem(\n",
            "mcData. itemsByName[\"stone_sword\"].id);\n",
            "if (sword) {\n",
            "await bot.equip(sword, \"“hand\");}\n",
            "\n",
            "else {\n",
            "\n",
            "await craftStoneSword(bot);} .--------- x Skill\n",
            "// Craft and equip a shield Retrieval\n",
            "ewaithcrartshteld (bot) ease es ¥ <——\n",
            "\n",
            "ab & OKO @\n",
            "\n",
            " \n",
            "\n",
            "Skill Library\n",
            "\n",
            "Mine Wood Log\n",
            "Make Crafting Table\n",
            "Craft Stone Sword\n",
            "Make Furnace\n",
            "\n",
            "Craft Shield\n",
            "\n",
            "Cook Steak\n",
            "\n",
            "Combat Zombie; Caption: The image is a slide from a presentation, titled 'A Running Example (2/4)'. It is divided into three main sections: 'Automatic Curriculum', 'Iterative Prompting Mechanism', and 'Skill Library'. The 'Automatic Curriculum' section shows a circular flow diagram with four tasks: 'Make Crafting Table', 'Combat Zombie', 'Mine Wood Log', and 'Mine Diamond'. Each task is represented by an image and is connected in a clockwise direction, indicating a sequence. The 'Iterative Prompting Mechanism' section contains a code snippet with a function named 'combatZombie(bot)' that includes steps to equip a weapon and craft a shield. The 'Skill Library' section lists various skills such as 'Mine Wood Log', 'Make Crafting Table', 'Craft Stone Sword', 'Make Furnace', 'Craft Shield', 'Cook Steak', and 'Combat Zombie'. The slide number '90' is visible at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_091.png-- OCR: A Running Example (3/4)\n",
            "\n",
            " \n",
            "\n",
            "Automatic Curriculum Iterative Prompting Mechanism\n",
            "\n",
            " \n",
            "\n",
            "async function combatZombie(bot) {\n",
            "// Equip a weapon\n",
            "const sword = bot.inventory.findInventoryItem(\n",
            "mcData. itemsByName[\"stone_sword\"].id);\n",
            "if (sword) {\n",
            "M\n",
            "\n",
            "await bot.equip(sword, \"“hand\");}\n",
            "jake Crafting Table Cea\n",
            "await craftStoneSword(bot);} .--------- x Skill\n",
            "Combat // Craft and equip a shield Retrieval\n",
            "await craftShield(bot);  .---------------. <_—_\n",
            "Zombie — ¥\n",
            "\n",
            " \n",
            "\n",
            "Mine Wood Log\n",
            "\n",
            " \n",
            "     \n",
            "\n",
            "Env Feedback Code as\n",
            "is Execution Errors Actions\n",
            "\n",
            "Mine Diamond\n",
            "Exploration\n",
            "Progress\n",
            "\n",
            "Environment\n",
            "\n",
            "91\n",
            "\n",
            " \n",
            "\n",
            "Skill Library\n",
            "\n",
            "Mine Wood Log\n",
            "Make Crafting Table\n",
            "Craft Stone Sword\n",
            "Make Furnace\n",
            "\n",
            "Craft Shield\n",
            "\n",
            "Cook Steak\n",
            "\n",
            "Combat Zombie; Caption: This image is a slide from a presentation, titled 'A Running Example (3/4)'. It appears to be related to a game or simulation, possibly Minecraft, given the visual elements that resemble the game's graphics. The slide is divided into three main sections: 'Automatic Curriculum', 'Iterative Prompting Mechanism', and 'Skill Library'. The 'Automatic Curriculum' section shows a circular flow diagram with images of a crafting table, a zombie, and a diamond, indicating a sequence of tasks or activities. The 'Iterative Prompting Mechanism' section contains a code snippet written in a programming language, which seems to be related to the game's mechanics, such as equipping a weapon or crafting items. The 'Skill Library' section lists various skills or items that can be used or created within the game, such as 'Mine Wood Log', 'Make Crafting Table', 'Craft Stone Sword', 'Make Furnace', 'Craft Shield', 'Cook Steak', and 'Combat Zombie'. There are also images of a sword and a shield, which are likely the items being crafted or used in the game. The slide number '91' is visible at the bottom, indicating its position within the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_092.png-- OCR: A Running Example (4/4)\n",
            "\n",
            " \n",
            "\n",
            "Automatic Curriculum Iterative Prompting Mechanism Skill Library\n",
            "\n",
            " \n",
            "\n",
            "async function combatZombie(bot) {\n",
            "// Equip a weapon Mine Wood Log\n",
            "const sword = bot.inventory.findInventoryItem(\n",
            "mcData. itemsByName[\"stone_sword\"].id); Make Crafting Table\n",
            "if (sword) {\n",
            "M\n",
            "\n",
            " \n",
            "\n",
            "      \n",
            "      \n",
            " \n",
            "   \n",
            "    \n",
            " \n",
            "\n",
            "ake Crafting Table ee Ce nena x? Craft Stone Sword\n",
            "await craftStoneSword(bot);} .--------- x Skill ] Make Furnace\n",
            "Combat a Craft and equip a shield Retrieval ¥ :\n",
            ": await craftShield(bot);  ---+-+--+-------- mm — Craft Shield\n",
            "Zombie oe ¥\n",
            "} @P Cook Steak\n",
            "Mine Wood Log + Combat Zombie\n",
            "Env Feedback Code as Refine Program\n",
            "Execution Errors Actions\n",
            "AUINECRRES\n",
            "\n",
            "Mine Diamond Update\n",
            "Exploration\n",
            "\n",
            "Progress\n",
            "\n",
            "Environment Self-Verification\n",
            "\n",
            "92; Caption: Page showing a running example.\n",
            "Caption for /content/drive/MyDrive/pages/page_093.png-- OCR: AN\n",
            "\n",
            "93\n",
            "\n",
            "The system prompt for automatic curriculum (1/3)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "You are a helpful assistant that tells me the next immediate task to do in Minecraft. My\n",
            "ultimate goal is to discover as many diverse things as possible, accomplish as many\n",
            "diverse tasks as possible and become the best Minecraft player in the world.\n",
            "\n",
            "I will give you the following information:\n",
            "\n",
            "Question 1:\n",
            "\n",
            "Answer:\n",
            "\n",
            "Question 2: ... . . . wee\n",
            "\n",
            "eneeT e (The list of question-answer pairs represents the additional context.)\n",
            "Question 3:\n",
            "\n",
            "Answer:\n",
            "\n",
            "Biome:\n",
            "\n",
            "Time:\n",
            "\n",
            "Nearby blocks:\n",
            "\n",
            "Other blocks that are recently seen:\n",
            "\n",
            "Nearbv entities (nearest to farthest): ...\n",
            "Health: Higher than 15 means I’m healthy.\n",
            "\n",
            "Hunger: Higher than 15 means I’m not hungry.\n",
            "\n",
            "Position:\n",
            "\n",
            "Equipment: If I have better armor in my inventory, you should ask me to equip it.\n",
            "\n",
            "Inventory (xx/36):\n",
            "\n",
            "Chests: You can ask me to deposit or take items from these chests. There also might be some\n",
            "unknown chest, you should ask me to open and check items inside the unknown chest.\n",
            "\n",
            "Completed tasks so far:\n",
            "\n",
            "Failed tasks that are too hard:; Caption: The image displays a slide titled 'The system prompt for automatic curriculum (1/3)' with a list of questions and answers related to a Minecraft game scenario. The slide is part of a presentation, indicated by the slide number 93 at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_094.png-- OCR: YJ The system prompt for automatic curriculum (2/3)\n",
            "yi\n",
            "\n",
            " \n",
            "\n",
            "You must follow the following criteria:\n",
            "\n",
            "1) You should act as a mentor and guide me to the next task based on my current learning\n",
            "progress.\n",
            "\n",
            "2) Please be very specific about what resources I need to collect, what I need to craft, or\n",
            "what mobs I need to kill.\n",
            "\n",
            "3) The next task should follow a concise format, such as \"Mine [quantity] [block]\", \"Craft [\n",
            "quantity] [item]\", \"Smelt [quantity] [item]\", \"Kill [quantity] [mob]\", \"Cook [quantity]\n",
            "[food]\", \"Equip [item]\" etc. It should be a single phrase. Do not propose multiple tasks\n",
            "\n",
            "at the same time. Do not mention anything else.\n",
            "\n",
            "4) The next task should not be too hard since I may not have the necessary resources or have\n",
            "\n",
            "learned enough skills to complete it yet.\n",
            "\n",
            "5) The next task should be novel and interesting. I should look for rare resources, upgrade\n",
            "my equipment and tools using better materials, and discover new things. I should not be\n",
            "doing the same thing over and over again.\n",
            "\n",
            "6) I may sometimes need to repeat some tasks if I need to collect more resources to complete\n",
            "more difficult tasks. Only repeat tasks if necessary.\n",
            "\n",
            "7) Do not ask me to build or dig shelter even if it’s at night. I want to explore the world\n",
            "and discover new things. I don’t want to stay in one place.\n",
            "\n",
            "8) Tasks that require information beyond the player’s status to verify should be avoided.\n",
            "For instance, \"Placing 4 torches\" and \"Dig a 2x1x2 hole\" are not ideal since they\n",
            "require visual confirmation from the screen. All the placing, building, planting, and\n",
            "trading tasks should be avoided. Do not propose task starting with these keywords.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "94; Caption: The image displays a slide titled 'The system prompt for automatic curriculum (2/3)'. It contains a list of criteria that must be followed when guiding a player through tasks in a game or simulation. The criteria include acting as a mentor, being specific about resources needed, following a concise format for tasks, ensuring tasks are not too hard, keeping tasks novel and interesting, repeating tasks if necessary, avoiding tasks that require visual confirmation from the screen, and not asking the player to build or dig shelter at night. The slide is numbered 94 at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_095.png-- OCR: YJ The system prompt for automatic curriculum (3/3)\n",
            "yi\n",
            "\n",
            "You should only respond in the format as described below:\n",
            "\n",
            "RESPONSE FORMAT:\n",
            "\n",
            "Reasoning: Based on the information I listed above, do reasoning about what the next task\n",
            "should be.\n",
            "\n",
            "Task: The next task.\n",
            "\n",
            "Here’s an example response:\n",
            "Reasoning: The inventory is empty now, chop down a tree to get some wood.\n",
            "Task: Obtain a wood log.\n",
            "\n",
            " \n",
            "\n",
            "95; Caption: The image displays a slide titled 'The system prompt for automatic curriculum (3/3)'. It contains instructions for responding to a system prompt, which includes a response format with two parts: 'Reasoning' and 'Task'. An example response is provided, demonstrating how to reason about the next task based on given information and what the task should be. The slide is part of a larger presentation, as indicated by the slide number 95.\n",
            "Caption for /content/drive/MyDrive/pages/page_096.png-- OCR: 96\n",
            "\n",
            "Sample tasks proposed by the automatic curriculum\n",
            "\n",
            "Inventory (5/36): {'oak_planks': 3, ‘stick’:\n",
            "4, 'crafting_table': 1, ‘stone’: 3,\n",
            "‘wooden_pickaxe': 1}\n",
            "\n",
            "Biome: river\n",
            "Inventory (4/36): {'sugar_cane': 3, ‘paper’:\n",
            "3, 'fishing_rod': 1, 'wooden_pickaxe': 1}\n",
            "\n",
            "Nearby entities: pig, cat, villager\n",
            "Health: 12/20\n",
            "Hunger: 0/20\n",
            "\n",
            "Inventory (6/36): {‘furnace': 1,\n",
            "‘stone_pickaxe’: 1, 'oak_planks': 7,\n",
            "‘crafting_table’: 1, 'raw_iron': 4, ‘coal’: 1}\n",
            "\n",
            "Time: night\n",
            "Nearby entities: zombie\n",
            "Equipment: ‘stone_sword’, ‘shield’\n",
            "\n",
            "    \n",
            "\n",
            "eHeen\n",
            "\n",
            " \n",
            "\n",
            "Reasoning: Since you have a wooden pickaxe and some stones, it would be\n",
            "beneficial to upgrade your pickaxe to a stone pickaxe for better efficiency.\n",
            "Task: Craft 1 stone pickaxe.\n",
            "\n",
            "Reasoning: Since you have a fishing rod in your inventory and you are near a river\n",
            "biome, it would be a good opportunity to catch some fish for food and experience.\n",
            "Task: Catch 1 fish.\n",
            "\n",
            "Reasoning: Your hunger is at 0, which means you need to find food to replenish\n",
            "your hunger. Since there are pigs nearby, you can kill one to obtain raw porkchops.\n",
            "Task: Kill 1 pig.\n",
            "\n",
            "Reasoning: You have raw iron and coal, and you have a furnace. It's time to smelt\n",
            "the iron to make iron ingots, which can be used to craft better tools and armor.\n",
            "Task: Smelt 4 raw iron.\n",
            "\n",
            "Reasoning: Since it's night and there's a zombie nearby, it's a good opportunity to\n",
            "try killing the zombie now that you have a stone sword and a shield equipped.\n",
            "Task: Kill 1 zombie.; Caption: Slide 96 titled 'Sample tasks proposed by the automatic curriculum' with images and text describing tasks in a game environment.\n",
            "Caption for /content/drive/MyDrive/pages/page_097.png-- OCR: yN Sample Skill Library\n",
            "mm\n",
            "\n",
            "Adding a new skill\n",
            "\n",
            "97\n",
            "\n",
            "Program Generated by GPT-4\n",
            "\n",
            "async function combatZombie(bot) {\n",
            "\n",
            "// Equip a weapon\n",
            "const sword =\n",
            "\n",
            "bot. inventory. findInventoryItem(\n",
            "\n",
            "mcData. itemsByName[\n",
            "“stone_sword\"\n",
            "J.id\n",
            "\n",
            "ds\n",
            "if (sword) {\n",
            "await bot.equip(sword, “hand\");\n",
            "\n",
            "F else {\n",
            "await craftStoneSword(bot);\n",
            "\n",
            "5\n",
            "// Craft and equip a shield\n",
            "\n",
            "// Recover hunger\n",
            "\n",
            "// Look for and combat a zombie\n",
            "\n",
            "Task: Craft Iron Pickaxe\n",
            "\n",
            "How to craft an iron pickaxe in\n",
            "Minecraft?\n",
            "\n",
            "Skill retrieval\n",
            "\n",
            "a\n",
            "\n",
            "Program Description\n",
            "\n",
            "async function combatZombie(bot) {\n",
            "// The function is about\n",
            "equipping a stone sword to combat\n",
            "a zombie. If a stone sword is not\n",
            "found, it will craft one.\n",
            "Additionally, it crafts and equips\n",
            "a shield for added protection.\n",
            "Afterwards, it proceeds to cook\n",
            "sticks in order to restore hunger.\n",
            "Once hunger is replenished, it\n",
            "actively searches for a zombie and\n",
            "engages in combat with it.\n",
            "\n",
            " \n",
            "\n",
            "Skill Library\n",
            "\n",
            "Mine Wood Log\n",
            "\n",
            "Make Crafting Table\n",
            "Craft Wooden Pickaxe\n",
            "-- Key Craft Stone Sword\n",
            "\n",
            "Make Furnace\n",
            "\n",
            "Add =\n",
            "Combat Cow\n",
            "Cook Steak\n",
            "\n",
            "Craft Iron Axe\n",
            "\n",
            " \n",
            "\n",
            "ae\n",
            "\n",
            "To craft an iron pickaxe, you\n",
            "need to 3 iron ingots and 2\n",
            "sticks. Once you have gathered\n",
            "the materials, ....\n",
            "\n",
            "Environment Feedback\n",
            "\n",
            " \n",
            "\n",
            "+ Value\n",
            "Combat Zombie\n",
            "\n",
            "=~ See OX YOO\n",
            "\n",
            " \n",
            "\n",
            "Top-5 Relevant Skills\n",
            "\n",
            "Smelt Iron Ingot\n",
            "\n",
            "Retrieve\n",
            "ae ft Stick\n",
            "ED — SkillLibrary + & “revs\n",
            "Make Crafting Table\n",
            "Make Furnace\n",
            "\n",
            "#* Craft Wooden Pickaxe; Caption: Sample Skill Library\n",
            "Caption for /content/drive/MyDrive/pages/page_098.png-- OCR: AN\n",
            "\n",
            "Inventory (8/36): {'oak_planks': 5, ‘cobblestone’: 2,\n",
            "‘porkchop\": 2, 'wooden_sword': 1, 'coal': 5, 'wooden_pickaxe':\n",
            "1, 'oak_log’: 3, ‘dirt’: 9}\n",
            "\n",
            "Task: Mine 5 coal ores\n",
            "\n",
            "Inventory (10/36): {‘raw_copper': 9, ‘copper_ingot': 3,\n",
            "‘acacia_planks': 1, 'raw_iron’: 1, ‘stick’: 1, ‘iron_sword': 1,\n",
            "‘iron_pickaxe': 1, ‘iron_ingot': 3, 'crafting_table': 1, ‘furnace’: 1}\n",
            "Task: Craft a spyglass\n",
            "\n",
            "Inventory (7/36): {'oak_log': 5, 'oak_planks': 1,\n",
            "‘wooden_pickaxe': 1, 'wooden_sword’: 1, ‘porkchop': 2,\n",
            "‘white_wool\": 2, 'mutton': 6}\n",
            "\n",
            "Task: Kill 3 sheep\n",
            "\n",
            "Inventory (9/36): {'string': 4, ‘coal’: 1, 'rotten_flesh': 1,\n",
            "‘iron_sword': 1, ‘furnace’: 1, ‘dirt': 6, 'stone_shovel': 1,\n",
            "‘wooden_pickaxe': 1, ‘granite’: 5}\n",
            "\n",
            "Task: Kill 1 zombie\n",
            "\n",
            "98\n",
            "\n",
            "Examples of Self-Verification\n",
            "\n",
            " \n",
            "\n",
            "hoe 8\n",
            "\n",
            " \n",
            "\n",
            "Reasoning: Mining coal_ore in Minecraft will get coal. You have 5 coal in your\n",
            "inventory.\n",
            "Success: True\n",
            "\n",
            "Reasoning: To craft a spyglass, you need 2 copper ingots and 1 amethyst shard.\n",
            "You have 3 copper ingots, but you don't have any amethyst shards.\n",
            "\n",
            "Success: False\n",
            "\n",
            "Critique: Find and mine an amethyst shard underground.\n",
            "\n",
            "Reasoning: You have 2 white_wool and 6 mutton in your inventory, which indicates\n",
            "that you killed 2 sheep. You needed to kill 3 sheep.\n",
            "\n",
            "Success: False\n",
            "\n",
            "Critique: Find and kill one more sheep to complete the task.\n",
            "\n",
            "Reasoning: You have 1 rotten_flesh in your inventory, which means you have killed\n",
            "at least 1 zombie.\n",
            "Success: True; Caption: The image displays a slide titled 'Examples of Self-Verification' with a series of tasks and inventory lists related to a game or simulation. Each task is accompanied by a reasoning process, success status, and a critique or suggestion for improvement. The tasks involve mining coal ore, crafting a spyglass, killing sheep, and killing a zombie. The slide is numbered 98.\n",
            "Caption for /content/drive/MyDrive/pages/page_099.png-- OCR: Stanford Al Village [UIST 2023]\n",
            "\n",
            " \n",
            "\n",
            "A\n",
            "\n",
            "of experience\n",
            "\n",
            "  \n",
            "\n",
            "BOTs cme erly\n",
            "\n",
            " \n",
            "\n",
            "Taking a walk\n",
            "Rte rtd\n",
            "\n",
            "[Klaus]: Not at all, Abigail.\n",
            "How are you?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Finishing a . ies n . R J\n",
            "“Ms - - [John]: Hey, have you heard\n",
            "the\n",
            "\n",
            "morning routine = 5\n",
            "ir ~ \" anything new about\n",
            "upcoming mayoral election?\n",
            "[Tom]: No, not really. Do you\n",
            "know who is running?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "a ee\n",
            "\n",
            " \n",
            "\n",
            "99 Source: “Generative Agents: Interactive Simulacra of Human Behavior,” UIST, 2023.; Caption: The image displays a virtual environment titled 'Stanford AI Village [UIST 2023]' which is a simulation of human behavior. It includes various scenarios such as 'Joining for coffee at a cafe', 'Arriving at school', 'Sharing news with colleagues', 'Taking a walk in the park', and 'Finishing a morning routine'. Each scenario is represented by a different room or area within the village, and there are text boxes that simulate conversations between characters. The source of the image is cited as 'Generative Agents: Interactive Simulacra of Human Behavior,' UIST, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_100.png-- OCR: Stanford Al Village [UIST 2023]\n",
            "of experience\n",
            "\n",
            " \n",
            "\n",
            "    \n",
            " \n",
            " \n",
            "\n",
            "Memory Stream\n",
            "\n",
            "100 Source: “Generative Agents: Interactive Simulacra of Human Behavior,” UIST, 2023.\n",
            "\n",
            "     \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Generative Agent Memory\n",
            "\n",
            "      \n",
            "  \n",
            "\n",
            "Perceive Retrieve Retrieved Memories; Caption: The image shows a flowchart of the agent architecture for episodic memory of experience.\n",
            "Caption for /content/drive/MyDrive/pages/page_101.png-- OCR: A Stanford Al Village [UIST 2023]\n",
            "\n",
            "= Episodic memory of experience\n",
            "\n",
            "The memory stream comprises a large number of observations that are relevant and irrelevant to the\n",
            "agent’s current situation. Retrieval identifies a subset of these observations that should be passed to the\n",
            "language model to condition its response to the situation.\n",
            "\n",
            " \n",
            "\n",
            "Q. What are you looking forward to\n",
            "the most right now?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Memory Stream\n",
            "\n",
            "Isabella Rodriguez is excited to be planning a\n",
            "Valentine's Day party at Hobbs Cafe on\n",
            "February 14th from 5pm and is eager to invite\n",
            "everyone to attend the party.\n",
            "\n",
            "2023-02-13 22:48:20: desk is idle\n",
            "2023-02-13 22:48:20: bed is idle\n",
            "\n",
            "2023-02-13 22:48:10: closet is idle\n",
            "\n",
            "2023-02-13 22:48:10: refrigerator is idle\n",
            "\n",
            "2023-02-13 22:48:10; Isabella Rodriguez is stretching\n",
            "2023-02-13 22:33:30: shelf is idle 2.34 = 092 * 063 + 0,80\n",
            "2023-02-13 22:33:30: desk is neat and organized\n",
            "\n",
            "2023-02-13 22:33:10; Isabella Rodriguez is writing in her journal\n",
            "2023-02-13 22:18:10: desk is idle\n",
            "\n",
            "2023-02-13 22:18:10; Isabella Rodriguez is taking a break\n",
            "2023-02-13 21:49:00: bed is idle\n",
            "\n",
            "2023-02-13 21:48:50; Isabella Rodriguez is cleaning up the\n",
            "\n",
            "retrieval recency importance _ relevance\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ordering decorations for the party\n",
            "\n",
            " \n",
            "\n",
            "222 = 087 + 063 * O74\n",
            "\n",
            " \n",
            "\n",
            "kitchen researching ideas for the party\n",
            "2023-02-13 21:48:50: refrigerator is idle\n",
            "\n",
            "2023-02-13 21:48:50: bed is being used 2.20 7 085 + 073\n",
            "2023-02-13 21:48:10: shelf is idle\n",
            "\n",
            "2023-02-13 21:48:10; Isabella Rodrigues is watching a movie\n",
            "2023-02-13 21:19:10: shelf is organized and tidy\n",
            "\n",
            "2023-02-13 21:18:10; desk is idle\n",
            "\n",
            "2023-02-13 21:18:10; Isabella Rodriguez is reading a book\n",
            "2023-02-13 21:03:40: bed is idle\n",
            "\n",
            "2023-02-13 21:03:30; refrigerator is idle\n",
            "\n",
            "2023-02-13 21:03:30: desk is in use with a laptop and some papers\n",
            "on it\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "I'm looking forward to the\n",
            "Valentine's Day party that @\n",
            "Isabella\n",
            "\n",
            "I'm planning at Hobbs Cafe!\n",
            "\n",
            " \n",
            "\n",
            "101; Caption: The image displays a slide from a presentation titled 'Stanford AI Village [UIST 2023] Episodic memory of experience'. It explains the concept of a memory stream in AI, which consists of a large number of observations relevant and irrelevant to the agent's current situation. The slide includes a text box with a list of observations from a memory stream, a question 'What are you looking forward to the most right now?', and a retrieval process with numerical values and attributes such as recency, importance, and relevance. The slide also features a character named Isabella Rodriguez, who is planning a Valentine's Day party.\n",
            "Caption for /content/drive/MyDrive/pages/page_102.png-- OCR: A Stanford Al Village [UIST 2023]\n",
            "\n",
            "= Episodic memory of experience\n",
            "\n",
            "Areflection tree for Klaus Mueller. The agent’s observations of the world, represented in the leaf nodes,\n",
            "are recursively synthesized to derive Klaus’s self-notion that he is highly dedicated to his research.\n",
            "\n",
            " \n",
            "\n",
            "[Reflection] Klaus\n",
            "Mueller is highly\n",
            "dedicated to research\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "  \n",
            "    \n",
            "\n",
            "[Reflection] Klaus [Reflection] Klaus\n",
            "Mueller is engaging in Mueller is dedicated to\n",
            "research activities research\n",
            "\n",
            "[Reflection] Klaus\n",
            "Mueller is dedicated to\n",
            "research\n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            "[Plan] For Wednesday ;\n",
            "February 13: wake up and [Observation] Klaus\n",
            "complete the morning routine Mueller is making\n",
            "ak 7300 am, read and take ‘ connections between\n",
            "notes for research paper a :\n",
            "\n",
            "8:00 am, have lunch at 12:00 the articles:\n",
            "pm, write down ideas or\n",
            "brainstorm potential\n",
            "solutions at 1:00 pm, [...]\n",
            "\n",
            "[Observation] Klaus Mueller is\n",
            "reading about gentrification\n",
            "\n",
            "[Observation] Klaus [Observation] Klaus\n",
            "Mueller is reading and Mueller is searching for\n",
            "taking notes on the relevant articles with\n",
            "articles the help of a librarian\n",
            "\n",
            "    \n",
            " \n",
            " \n",
            "\n",
            "[Observation] Klaus\n",
            "Mueller is engaging with\n",
            "a librarian to further\n",
            "his research\n",
            "\n",
            "[Observation] library table\n",
            "is being used to research\n",
            "material and make connections\n",
            "between the articles\n",
            "\n",
            "[Observation] Klaus\n",
            "Mueller is reading the\n",
            "assigned material\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "  \n",
            "\n",
            "[Observation] Klaus\n",
            "Mueller is discussing\n",
            "his research with a\n",
            "librarian\n",
            "\n",
            "  \n",
            "     \n",
            "\n",
            "[Reflection] Klaus\n",
            "Mueller spends many\n",
            "hours reading\n",
            "\n",
            "[Observation] library\n",
            "table is being used to\n",
            "discuss research material\n",
            "\n",
            " \n",
            "\n",
            "[Observation] Klaus Mueller\n",
            "is reading about urban design\n",
            "\n",
            " \n",
            "\n",
            "102; Caption: Page showing a flowchart.\n",
            "Caption for /content/drive/MyDrive/pages/page_103.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_104.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_105.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_106.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_107.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_108.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_109.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_110.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_111.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_112.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_113.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_114.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_115.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_116.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_117.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_118.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_119.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_120.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_121.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_122.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_123.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_124.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_125.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_126.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_127.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_128.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_129.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_130.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_131.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_132.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_133.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_134.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_135.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_136.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_137.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_138.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_139.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_140.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_141.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_142.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_143.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_144.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_145.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_146.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_147.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_148.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_149.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_150.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_151.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_152.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_153.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_154.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_155.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_156.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_157.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_158.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_159.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_160.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_161.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_162.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_163.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_164.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_165.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_166.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_167.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_168.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_169.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_170.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_171.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_172.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_173.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_174.png-- OCR: ; Caption: The image is completely white without any visible content or details.\n",
            "Caption for /content/drive/MyDrive/pages/page_175.png-- OCR: Reasoning\n",
            "\n",
            "Wen-Huang Cheng (2832 2)\n",
            "\n",
            "National Taiwan University\n",
            "\n",
            " \n",
            "\n",
            "wenhuang@csie.ntu.edu.tw; Caption: The image displays a presentation slide with a blue background. At the top, there is a title 'Artificial Intelligence' followed by a subheading 'Reasoning'. Below this, there is a name 'Wen-Huang Cheng (郭文皇)' and the affiliation 'National Taiwan University'. An email address 'wenhuang@csie.ntu.edu.tw' is also provided. The slide features a graphic of a human head with a circuit pattern, symbolizing artificial intelligence, and a network of interconnected nodes and lines, representing a digital or technological theme. The slide number '1' is visible at the bottom left corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_176.png-- OCR: Al Weekly\n",
            "- Towards Long-Horizon Vision-Language\n",
            "CVPR ams Navigation: Platform, Benchmark and\n",
            "\n",
            "Method\n",
            "\n",
            "Take the to the couch in the living room, then retrieve a book\n",
            "from the living room\n",
            "\n",
            " \n",
            "\n",
            "turn left for; Caption: The image shows a visual representation of a navigation task where a backpack is moved from the bedroom to the couch in the living room, and then a book is retrieved from the living room. The image is part of a presentation slide from CVPR/Mashville 2025, discussing long-horizon vision-language navigation.\n",
            "Caption for /content/drive/MyDrive/pages/page_177.png-- OCR: Al Weekly\n",
            "\n",
            "Observation Vision Encoder\n",
            "\n",
            "  \n",
            " \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            " \n",
            "   \n",
            "   \n",
            " \n",
            " \n",
            "  \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Stop\n",
            "0\n",
            "Multi- i\n",
            "view\n",
            "Take the book in living room S2\n",
            "to the desk in office, then take\n",
            "the clock in the office to the\n",
            "kitchen counter in kitchen. S3\n",
            "Subtask finished \"\n",
            "& Few steps i =\n",
            "‘ Short-Term Memory\n",
            "CoT Feedback !\n",
            "Ace eee edn etn es IN '\n",
            "I\n",
            "{ nena eee hg hy hy By hy ~My\n",
            "‘ Task positioning : Avg Pool\n",
            "TSE Tt tee 4\n",
            "CoT generation\n",
            "Lo ene I GPT-4 Min(— )\" ei 1og(e))\n",
            "Instruction: <ins> +\n",
            "ob 9 CoT: <cot>\n",
            "a eae ase\n",
            "Memory: <hist> ‘Avg Pool\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "Obs\n",
            "\n",
            "Long-Term\n",
            "Memory\n",
            "\n",
            "G “)\n",
            "\n",
            "Q Target\n",
            "\n",
            "t\n",
            "\n",
            "8 Match\n",
            "Gai\n",
            "\n",
            "Weight; Caption: Slide 3\n",
            "Caption for /content/drive/MyDrive/pages/page_178.png-- OCR: 18 Dec 2024\n",
            "\n",
            "Ne. <\n",
            "\n",
            "Thinking in Space: How Multimodal Large Language Models\n",
            "See, Remember, and Recall Spaces\n",
            "\n",
            "Jihan Yang'* Shusheng Yang!* Anjali W.Gupta!* RilynHan’* Li Fei-Fei> Saining Xie!\n",
            "'New York University *Yale University *Stanford University\n",
            "\n",
            "® Project Page €) Evaluation Code ® VSI-Bench\n",
            "\n",
            "See a video of an apartment a laboratory\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "      \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Remember? BASTEPeO@soeP@e@SeGrntserEsS Taga\n",
            "\n",
            "Multimodal LLMs “tognitive map” of the space | cabin Door char sink PLAS Teh poy LAM CHING priniar Banke Keyoourd TV Blanket Fridge Tabla Pan ling, Pale Roling ging Wooden Workbench; Caption: The image is a slide from a presentation titled 'AI Weekly' with the subtitle 'Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces'. It features a list of authors and their affiliations, links to a project page, evaluation code, and VSI-Bench, and a section titled 'See' with images of an apartment, a laboratory, and a factory. Below the images, there is a 'Remember?' section with a 'cognitive map' of a space, listing various objects and furniture. The slide is dated '18 Dec 2024' and is numbered '4'.\n",
            "Caption for /content/drive/MyDrive/pages/page_179.png-- OCR: Probing “Visible” Physical Commonsense Knowledge JL qo0il\n",
            "\n",
            "Visually accessible knowledge representing color, size and space\n",
            "\n",
            "Task Setting Prompt\n",
            "\n",
            "Color ZS O is of [MASK] color\n",
            "FT [CLS] color of O\n",
            "QA What is the color of O? (a) .. (b) ..\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Size ZS O; is [MASK] than Og in size\n",
            "FT [CLS] size of Oi in comparison to O2\n",
            "QA what is the size of Oi in comparison to\n",
            ": ; O2? (a) .. (b) ..\n",
            "~+ Color: keyboard is white, black . : .\n",
            "Spatial ZS ina S, the O; is located [MASK] the\n",
            "~- Size: monitor is larger than keyboard O2\n",
            "A VLM N~ Space: monitor is located above table FE fens!) ana as te; isi Jocaten anccomt:\n",
            "parison to O2\n",
            "QA in a S, where is O, is located in com-\n",
            "\n",
            "parison to O2? (a) .. (b) ..\n",
            "\n",
            "Source: “VIPHY: Probing “Visible” Physical Commonsense Knowledge,” EMNLP 2023.; Caption: The image displays a slide from a presentation titled 'Probing \"Visible\" Physical Commonsense Knowledge.' It includes a table with tasks related to color, size, and spatial relationships, and a visual example showing a keyboard, a monitor, and a table with a laptop. The slide also contains a logo and a source citation for the content.\n",
            "Caption for /content/drive/MyDrive/pages/page_180.png-- OCR: Probing “Visible” Physical Commonsense Knowledge JL qWovil\n",
            "\n",
            "Visually accessible knowledge representing color, size and space\n",
            "\n",
            "Zero-shot results (R-Acc) across all tasks\n",
            "\n",
            " \n",
            "\n",
            "Model Color Size Spatial\n",
            "\n",
            "—_———_—_—__- _—_——— Relaxed Accuracy (R-Acc) — The prediction (P;)\n",
            "BERT, arge 48.39 44.61 20.96 is accurate if the most probable label (/;) belongs\n",
            "RoBERTatarge 0.59 47.01 17.52 to the set of ground-truth labels (Z;).\n",
            "UnifiedQAtarge 51.00 51.76 63.04 Ra = EOTIAL = arg man RO)\n",
            "VisualBERT 9.06 24.91 9.57 ‘eb m'\n",
            "Human 97.45 90.12 88.24\n",
            "\n",
            "Source: “VIPHY: Probing “Visible” Physical Commonsense Knowledge,” EMNLP 2023.; Caption: The image displays a slide titled 'Probing \"Visible\" Physical Commonsense Knowledge' with a table and a mathematical formula. The table lists different models and their corresponding zero-shot results (R-Acc) across tasks related to color, size, and spatial knowledge. The models include BERTlarge, RoBERTa, UnifiedQA, VisualBERT, and Human. The formula defines Relaxed Accuracy (R-Acc) as the prediction being accurate if the most probable label belongs to the set of ground-truth labels. The source of the information is cited as 'VIPHY: Probing \"Visible\" Physical Commonsense Knowledge,' EMNLP 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_181.png-- OCR: See a video of an apartment a laboratory\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "        \n",
            "\n",
            "Remember? BDSTvrEeBe OM oee@eG0iverkBsetTAsa\n",
            "Multimodal LLM’s “cognitive map” of the space | Cabinet Door Chair Sink Quine \"Cas\" Box\n",
            "\n",
            "Light Ceiling Milling Plastic Rolling\n",
            "Cabinet Can Switch Light Printer Basket Keyboard TV Blanket Fridge Table Pan SRST si\n",
            "\n",
            " \n",
            "\n",
            "3\n",
            "z\n",
            "i\n",
            "5\n",
            "3\n",
            "i\n",
            "\n",
            "Oe ¢ Bl y, @ & ye mica\n",
            "\n",
            "Bi\n",
            "\n",
            " \n",
            "\n",
            "d (ao) &) pss\n",
            "S86 fue ve\n",
            "eS &\n",
            "\n",
            " \n",
            "\n",
            "Recall? .:’\n",
            "\n",
            "What is the distance between the How many cabinet(s) are in this room? = What is the height of the stool, in cm?\n",
            "keyboard and the TV, in meters?; Caption: This image is a visual representation of a cognitive map created by a multimodal language model (LLM) for three different types of spaces: an apartment, a laboratory, and a factory. Each space is depicted with a series of icons that correspond to various objects and furniture found within that environment. Below the images, there are questions related to the objects in the cognitive maps, such as the distance between a keyboard and a TV, the number of cabinets, and the height of a stool. The cognitive map is a tool used by the LLM to understand and navigate through different spaces by associating words with visual representations.\n",
            "Caption for /content/drive/MyDrive/pages/page_182.png-- OCR: Al Weekly\n",
            "\n",
            " \n",
            "\n",
            "Object Count\n",
            "\n",
            "How many chairs are there in this room?\n",
            "Answer: 4\n",
            "\n",
            "Relative Distance\n",
            "\n",
            "Measuring from the closest point of each object, which of these\n",
            "objects (refrigerator, sofa, ceiling light, cutting board) is\n",
            "the closest to the printer?\n",
            "\n",
            "A. refrigerator B. sofa C. ceiling Light D. cutting board\n",
            "\n",
            "Appearance Order\n",
            "\n",
            "What will be the first-time appearance order of the following\n",
            "categories in the video: basket, printer, refrigerator, kettle?\n",
            "\n",
            "A. kettle, basket, printer, refrigerator\n",
            "B. refrigerator, printer, basket, kettle\n",
            "C. basket, printer, refrigerator, kettle\n",
            "D\n",
            "\n",
            ". basket, refrigerator, kettle, printer\n",
            "\n",
            "Relative Direction\n",
            "\n",
            "If I am standing by the refrigerator and facing the sofa,\n",
            "is the kettle to my left, right, or back?\n",
            "\n",
            "A. Left B. right C. back\n",
            "\n",
            "c_\n",
            "\n",
            "   \n",
            "    \n",
            "\n",
            "9 Video Starting Position\n",
            "\n",
            "{— Camera Trajectory\n",
            "\n",
            "fa Video Ending Position\n",
            "\n",
            "=f] Camera Perspective\n",
            "\n",
            "Object Size\n",
            "\n",
            "What is the length of the longest dimension (length, width, or\n",
            "height) of the refrigerator in centimeters?\n",
            "\n",
            "Answer: 119\n",
            "\n",
            "Absolute Distance\n",
            "Measuring from the closest point of each object, what is the\n",
            "distance between the bed and the sofa in meters?\n",
            "\n",
            "Answer: 3.2\n",
            "\n",
            "Room Size\n",
            "What is the size of this room (in square meters)? If multiple\n",
            "rooms are shown, estimate the size of the combined space.\n",
            "\n",
            "Answer: 57.6\n",
            "\n",
            "Route Plan\n",
            "\n",
            "You are a robot beginning at the toilet and facing the washer.\n",
            "Navigate to the pan. Fill in this route: 1. Go forward until\n",
            "the washing machine 2. [?] 3. Go forward until the sofa 4. [?]\n",
            "5. Go forward until the pan.\n",
            "\n",
            "A. Turn Left, Turn Left B. Turn Left, Turn Right\n",
            "\n",
            "c. Turn Back, Turn Right D. Turn Right, Turn Right\n",
            "\n",
            "Figure 3. Tasks demonstration of VSI-—Bench. Note: the questions above are simplified slightly for clarity and brevity.; Caption: The image displays a webpage with a header reading 'AI Weekly' and a series of tasks related to spatial reasoning and video analysis. The tasks are divided into two columns, with the left column featuring questions about object count, relative distance, appearance order, and relative direction, each with multiple-choice answers. The right column contains questions about object size, absolute distance, room size, and a route plan, also with multiple-choice answers. Below the tasks, there is a diagram illustrating a room layout with a camera trajectory and starting and ending positions. The slide is labeled as 'Figure 3. Tasks demonstration of VSI-Bench' and includes a note that the questions have been simplified for clarity and brevity.\n",
            "Caption for /content/drive/MyDrive/pages/page_183.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner\n",
            "\n",
            "m Analogical Reasoning\n",
            "= Multimodal Reasoning\n",
            "= Multimodal CoT\n",
            "= Commonsense Knowledge\n",
            "= Reasoning Benchmarks; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. Below the title, there is a list of topics that will be covered in the lecture. The topics are organized into four main categories: 'Reasoning Basics,' 'Analogical Reasoning,' 'Multimodal Reasoning,' and 'Reasoning Benchmarks.' Each category has sub-points. For example, under 'Reasoning Basics,' there are two sub-points: 'Chain-of-Thought (CoT)' and 'Zero-shot reasoner,' with 'Few-shot reasoner' as a further sub-point under 'Chain-of-Thought.' The slide number '9' is visible at the bottom, indicating its position within the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_184.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is structured with bullet points under the main heading 'Reasoning Basics'. The sub-points include 'Chain-of-Thought (CoT)', 'Zero-shot reasoner', 'Few-shot reasoner', 'Analogical Reasoning', 'Multimodal Reasoning', 'Multimodal CoT', 'Commonsense Knowledge', and 'Reasoning Benchmarks'. The slide number '10' is visible at the bottom, indicating its position within the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_185.png-- OCR: YJ Let’s start from a toy problem C\n",
            "mm\n",
            "\n",
            "‘Last Letter Concatenation’\n",
            "Rule: Take the last letter of each word, and then concatenate them\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Input Output\n",
            "“Elon Musk” “nk”\n",
            "“Bill Gates” “Is”\n",
            "\n",
            " \n",
            "\n",
            "“Barack Obama” ?\n",
            "\n",
            " \n",
            "\n",
            "11 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide from a presentation with the title 'Last Letter Concatenation'. It explains a rule for a toy problem where the last letter of each word is taken and then concatenated. There is a table with two columns labeled 'Input' and 'Output'. The inputs listed are 'Elon Musk', 'Bill Gates', and 'Barack Obama', with the corresponding outputs being 'nk', 'Is', and a question mark indicating an unknown output for 'Barack Obama'. The source of the content is cited as 'Large Language Model Agents,' CS294/194-196, Berkeley University, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_186.png-- OCR: aN Solve it by Machine Learning (ML)? C\n",
            "\n",
            "Tons of labeled data needed! “Is”\n",
            "\n",
            " \n",
            "\n",
            "“Bill Gates”\n",
            "\n",
            "Would we still refer to ML as Al when it requires vast\n",
            "amounts of labeled data to learn such a “simple” task?\n",
            "\n",
            "12 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image shows a diagram of a machine learning model with an encoder and a decoder, and a question about the amount of labeled data needed for machine learning.\n",
            "Caption for /content/drive/MyDrive/pages/page_187.png-- OCR: YJ What is reasoning? C\n",
            "mm\n",
            "\n",
            "Humans can learn from just a few examples\n",
            "because humans can reason\n",
            "\n",
            "Al should be able to learn from just a few examples, like\n",
            "what humans usually do\n",
            "\n",
            "Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide with a blue header that reads 'What is reasoning?' Below the header, there are two bullet points. The first bullet point states 'Humans can learn from just a few examples' and the second bullet point says 'AI should be able to learn from just a few examples, like what humans usually do.' At the bottom of the slide, there is a source citation that reads 'Source: \"Large Language Model Agents,\" CS294/194-196, Berkeley University, 2024.' The slide number '13' is also visible at the bottom left corner.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_188.png-- OCR: YJ Few-shot chain-of-thought (CoT)\n",
            "mm\n",
            "\n",
            "Let’s revisit the “Last Letter Concatenation” problem\n",
            "using LLMs.\n",
            "\n",
            ": Q: “Elon Musk”\n",
            "i : “nk”\n",
            "\n",
            "Q\n",
            "A\n",
            "| Q: “Bill Gates”\n",
            "A: “Is”\n",
            "Q\n",
            "\n",
            " \n",
            "\n",
            ": “Barack Obama”\n",
            "\n",
            "Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide from a presentation titled 'Few-shot chain-of-thought (CoT)' which revisits the 'Last Letter Concatenation' problem using LLMs. The slide includes a diagram with a question and answer format, where the question is 'Q: \"Elon Musk\"' and the answer is 'A: \"nk\"'. Below this, there is another question 'Q: \"Bill Gates\"' with the answer 'A: \"Is\"'. The third question 'Q: \"Barack Obama\"' is shown without an answer. There is a box labeled 'LLM' with an arrow pointing to a red 'X' mark, indicating a failed attempt or error in the process. The source of the content is cited as 'Large Language Model Agents,' CS294/194-196, Berkeley University, 2024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_189.png-- OCR: YJ Few-shot chain-of-thought (CoT)\n",
            "mm\n",
            "\n",
            "Let’s add “reasoning process” before “answer”\n",
            "\n",
            "Q: “Elon Musk”\n",
            "A: the last letter of \"Elon\" is \"n\". the last letter of \"Musk\" is \"k\". Concatenating \"n\", \"k\"\n",
            "\n",
            "so the output is \"nk\".\n",
            "\n",
            "Q: “Bill Gates”\n",
            "A: the last letter of \"Bill\" is \"I\". the last letter of \"Gates\" is \"s\". Concatenating \"I\", \"s\" leads\n",
            "to \"Is\". so the output is \"Is\".\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "reasoning process\n",
            "\n",
            "Q: “Barack Obama\"\n",
            "A:\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "15 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide from a presentation titled 'Few-shot chain-of-thought (CoT)'. It explains the concept of adding a 'reasoning process' before providing an answer. The slide contains three questions with their corresponding answers, demonstrating the reasoning process. The first question is 'Elon Musk' and the answer is 'nk', which is derived by concatenating the last letter of 'Elon' (n) with the last letter of 'Musk' (k). The second question is 'Bill Gates' with the answer 'Is', obtained by concatenating the last letter of 'Bill' (l) with the last letter of 'Gates' (s). The third question is 'Barack Obama' but the answer is not provided on the slide. The slide also includes a source citation at the bottom for 'Large Language Model Agents', CS294/194-196, Berkeley University, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_190.png-- OCR: YJ Few-shot chain-of-thought (CoT)\n",
            "mm\n",
            "\n",
            "Let’s add “reasoning process” before “answer”\n",
            "\n",
            "Q: “Elon Musk”\n",
            "A: the last letter of \"Elon\" is \"n\". the last letter of \"Musk\" is \"k\". Concatenating \"n\", \"k\"\n",
            "\n",
            "so the output is \"nk\".\n",
            "\n",
            "Q: “Bill Gates”\n",
            "A: the last letter of \"Bill\" is \"I\". the last letter of \"Gates\" is \"s\". Concatenating \"I\", \"s\" leads\n",
            "to \"Is\". so the output is \"Is\".\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "reasoning process\n",
            "\n",
            "Q: “Barack Obama\"\n",
            "\n",
            "A. the last letter of \"Barack\" is \"k’. the last letter of \"Obama’ is \"a. Concatenating \"k\", \"a\"\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "100% accuracy with demonstration examples\n",
            "\n",
            "16 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide from a presentation titled 'Few-shot chain-of-thought (CoT)'. It explains the concept of adding a 'reasoning process' before providing an 'answer'. The slide includes three examples where the reasoning process is highlighted in red text, demonstrating how to arrive at the answer by concatenating the last letters of the names 'Elon Musk', 'Bill Gates', and 'Barack Obama'. The slide claims a 100% accuracy rate with these demonstration examples. The source of the content is cited as 'Large Language Model Agents,' from CS294/194-196, Berkeley University, 2024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_191.png-- OCR: ‘YJ Key Idea: Derive the Final Answer through\n",
            "- | a\n",
            "\n",
            " \n",
            "\n",
            "A pioneering study utilized natural language rationales to solve\n",
            "math problems by ... “derive the final answer through a series of\n",
            "\n",
            "small steps. Dobler\n",
            "\n",
            "Question: Two trains running in opposite directions cross a\n",
            "man standing on the platform in 27 seconds and 17 seconds\n",
            "respectively and they cross each other in 23 seconds. The ratio\n",
            "‘ ‘ of their speeds is:\n",
            "\n",
            "proposed te jointly learn __-Options: A) 3/7_B) 3/2_C) 3/88_D) 3/8_E) 2/2\n",
            "\n",
            "to generate the text in Rationale: Let the speeds of the two trains be x m/sec and y\n",
            "the rationale, and to m/sec respectively. Then, length of the first train = 27x meters,\n",
            "perform the math and length of the second train = 17 y meters. (27x + 17y)/(x +\n",
            "operations required to y) = 23 — 27x + 17y = 23x + 23y — 4x = 6y > x/y = 3/2.\n",
            "solve the problem. orrect Option: E\n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "17 Source: “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,” ACL, 2017.; Caption: The image displays a slide from a presentation with a blue header that reads 'Key Idea: Derive the Final Answer through Intermediate Steps.' Below the header, there is a paragraph explaining a study that used natural language rationales to solve math problems by deriving the final answer through a series of small steps. The slide number '17' is visible at the bottom left corner. The main content of the slide is a math problem labeled 'Problem 1' with a question about two trains running in opposite directions and crossing each other, followed by a list of options for the ratio of their speeds. A red box highlights the correct answer 'B) 3/2' and includes a rationale for the solution. The source of the content is cited at the bottom as 'Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,' ACL, 2017.\n",
            "Caption for /content/drive/MyDrive/pages/page_192.png-- OCR: How to perform step by step reasoning\n",
            "without using demonstration examples?\n",
            "\n",
            " \n",
            "\n",
            "AN\n",
            "\n",
            "“Let’s think step by step” (Zero-shot CoT)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
            "and half of the golf balls are blue. How many blue golf balls are\n",
            "there?\n",
            "\n",
            "A: The answer (arabic numerals) is\n",
            "\n",
            " \n",
            "\n",
            "(Output) 8 X\n",
            "\n",
            " \n",
            "\n",
            "~<a\n",
            "\n",
            "Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
            "and half of the golf balls are blue. How many blue golf balls are\n",
            "there?\n",
            "\n",
            "A: Let’s think step by step.\n",
            "\n",
            "(Output) There are 16 balls in total. Half of the balls are golf\n",
            "balls. That means that there are 8 golf balls. Half of the golf balls\n",
            "are blue. That means that there are 4 blue golf balls. “\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "  \n",
            " \n",
            "\n",
            "   \n",
            " \n",
            "   \n",
            "     \n",
            "   \n",
            "\n",
            "However, zero-shot is usually significantly worse than few-shot ...\n",
            "\n",
            "18 Source: “Large Language Models are Zero-Shot Reasoners,” NeurlPS, 2022.; Caption: The image displays a slide from a presentation with the title 'How to perform step by step reasoning without using demonstration examples?' The slide is divided into two sections. The top section has a blue header with white text, and the bottom section has a white background with black text. The slide number '18' is visible at the bottom left corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_193.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner\n",
            "\n",
            "m Analogical Reasoning; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is numbered 19 and outlines the agenda for the lecture. It is divided into several bullet points, each representing a different topic to be covered. The topics listed are 'Reasoning Basics,' 'Chain-of-Thought (CoT),' 'Zero-shot reasoner,' 'Few-shot reasoner,' 'Analogical Reasoning,' 'Multimodal Reasoning,' 'Multimodal CoT,' 'Commonsense Knowledge,' and 'Reasoning Benchmarks.' The slide has a simple design with a white background and black text, and there are no images or graphics other than the text and bullet points.\n",
            "Caption for /content/drive/MyDrive/pages/page_194.png-- OCR: AN\n",
            "\n",
            "20\n",
            "\n",
            "  \n",
            "   \n",
            "\n",
            "  \n",
            "\n",
            "How to perform step by step reasoning\n",
            "without using demonstration examples?\n",
            "\n",
            " \n",
            "\n",
            "Analogical Prompting\n",
            "\n",
            "The inspiration comes from analogical reasoning in\n",
            "psychology, a concept where humans draw from relevant past\n",
            "experiences to tackle new problems.\n",
            "\n",
            "For instance, when faced with a new math problem, humans\n",
            "often think about “do | know a related problem?” and recall how\n",
            "they solved related problems in the past to derive insights for\n",
            "solving the new problem.\n",
            "\n",
            "Source: “Large Language Models as Analogical Reasoners,” ICLR, 2024.; Caption: The image displays a slide from a presentation with a blue header that reads 'How to perform step by step reasoning without using demonstration examples?' Below the header, there is a section titled 'Analogical Prompting' with a paragraph explaining the concept. The slide also includes a source citation at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_195.png-- OCR: 21\n",
            "\n",
            " \n",
            "\n",
            "What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6,\n",
            "-2)? Recall a related problem, and then solve this one.\n",
            "\n",
            "The related problem is finding the distance between two points on a coordinate\n",
            "i plane. To find the distance between two points, we use the distance formula: d = V\n",
            "((x2-x1)42 + (y2-y1)42).\n",
            "\n",
            "and (2, -2) to find the distance d between them: d = v((2-(-2))42 + (-2-2)*2) = v(442 +\n",
            "(-4)42) = (16 + 16) = ¥32. We know that the distance between two opposite sides of a\n",
            "square is equal. So, the length of each side of the square is V32. To find the area of the\n",
            "square, we square the length of one side: Area = (V32)2 = 32.\n",
            "\n",
            "In this problem, we have four points: (-2, 2), (2, -2), (-2, -6), and (-6, -2). Let's choose (-2, 2) :\n",
            "\n",
            "Source: “Large Language Models as Analogical Reasoners,” ICLR, 2024.; Caption: The image displays a slide from a presentation titled 'Analogical Reasoning: An Example'. It contains text explaining the process of finding the area of a square using the distance formula to solve a related problem. The slide is numbered 21 and cites 'Large Language Models as Analogical Reasoners' from ICLR, 2024 as the source.\n",
            "Caption for /content/drive/MyDrive/pages/page_196.png-- OCR: YJ Analogical Prompting C\n",
            "yi\n",
            "\n",
            "Goal: adaptively generate relevant examples and\n",
            "knowledge, rather than just using a fix set of examples\n",
            "\n",
            "An example of coding (Codeforces task)\n",
            "\n",
            "22 Source: “Large Language Models as Analogical Reasoners,” ICLR, 2024.; Caption: The image displays a slide titled 'Analogical Prompting' with a goal statement and an example of coding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_197.png-- OCR: YJ An Example of Coding (Codeforces task) (1/3)\n",
            "mm\n",
            "\n",
            "Our Prompt: Self-generate Exemplars + Knowledge\n",
            "\n",
            "Your goal is to write Python3 code to solve competitive programming problems. Given a problem, explain the core concepts in it and provide\n",
            "other relevant problems. Then solve the original problem.\n",
            "\n",
            "# Problem:\n",
            "You are given a sequence a_1l, a_2,\n",
            "\n",
            "=, an. Each element of a is 1 or 2. Find out if an integer k exists so that the following conditions are\n",
            "met. 1 < k <n-1, and al a2... ak =\n",
            "\n",
            "a_{kt+1} .. an. If there exist multiple k that satisfy the given condition, print the smallest.\n",
            "\n",
            "# Instruction:\n",
            "## Algorithms:\n",
            "Identify the core concepts or algorithms used to solve the problem.\n",
            "\n",
            "## Tutorial:\n",
            "Write a tutorial about these algorithms.\n",
            "\n",
            "## Example Problems:\n",
            "Provide three examples of relevant competitive programming problems that involve these algorithms. For each problem, describe the problem,\n",
            "explain the solution in detail, and then write the correct Python3 code.\n",
            "\n",
            "## Python3 code to solve the original problem:\n",
            "- Explanation of the solution:\n",
            "- Python3 code to solve the problem:\n",
            "\n",
            " \n",
            "\n",
            "23; Caption: The image displays a slide from a presentation, specifically slide number 23, which is part of a series titled 'An Example of Coding (Codeforces task) (1/3)'. The slide is divided into several sections with headings and bullet points. The main heading is 'Our Prompt: Self-generate Exemplars + Knowledge'. Below this, there is a subheading 'Problem:' followed by a description of a programming problem that involves finding an integer 'k' that satisfies certain conditions given a sequence of numbers. The next subheading 'Instruction:' is followed by a section 'Algorithms:' where the task is to identify the core concepts or algorithms used to solve the problem. This is followed by a 'Tutorial:' section where a tutorial about these algorithms is to be written. The next section 'Example Problems:' instructs to provide three examples of relevant competitive programming problems that involve these algorithms, describe the problem, explain the solution in detail, and write the correct Python3 code. The final section 'Python3 code to solve the original problem:' is left blank for the solution to be filled in. The slide has a blue header with white text and a white background for the rest of the content.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_198.png-- OCR: YJ An Example of Coding (Codeforces task) (2/3)\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "| a Tutorial: The prefix product algorithm is used to calculate the product of all the elements in a prefix of an array. The algorithm works\n",
            "' by initializing a variable \"prefix\" to 1, and then iterating through the array from left to right. At each iteration, the current element is\n",
            "' multiplied with the prefix, and the result is stored in the prefix variable. This way, the prefix variable keeps track of the product of all\n",
            "\n",
            "(gat Example Problemse 0 a Sa SSS RS a SS ESSE: Self-generated Exemplars —~>\n",
            "1. Problem: Given an array of integers, find the product of all the elements in each subarray of size k.\n",
            "Solution: We can use the prefix product algorithm to solve this problem. We initialize a prefix variable to 1, and then iterate through\n",
            "the array from left to right. At each iteration, we multiply the current element with the prefix, ......\n",
            "\n",
            "‘def product_of_subarrays(arr, k):\n",
            "\n",
            "' prefix =1\n",
            "\n",
            "1 result = []\n",
            "\n",
            "' for i in range(len(arr)):\n",
            "\n",
            "' prefix = prefix * arr[i]\n",
            "\n",
            "1 if i >= k-1: '\n",
            "' result. append(prefix)\n",
            "\n",
            "prefix = prefix // arrli-k+1]\n",
            "‘ return result '\n",
            "\n",
            "2. Problem: Given an array of integers, find the longest subarray with a product less than or equal to a given target.\n",
            "Solutions 2% cum\n",
            "\n",
            "3. Problem: Given an array of integers, find the smallest k such that the product of all the elements in the prefix of the array is equal to\n",
            "the product of all the elements in the suffix of the array.\n",
            "\n",
            "24; Caption: The image displays a slide from a presentation titled 'An Example of Coding (Codeforces task) (2/3)'. The slide is divided into two main sections. On the left, there is a text box with a heading 'Output by GPT3.5-turbo' followed by a section titled 'Self-generated Knowledge' which contains a description of the prefix product algorithm, including its definition, a tutorial on how it works, and its time complexity. Below this, there is a section titled 'Self-generated Exemplars' with three problems listed, each followed by a brief solution outline. On the right side of the slide, there is a code snippet in Python that defines a function 'product_of_subarrays' which appears to implement the prefix product algorithm described in the text. The slide number '24' is visible at the bottom right corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_199.png-- OCR: YJ An Example of Coding (Codeforces task) (3/3)\n",
            "mm\n",
            "\n",
            "#4# Python3 code to solve the original problem:\n",
            "Explanation of the solution:\n",
            "- Initialize prefix and suffix variables to 1, and k to -1.\n",
            "Calculate the product of all the elements in the sequence using the prefix product algorithm.\n",
            "Iterate through the sequence from right to left.\n",
            "- Update the prefix and suffix variables.\n",
            "- If the prefix is equal to the suffix, update k to the current index.\n",
            "Print the value of k.\n",
            "\n",
            "int (input ())\n",
            "\n",
            "_ in range(t):\n",
            "\n",
            "n = int(input())\n",
            "\n",
            "arr = list(map(int, input().split()))\n",
            "\n",
            "prefix, suffix = 1, 1\n",
            "\n",
            "k = -1\n",
            "\n",
            "for i in range(n):\n",
            "prefix = prefix * arrli]\n",
            "\n",
            "for i in range(n-1, @, -1):\n",
            "prefix = prefix // arr[i]\n",
            "suffix = suffix * arr[i]\n",
            "if prefix == suffix:\n",
            "\n",
            "k=i\n",
            "print(k)\n",
            "\n",
            " \n",
            "\n",
            "25; Caption: The image displays a slide from a presentation, specifically slide number 25, which is part of a series titled 'An Example of Coding (Codeforces task) (3/3)'. The slide contains a block of Python3 code that solves a coding problem, along with an explanation of the solution. The code is presented in a monospaced font, typical for programming code, and is divided into two sections: the explanation of the solution and the actual Python3 code. The explanation section outlines the steps to solve the problem, while the code section provides the implementation of those steps. The slide background is white, and the text is primarily in black, with the title in a larger font size. The slide number is indicated at the bottom right corner.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_200.png-- OCR: yN Overview of Different Reasoning Methods\n",
            "mm\n",
            "\n",
            "¢ Generic guidance reasoning\n",
            "\n",
            "0-shot\n",
            "\n",
            "Q: What is the area of the square\n",
            "\n",
            "with the four vertices at (-2, 2), (2,\n",
            "-2), (-2, -6), and (-6, -2)?\n",
            "\n",
            " \n",
            "\n",
            "0-shot CoT\n",
            "\n",
            "Q: What is the area of the square\n",
            "with the four vertices at (-2, 2), (2,\n",
            "\n",
            " \n",
            "\n",
            "* Need labeled exemplars\n",
            "\n",
            "of reasoning\n",
            "Few-shot CoT\n",
            "\n",
            "| Q: Roger has 5 tennis balls. He\n",
            "\n",
            "buys 2 more cans of tennis balls.\n",
            "Each can has 3 tennis balls. How\n",
            "many tennis balls does he have?\n",
            "A: Roger started with 5 balls. 2\n",
            "cans of 3 balls each is 6 balls.\n",
            "The answer is 5 + 6 = 11.\n",
            "\n",
            "Q: What is the area of the square\n",
            "with the four vertices at (-2, 2), (2,\n",
            "-2), (-2, -6), and (-6, -2)?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "26\n",
            "\n",
            " \n",
            "\n",
            "¢ Exemplars are automatically generated\n",
            "\n",
            "and tailored to each problem\n",
            "Analogical Prompting\n",
            "\n",
            "7—( Model input 9).\n",
            "\n",
            "Q: What is the area of the square with the four vertices at\n",
            "(-2, 2), (2, -2), (-2, -6), and (-6, -2)?\n",
            "\n",
            "# Instruction:\n",
            "\n",
            "## Recall relevant exemplars:\n",
            "## Solve the initial problem:\n",
            "Nn }\n",
            "\n",
            "Model Output\n",
            "\n",
            "# Relevant exemplars:\n",
            "\n",
            "Q: What is the area of the square with a side length of 5?\n",
            "\n",
            "A: The area of a square is found by squaring the length of its\n",
            "side. So, the area of this square is 5*2 = 25. ...\n",
            "\n",
            "# Solve the initial problem:\n",
            "\n",
            "To find the area of the square, we need to find the side\n",
            "\n",
            "length. The length is ... \\sqrt{(2 - (-2))*2 + (-2 - 2)42} =\n",
            "\n",
            "\\sqrt{32}. So, the area of the square is (\\sqrt{32})*2 = 32.\n",
            "QD; Caption: The image is a slide from a presentation titled 'Overview of Different Reasoning Methods'. It is divided into three sections, each representing a different method of reasoning. The first section is labeled 'Generic guidance reasoning' and includes a '0-shot' and '0-shot CoT' example, both asking about the area of a square with given vertices. The second section is labeled 'Need labeled exemplars of reasoning' and shows a 'Few-shot CoT' example with a problem about Roger buying tennis balls. The third section is labeled 'Exemplars are automatically generated and tailored to each problem' and presents an 'Analogical Prompting' example, which is a mathematical problem about finding the area of a square with a side length of 5. Each section has a 'Model Input' and 'Model Output' box, with the 'Model Output' box containing the solution to the problem. The slide is numbered 26.\n",
            "Caption for /content/drive/MyDrive/pages/page_201.png-- OCR: yN CoT Reasoning without Prompting\n",
            "mA\n",
            "\n",
            "CoT prompting (few-shot or zero-shot), while effective, often involves\n",
            "manually intensive prompt engineering.\n",
            "\n",
            "New findings reveal that CoT reasoning paths can be elicited from pre-\n",
            "trained LLMs by simply altering the decoding process.\n",
            "\n",
            "Decoding step 0 Continue greedy decoding\n",
            "\n",
            "Q: | have 3 apples, my dad has 2 mapples x\n",
            "\n",
            "more apples than me, how many top-1: 5 |! have 3 apples, my dad has 2 more apples than me, so he\n",
            "apples do we have in total? top-2: | — has 5 apples. 3+5=8. We havel8 apples in total.\n",
            "A:\n",
            "\n",
            "top-3: We ~~ We have 5 apples in total. >\n",
            "\n",
            "top-4: You\n",
            "top-5: The I You have 3 apples, your dad has 2 more apples than you,\n",
            ": \\ = he has 5 apples. 3+5=8. You have 8 apples in total. VY\n",
            "\n",
            "The answeris5. >\n",
            "\n",
            "Question in standard QA format\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Language\n",
            "model\n",
            "\n",
            "  \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            ". . . uncertain certain\n",
            "Considering alternative top-k tokens, rather than\n",
            "\n",
            "solely relying on the top-1 greedy decoding path!\n",
            "27 Source: “Chain-of-Thought Reasoning without Prompting,” NeurlPS, 2024.; Caption: The image is a slide from a presentation titled 'CoT Reasoning without Prompting'. It discusses the concept of Chain of Thought (CoT) reasoning and how it can be elicited from pre-trained Language Learning Models (LLMs) by altering the decoding process. The slide includes a question in standard QA format, a diagram illustrating the decoding process, and a comparison of different decoding paths with their outcomes. The slide also mentions that considering alternative top-k tokens can be more effective than relying solely on the top-1 greedy decoding path. The source of the information is cited as 'Chain-of-Thought Reasoning without Prompting,' from NeurIPS, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_202.png-- OCR: YJ A Running Example\n",
            "mm\n",
            "\n",
            "Chain-of-Thought Decoding\n",
            "\n",
            "Was Nicolas Cage born in an even or odd year?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Top-1: Nicolas Nicolas Cage was born in an odd year.\n",
            "Top-2: Even Even.\n",
            "\n",
            "Top-3: Odd Odd.\n",
            "\n",
            "Top-4: 1964 1964, an even year.\n",
            "\n",
            "Top-5: He He was born in an even year.\n",
            "\n",
            "Top-6: Cage Cage was born in 1964, an even year.\n",
            "\n",
            " \n",
            "\n",
            "28 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide titled 'A Running Example' with a subtitle 'Chain-of-Thought Decoding'. It presents a question 'Was Nicolas Cage born in an even or odd year?' and a table with six rows, each representing a step in the thought process to answer the question. The table includes the top part of the question, the reasoning, and the conclusion. The source of the content is cited at the bottom as 'Large Language Model Agents,' CS294/194-196, Berkeley University, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_203.png-- OCR: YJ A Running Example C\n",
            "mm\n",
            "\n",
            "Probability for the answer token\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "1 Nicolas Cage was born in an odd year. 0.117\n",
            "2 Even. 0.207\n",
            "3 Odd. 0.198\n",
            "4 1964, an even year. 0.949\n",
            "5 He was born in an even year. 0.000\n",
            "6 Cage was born in 1964, an even year. 0.978\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "29 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a table with the title 'Probability for the answer token'. It lists six different answer tokens related to the birth year of Nicolas Cage, with corresponding probabilities. The tokens are 'Nicolas Cage was born in an odd year.', 'Even.', 'Odd.', '1964, an even year.', 'He was born in an even year.', and 'Cage was born in 1964, an even year.' The probabilities range from 0.117 to 0.978. At the bottom, there is a source citation for 'Large Language Model Agents,' CS294/194-196, Berkeley University, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_204.png-- OCR: A Any concern on generating intermediate steps\n",
            "_ instead of direct answers?\n",
            "\n",
            " \n",
            "\n",
            "Always keep in mind that MLLMs are probabilistic models of generating\n",
            "next tokens. They are not humans.\n",
            "\n",
            "What MLLM does in decoding:\n",
            "\n",
            "_ arg max P(reasoning path, final answer|problem)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "What we want:\n",
            "\n",
            "arg max P(final answer|problem)\n",
            "\n",
            "30 Source: “Large Language Model Agents,” CS294/194-196, Berkeley University, 2024.; Caption: The image displays a slide from a presentation with a blue header that reads 'Any concern on generating intermediate steps instead of direct answers?' Below the header, there is a statement reminding the audience that MLLMs are probabilistic models and not humans. The slide then presents two mathematical expressions, one representing what MLLM does in decoding and the other representing what is desired. The expressions are related to the probability of a reasoning path leading to a final answer for a problem. At the bottom, there is a source citation for the content.\n",
            "Caption for /content/drive/MyDrive/pages/page_205.png-- OCR: Inference Approaches\n",
            "\n",
            "   \n",
            "\n",
            "AN\n",
            "\n",
            "¢ Best-of-N search generates N complete responses and selects the best one among them.\n",
            "¢ Sentence-level Beam Search generates multiple candidate options for each sentence and chooses the best one.\n",
            "\n",
            "* Stage-level Beam Search generates candidates for each reasoning stage (e.g., summary, caption, reasoning, and\n",
            "conclusion) and selects the best option at each stage.\n",
            "\n",
            "Best-of-N Search Sentence-level Beam Search Stage-level Beam Search\n",
            "\n",
            "Question Question Question\n",
            "\n",
            " \n",
            "\n",
            "=\n",
            "| = Apply Verification = Skip Verification oO = Selected upon Verification e = Rejected upon Verification\n",
            "\n",
            "3\n",
            "\n",
            "31 Source: “LLaVA-CoT: Let Vision Language Models Reason Step-by-Step,” arXiv, 2025.; Caption: Inference Approaches\n",
            "Caption for /content/drive/MyDrive/pages/page_206.png-- OCR: ‘YJ LLaVA-CoT C\n",
            "ye\n",
            "\n",
            "¢ LLaVA-CoT is a novel VLM designed to conduct autonomous\n",
            "multistage reasoning, capable of generating four distinct\n",
            "stages: summary, caption, reasoning, and conclusion.\n",
            "Summary: A brief outline in which the model summarizes the\n",
            "forthcoming task.\n",
            "Caption: A description of the relevant parts of an image (if present),\n",
            "focusing on elements related to the question.\n",
            "Reasoning: A detailed analysis in which the model systematically\n",
            "considers the question.\n",
            "Conclusion: A concise summary of the answer, providing a final\n",
            "response based on the preceding reasoning.\n",
            "\n",
            "32 Source: “LLaVA-CoT: Let Vision Language Models Reason Step-by-Step,” arXiv, 2025.; Caption: The image displays a slide from a presentation with the title 'LLLaVA-CoT' at the top. The slide outlines the four stages of a novel Visual Language Model (VLM) called LLaVA-CoT, which is designed for autonomous multistage reasoning. The stages listed are summary, caption, reasoning, and conclusion. Each stage is briefly described: 'Summary' is a brief outline of the task, 'Caption' is a description of relevant image parts, 'Reasoning' is a detailed analysis, and 'Conclusion' is a summary of the answer. The source of the information is cited at the bottom as 'LLLaVA-CoT: Let Vision Language Models Reason Step-by-Step,' arXiv, 2025.\n",
            "Caption for /content/drive/MyDrive/pages/page_207.png-- OCR: YJ LLaVA-CoT\n",
            "ye\n",
            "\n",
            "Briefly explain what steps you'll take ...\n",
            "Question\n",
            "<SUMMARY> ... </SUMMARY> |\n",
            "\n",
            "® |\n",
            "\n",
            " \n",
            "\n",
            "PT-40\n",
            "\n",
            "a\n",
            "\n",
            " \n",
            "\n",
            "Outline a step-by-step thought process ... S\n",
            "<REASONING? ... </REASONING>\n",
            "State the final answer in a clear format ... S\n",
            "- GPT-4o\n",
            "\n",
            "<CONCLUSION?> ... </CONCLUSION> Answer\n",
            "\n",
            "GPT-40\n",
            "\n",
            " \n",
            "\n",
            "33 Source: “LLaVA-CoT: Let Vision Language Models Reason Step-by-Step,” arXiv, 2025.; Caption: The image displays a flowchart with a series of steps and corresponding labels, alongside a list of terms such as Question, Summary, Caption, Reasoning, and Answer.\n",
            "Caption for /content/drive/MyDrive/pages/page_208.png-- OCR: Example 1: Reasoning Problems\n",
            "\n",
            " \n",
            "\n",
            "Question: Subtract all tiny shiny balls. Subtract all\n",
            "purple objects. How many objects are left?\n",
            "\n",
            "Options: A. 4, B. 8, C. 2, D. 6\n",
            "\n",
            "Example 2: Science Problems\n",
            "\n",
            "400 mm\n",
            "P=2kN\n",
            "\n",
            " \n",
            "\n",
            "200mm\n",
            "\n",
            "Question:\n",
            "\n",
            "An acetal polymer block is fixed to the rigid plates at\n",
            "its top and bottom surfaces. If the top plate displaces 2}\n",
            "mm horizontally when it is subjected to a horizontal\n",
            "force P=2 kN, determine the shear modulus of the\n",
            "polymer. The width of the block is 100 mm. Assume\n",
            "that the polymer is linearly elastic and use small angle}\n",
            "analysis.\n",
            "\n",
            "Options:\n",
            "A: 15 MPa, B: 5 MPa, C: 20 MPa, D: 2 MPa\n",
            "\n",
            "Answer from Llama-3.2-11B-Vision-Instruct: | Answer from LLaVA-CoT:\n",
            "Summary stage: What's the problem? What should I do?\n",
            "\n",
            "To determine the correct answer, we need to follow the <SUMMARY> I will examine the image to identify and count all the objects, focusing on the tiny\n",
            "\n",
            "instructions provided. shiny balls and purple objects. Then, I will subtract these objects from the total count and determine\n",
            "the number of remaining objects. </SUMMARY>\n",
            "\n",
            "The first instruction is to subtract all tiny shiny balls. Caption stage: What can I know from the image?\n",
            "\n",
            "Since there are three tiny shiny balls in the image, we would | <CApT/ON> The image shows various 3D geometric shapes on a flat surface, including spheres,\n",
            "\n",
            "be left with no shiny balls after performing this action. cubes, and cylinders in different colors. Notably, there are small shiny green spheres and a purple\n",
            "The second instruction is to subtract all purple objects. cylinder. </CAPTION>\n",
            "\n",
            "However, upon closer inspection, it becomes apparent that\n",
            "there are no shiny purple objects present in the image.\n",
            "Therefore, we cannot subtract any purple objects. <REASONING > First, I will count all the objects in the image: there are 10 objects in total.\n",
            "Next, I will identify and subtract the tiny shiny balls. There is one small shiny green sphere.\n",
            "Given that we have already removed all the tiny shiny balls and|Then, I will identify and subtract the purple objects. There is one purple cylinder.\n",
            "there were no purple objects to begin with, the correct answer _|By subtracting the two objects (the shiny ball and the purple cylinder) from the total, I find that 8\n",
            "is C. 2, as initially, there were 2 purple objects, but they are | objects remain. </REASONING>\n",
            "not to be subtracted according to the instruction. Conclusion stage: What is the final answer?\n",
            "<CONCLUSION> B </CONCLUSION>\n",
            "Answer from Llama-3.2-11B-Vision-Instruct: Answer from LLaVA-CoT:\n",
            "‘To determine the shear modulus of the polymer, we can use the|Summary stage: What's the problem? What should I do?\n",
            "formula for shear modulus in a small angle analysis: <SUMMARY>To solve the problem, I will determine the shear modulus of the acetal polymer block\n",
            "\\[G = \\frac(F} {A \\gamma}\\] using the relationship between the applied force, displacement, and dimensions of the block. I will use\n",
            "\n",
            "where \\(G \\) is the shear modulus, \\¢ F \\) is the applied force, | (he formula for shear modulus and calculate the result. </SUMMARY>\n",
            "\n",
            "\\(A)) is the area of the block, and \\( \\gamma\\) is the shear | Caption stage: What can I know from the image?\n",
            "\n",
            "soa | <CAPTION >The image shows a rectangular block of acetal polymer fixed between two rigid plates.\n",
            "Given that the top plate of the block displaces horizontally by | The top plate moves horizontally by 2 mm when a force \\( P = 2\\, \\text( kN} \\) is applied. The block's\n",
            "2 mm when subjected to a horizontal force of 2 KN, we can__| dimensions are given as 400 mm in length, 200 mm in height, and 100 mm in width. </CAPTION>\n",
            "\n",
            "calculate the shear strain \\( \\gamma \\). Since the displacement .\n",
            "IR o> He Ive th rb i.\n",
            "is horizontal and the original length of the block is 400 mm, |*¢4so\"ng stage: How to solve the problem step-by-step\n",
            "\n",
            "[Reasoning stage: How to solve the problem step-by-step?\n",
            "\n",
            "the shear strain \\( \\gamma \\) is: <REASONING >The shear modulus \\( G \\) of the polymer can be calculated using the formula:\n",
            "\\[\\gamma = \\frac{\\Delta x} {L} = \\frac{2 \\, \\text{mm}}{400 \\, |\\l G = Mrac{F}{A \\edot \\fract\\delta}{1}} \\]\n",
            "\\text{mm}} = 0.0051] where:\n",
            "\n",
            "Next, we calculate the area \\( A \\) of the block. The width of _|-\\( F \\) is the applied force (2 KN),\n",
            "the block is given as 100 mm, and assuming the height of the |- \\( A'\\) is the area of the block's top surface (100 mm x 400 mm = 40,000 mm?),\n",
            "\n",
            " \n",
            "\n",
            "block is also 200 mm (as given in the image analysis), the __|- \\( \\delta \\) is the horizontal displacement (2 mm),\n",
            "\n",
            "area \\(A\\) is: -\\(1)) is the height of the block (200 mm).\n",
            "\n",
            "\\[A= 100 \\, \\text{mm)} \\times 200, \\text{mm} = 20000\\, __ | First, convert the force to Newtons: \\( F = 2\\, \\text(kN} = 2000 \\, \\text{IN} \\).\n",
            "\\text{mm}A2\\] Next, calculate the shear modulus:\n",
            "\n",
            "Now, we can substitute these values into the formula for shear |\\{ G = \\frac{2000} (40,000 \\cdot \\frac{2} {200} } \\]\n",
            "\n",
            "modulus: AE G = \\frac{2000} {40,000 \\cdot 0.01} \\]\n",
            "\n",
            "\\[G = \\frac{F}{A \\gamma)} = \\frac{2000 \\, \\text{N}}{20000\\, |\\{ G=\\frac{2000}(400} \\]_— ;\n",
            "\n",
            "\\text{mm}/2 \\times 0.005} = \\frac{2000}{100} = 20\\, \\[ G = 5\\, \\text{MPa} \\] </REASONING>\n",
            "\n",
            "\\text{MPa}\\] Conclusion stage: What is the final answer?\n",
            "Therefore, the correct answer is C: 20 MPa. <CONCLUSION> B, 5 MPa </CONCLUSION>; Caption: The image displays a slide from a presentation, featuring two examples of problem-solving scenarios. The first example involves a reasoning problem with a visual of various geometric shapes, and the second example is a science problem related to calculating the shear modulus of a polymer block. Each example includes a question, a set of options, and detailed answers from two different AI models, Llama-3.2-11B-Vision-Instruct and LLava-CoT, which provide step-by-step reasoning and calculations to arrive at the correct answers.\n",
            "Caption for /content/drive/MyDrive/pages/page_209.png-- OCR: YJ Leap-of-Thought (LoT)\n",
            "mm\n",
            "\n",
            "  \n",
            "\n",
            "* CoT guides MLLM to reason step-by-step, and can motivate their logical\n",
            "reasoning ability. While effective for logical tasks, CoT is not conducive\n",
            "to creative problem-solving which often requires out-of-box thoughts and\n",
            "is crucial for innovation advancements.\n",
            "\n",
            "* CoT is a non-sequential, creative paradigm involving strong associations\n",
            "and knowledge leaps.\n",
            "\n",
            "Think step by step Think outside the box\n",
            "Chain-of-Thought (CoT) Leap-of-Thought (LoT)\n",
            "\n",
            "35 Source: “Let’s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,” CVPR, 2024.; Caption: The image is a slide from a presentation, discussing the concept of 'Leap-of-Thought (LoT)'. It includes text and two diagrams comparing 'Chain-of-Thought (CoT)' and 'Leap-of-Thought (LoT)'. The text explains that CoT guides MLLM to reason step-by-step and is effective for logical tasks but not conducive to creative problem-solving. LoT is described as a non-sequential, creative paradigm involving strong associations and knowledge leaps. The diagrams visually represent the difference between thinking step-by-step and thinking outside the box.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_210.png-- OCR: A Example: Exploring LoT in MLLM with Creative\n",
            "_ Humor Generation\n",
            "\n",
            " \n",
            "\n",
            "* Oogiri (A=) game: Players are required to make surprising\n",
            "and creative humorous responses to the given multimodal\n",
            "information e.g., images, text, or both.\n",
            "\n",
            "   \n",
            "\n",
            "      \n",
            "\n",
            "How many\n",
            "2| fingers do\n",
            "| programmers\n",
            "usually use\n",
            "\n",
            "to write code?\n",
            "\n",
            "(d :\n",
            "Excuse me, Sir\n",
            "4\n",
            "\n",
            "«\n",
            "\n",
            "Would....Would you mind Get out of my way! Two.\n",
            "uncuffing me? My friend is badly hurt! Ctrl+C / Ctrl+V\n",
            "\n",
            "36 Source: “Let’s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,” CVPR, 2024.; Caption: The image displays a slide from a presentation titled 'Example: Exploring LoT in MLLM with Creative Humor Generation.' It explains the Oogiri game, where players create humorous responses to multimodal information such as images, text, or both. The slide is divided into three sections, each representing a different type of input-output scenario: Image & Text to Text (IT2T), Image to Text (I2T), and Text to Text (T2T). Each section contains an example of the game in action. The IT2T section shows an image of two people with a question mark, followed by a humorous response. The I2T section shows an image of a truck, followed by a humorous response. The T2T section shows a text question about programmers' fingers, followed by a humorous response. The source of the information is cited as 'Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,' CVPR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_211.png-- OCR: Example: Exploring LoT in MLLM with Creative\n",
            "\n",
            "Humor Generation\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "© a\n",
            "\n",
            "Text Image\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Instructions\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Qwen-VL\n",
            "\n",
            " \n",
            "\n",
            "37\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Visual Language\n",
            "Model\n",
            "\n",
            " \n",
            "\n",
            "Qwen-VL+ cLot (Ours)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "(EN) Forgot to remove glasses\n",
            "while swimming. @\n",
            "\n",
            "(EN) An apple a day keep the\n",
            "\n",
            "doctor away.\n",
            "\n",
            "(CN) RENEE AER ARTIS\n",
            "K, Cae\n",
            "\n",
            "@ ae this is a i cat with his\n",
            "mouth open ....\n",
            "\n",
            "| (CN) HA SESAME\n",
            "\n",
            "REBAR?\n",
            "\n",
            "@ What?! Going to the pet hospital now\n",
            "\n",
            "Vor a neutering surgery!? Be )\n",
            "\n",
            " \n",
            "\n",
            "(EN) What else can wake you up besides\n",
            "coffee when you are coding?\n",
            "\n",
            "(EN) Maybe you need to enlist the help of some\n",
            "angry bees.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "(EN) A cup of deadline. a\n",
            "\n",
            " \n",
            "\n",
            "(CN) (ROMS SCM — fe SFC FEES?\n",
            "\n",
            "@ Do you think writing a DL paper is exhausting?\n",
            "\n",
            "(CN) sche C STR R ERR\n",
            "\n",
            "@ Completing a paper is actually very enjoyable.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "(NAS, SEWERS\n",
            "\n",
            "@ Not for me, but it's hard for my supervisor.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Source: “Let’s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,” CVPR, 2024.; Caption: This image is a presentation slide from a conference, specifically from CVPR 2024, discussing the exploration of Large Language Models (LLMs) with creative humor generation. The slide is divided into three main sections, each representing a different stage of the process: Visual Language Model, Image to Text (I2T), and Text to Text (T2T). The Visual Language Model section includes a flowchart that shows the input of text and image into a model that generates human-like responses. The I2T section displays examples of text generated from images, with English (EN) and Chinese (CN) examples. The T2T section shows text generated from other text, again with English and Chinese examples. The slide also includes a source citation at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_212.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner\n",
            "\n",
            "m Analogical Reasoning\n",
            "= Multimodal Reasoning\n",
            "= Multimodal CoT\n",
            "\n",
            "38; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide outlines the agenda for the lecture, which includes four main topics: 'Reasoning Basics,' 'Analogical Reasoning,' 'Multimodal Reasoning,' and 'Reasoning Benchmarks.' Under 'Reasoning Basics,' there are two subtopics: 'Chain-of-Thought (CoT)' and 'Zero-shot reasoner,' with 'Few-shot reasoner' as a further subtopic under 'Chain-of-Thought.' 'Multimodal Reasoning' is divided into 'Multimodal CoT' and 'Commonsense Knowledge.' The slide number '38' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_213.png-- OCR: YJ Multimodal Reasoning\n",
            "mm\n",
            "\n",
            "  \n",
            "\n",
            "Example Problem: Spatial\n",
            "\n",
            "i > Please describe the information of\n",
            "\n",
            "ewe\n",
            "I Pe , especially the relationship\n",
            "\n",
            "    \n",
            "\n",
            "\\\n",
            "i\n",
            "i\n",
            "i i\n",
            "i\n",
            "i between 5 te and 6, ,\n",
            "i i\n",
            "\\ i\n",
            "I A man is seen walking down a street — l\n",
            "l with a large orange cat. The cat is I\n",
            "I sitting on the man's head, and it seems I\n",
            "i to be enjoying the walk. I\n",
            ": |\n",
            "\\ wo eee eee ee eee _?¢\n",
            "\n",
            "39 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a slide from a presentation on Multimodal Reasoning, specifically an example problem related to spatial reasoning. The slide includes a text box with a prompt asking to describe the information of a scene, particularly the relationship between two images. Below the prompt, there is a description of a scene where a man is walking down a street with a large orange cat sitting on his head. The source of the content is cited as 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_214.png-- OCR: YJ Multimodal Reasoning\n",
            "mm\n",
            "\n",
            "Example Problem: Logical\n",
            "\n",
            " \n",
            "\n",
            "\\ Please take a closer look at the two\n",
            "\\ images and explain the connection\n",
            "i = between them. ‘\n",
            "\n",
            " \n",
            "    \n",
            "\n",
            "The baby is crying as he broke the\n",
            "cup.\n",
            "\n",
            "XN 4\n",
            "\n",
            "40 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a slide from a presentation on Multimodal Reasoning. It includes a title 'Multimodal Reasoning' and a subtitle 'Example Problem: Logical'. There are two images: one of a broken cup and another of a baby crying. Below the images, there is a text box with a statement 'The baby is crying as he broke the cup.' and a prompt asking to explain the connection between the images. There is also a small robot icon with a speech bubble. The source of the content is cited at the bottom as 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_215.png-- OCR: YJ Multimodal Reasoning\n",
            "mm\n",
            "\n",
            "  \n",
            "\n",
            "Example Problem: Temporal\n",
            "\n",
            "=== = B= B= B= ss sess sess sess sess ss sess sess se see SBP Ss se se se se se Bese sess ss =\n",
            "\n",
            "f\n",
            "\n",
            ": ,\n",
            "\n",
            "| ge What happens to the\n",
            "& man after hitting the\n",
            "I ball?\n",
            "\n",
            "I\n",
            "\\ I\n",
            "\n",
            "Ce ee ee ee\n",
            "\n",
            "“\n",
            "i\n",
            "\n",
            "He falls to the ground in front of a green grassy S i\n",
            "\n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "\n",
            "area with trees and shrubbery surrounding the i\n",
            "spot where he hit the golf ball.\n",
            "\n",
            "41 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a sequence of four photographs depicting a person playing golf. In the first photo, the person is preparing to hit the ball. In the second photo, the person has just hit the ball. In the third photo, the person is falling to the ground. In the fourth photo, the person is lying on the ground. There is also a text box with a question about what happens to the man after hitting the ball, and a speech bubble with an answer.\n",
            "Caption for /content/drive/MyDrive/pages/page_216.png-- OCR: Multimodal Reasoning\n",
            "\n",
            "  \n",
            "\n",
            "Example Problem: Text-to-image reference\n",
            "\n",
            " \n",
            "  \n",
            "     \n",
            "\n",
            "  \n",
            "      \n",
            "\n",
            "ee eee ee ee eBRBaetaeaaeeateaaeautzs =\n",
            "t a ‘\n",
            "I AP Carefully analyze the given images it I\n",
            "& and answer the question: 1 Tell me, is there a horse i ion I\n",
            "I What differences does il l\n",
            "I 3 : i '\n",
            "I piel and ea a It is not possible to tell from the :\n",
            "it image a horse is in the image.\n",
            "\\ have? s i\n",
            "I ut 1\n",
            "I i Here are some examples. 1\n",
            "I Sporty car on one side of road vs. off- oom 1\n",
            "I road jeep with mountainous terrain in it There is a horse in a 5 I\n",
            "I the background on the other hand. A 1\n",
            ". ph\n",
            "\n",
            "reerrers it while there is no horse in me - :\n",
            "\n",
            "CP What similarities does the first and a :\n",
            "e second image have? it Therefore, please refer to = :\n",
            "I af : : 5 I\n",
            "I They has similarity in size and shape af and a I\n",
            "I of vehicle. One is on the road and the it Be 1\n",
            "I other has mountainous terrain with a A I\n",
            "1 snow capped mountains in the it Yes, there is a horse in the image and SS!\n",
            "I foreground. il it's standing on a grassy field. 1\n",
            "\\ i 4\n",
            "\n",
            "ee eee ae Nea meee aunaauaaaaaa ®t\n",
            "\n",
            "Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image displays a slide from a presentation on 'Multimodal Reasoning' with a focus on 'Text-to-image reference'. It includes a dialogue between two characters discussing the differences and similarities between two images of vehicles, one being a sporty car and the other an off-road jeep. The slide also contains a reference to a source titled 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_217.png-- OCR: YJ Multimodal Reasoning\n",
            "mm\n",
            "\n",
            "Example Problem: Text-to-image reference\n",
            "\n",
            "= = == = sees ee ee eeeeeeeeeeeeeeeeseeeeeees eee eee eee eee eB eB BK\n",
            "\n",
            "t\n",
            "\n",
            "‘ g2 ia O These images illustrate the growth\n",
            "& ¢ V4 phases of the tree, please describe the\n",
            "i contents of each image carefully.\n",
            "\n",
            "I a =. a y_Y\n",
            "\n",
            "S\n",
            "i\n",
            "The image 0 is just germinating, the image | is w\n",
            "\n",
            "      \n",
            "\n",
            "only a bare trunk, the image 2 is luxuriant, and\n",
            "the image 3 is a growing plant. I\n",
            "\n",
            " \n",
            "\n",
            "43 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a series of four illustrations depicting the growth phases of a tree.\n",
            "Caption for /content/drive/MyDrive/pages/page_218.png-- OCR: YJ Multimodal Reasoning\n",
            "mm\n",
            "\n",
            "For MLLMs, how to understand text-to-image references?\n",
            "The Image Declaration is proposed as a solution to establish the\n",
            "reference between image and text.\n",
            "\n",
            "Original VL Task (a) Image Declaration (b) Multi-modal Data with Interconnected Images\n",
            "Visual Question Answering\n",
            "Carefully analyze image j: [[MG;] Carefully analyze images to answer the question.\n",
            "g s e . A £ ty ae f\n",
            "es « . qi 0: [/MG 4 B\n",
            "1 « 7 we AN o % to answer the question. In image L ol A « , is image 1: [/MG,] f “a\n",
            "Are the men and women Q Are the men and women are quarrelling with image 2: [MG] Sy 2\n",
            "are quarrelling? eel\n",
            "Answer: Ye a\n",
            "Swers nes rm (c) Unified Multi-modal-in-context Format\n",
            "a\n",
            "Q: The image 0 is [IMG] . Carefully analyze the\n",
            "Image Captioning . .. re,\n",
            "‘The image j is [/MG;] -. image 0 to generate a concise and accurate description that\n",
            "ry\n",
            "\n",
            "accurately represents the objects, people, or scenery present.\n",
            "\n",
            "Carefully analyze image jt\n",
            "eure pee aligned A: An airplane flying in the sky,\n",
            "\n",
            "generate a concise and accurate\n",
            "\n",
            "   \n",
            " \n",
            "\n",
            "i i Ce ee : The image j is [IMG; Carefully analyze the\n",
            "An eerie tepresents the objects, people, and io eds RG : ieee\n",
            "inane shy scenery present. image j to generate a concise and accurate description that\n",
            "accurately represents the objects, people, or scenery present.\n",
            "A\n",
            "“ly Machine Annotation 9 Manual Annotation [JMG] Image Proxy\n",
            "\n",
            "44 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a slide from a presentation discussing 'Multimodal Reasoning' and the concept of 'Image Declaration' as a solution for MLLMs to understand text-to-image references. It includes examples of tasks like Visual Question Answering, Image Captioning, and a format for Multi-modal Data with Interconnected Images. The slide also references a source from 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_219.png-- OCR: 45\n",
            "\n",
            "aN Image Declaration C\n",
            "\n",
            "* Firstly, we allocate a unique image proxy ([IMGj]) to reference\n",
            "the visual embedding of image j, which provides a unique\n",
            "identifier for MLLMs to index and distinguish between visual\n",
            "and text embeddings.\n",
            "\n",
            "¢ Then, we utilize natural language prompts to establish\n",
            "references between text and image.\n",
            "\n",
            "¢ Incorporating the explicit text-to-image reference in the image\n",
            "declaration assists the model in correlating the text with the\n",
            "appropriate image.\n",
            "\n",
            "Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image shows a slide from a presentation with the title 'Image Declaration'. It contains text explaining the process of allocating a unique image proxy, utilizing natural language prompts to establish references between text and image, and incorporating the explicit text-to-image reference in the image declaration. The slide also includes a source citation at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_220.png-- OCR: Multimodal Reasoning\n",
            "\n",
            "  \n",
            "\n",
            "VPG: Visual-Prompt Generators\n",
            "\n",
            "1 1 1 t 1 1 1 te, T_T\n",
            "oe Text Text. Text Text ‘Text. = Text “Text | a ene | : “Text Text (cl embed\n",
            "t t a t--7 t\n",
            "VPG ] VPG ] vPG Image Declaration VPG ] NG }\n",
            "iy Img Img i nl\n",
            "MLLMs focus on a single image MLLMs with few-shot ability MLLMs with text-to-image reference\n",
            "\n",
            "46 Source: “MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,” ICLR, 2024.; Caption: The image is a slide from a presentation titled 'Multimodal Reasoning'. It features a diagram with three sections, each representing a different type of Multimodal Language Model (MLLM). The first section on the left shows a LLM focusing on a single image, the middle section shows a LLM with few-shot ability, and the right section shows a LLM with text-to-image reference. Each section includes a flow of information starting from an image or text input, going through a Visual-Prompt Generator (VPG), and then into the LLM. The diagram uses arrows to indicate the flow of information and includes labels such as 'Text', 'Img', and 'embed' to describe the process. The source of the content is cited as 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,' ICLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_221.png-- OCR: YJ Performance on ‘Winoground’\n",
            "mm\n",
            "\n",
            "  \n",
            "\n",
            "A task of correctly matching two given images and captions\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Cl: Some plants surrounding a lightbulb.\n",
            "\n",
            "L C2: A lightbulb surrounding some plants.\n",
            "\n",
            ". QI: Is the Caption! matches the image1? Table 2: Results on Winoground across text, image\n",
            "Q2: Is the Caption! matches the image2? and group score metrics.\n",
            "Q3: Is the Caption2 matches the image1? Model Text Image Group\n",
            "Q4: Is the Caption2 matches the image2? MTurk Human 89.50 88.50 85.50\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "VQ2 (Yarom et al., 2023) 47.00 42.20 30.50\n",
            "PALI (Chen et al., 2022) 46.50 38.00 28.75\n",
            "Blip-2 (Li et al., 2023d) 44.00 26.00 23.50\n",
            "GPT4-V (Wu et al, 2023) 69.25 46.25 = 39.25\n",
            "\n",
            "a a a a Ya, MMICL (FLAN-TS-XXL) 45.00 45.00 43.00\n",
            "(Cop) (Coty) — 8(Cy,1p) S(Cy, 1) DAME CANTO) SOOO\n",
            "\n",
            "47 Source: “Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,” CVPR, 2022.; Caption: The image shows a lightbulb surrounded by plants.\n",
            "Caption for /content/drive/MyDrive/pages/page_222.png-- OCR: Performance on ‘RAVEN’\n",
            "\n",
            "   \n",
            "\n",
            "AN\n",
            "\n",
            "Task: Each question has 3 or 8 images as input and 6 candidate images\n",
            "with a unique answer, and the goal is to predict the right image\n",
            "\n",
            "Q: Do you agree the following image is:\n",
            "| leJL| @ a\n",
            "Table 3: Zero-shot generalization on Raven IQ test.\n",
            "ca (+) |@| Correct? -- Correct?) .- Correct? Model Accuracy\n",
            "Random Choice 17\n",
            "\n",
            "Answer: P {Yes|Q} InstructBlip (Dai et al., 2023) 10.00\n",
            "\n",
            "a +) Otter (Li et al., 2023a) 22.00\n",
            "KOSMOS-1 (Huang et al., 2023a) 22.00\n",
            "\n",
            "MMICL (FLAN-T5-XXL) 34.00\n",
            "\n",
            "48 Source: “Raven: A dataset for relational and analogical visual reasoning,” CVPR, 2019.; Caption: The image displays a slide from a presentation, which includes a task description, a sample question with multiple-choice answers, and a table comparing the accuracy of different models on the Raven IQ test.\n",
            "Caption for /content/drive/MyDrive/pages/page_223.png-- OCR: Multimodal CoT Reasoning\n",
            "\n",
            " \n",
            "\n",
            "¢ The rationales have a significant impact on answer prediction.\n",
            "\n",
            "e .\n",
            "An example: A drop by over 12%\n",
            "\n",
            "       \n",
            "   \n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "Method Format\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            "    \n",
            "  \n",
            " \n",
            "   \n",
            " \n",
            "\n",
            "Accuracy\n",
            "poe meniscinmencmion: Language ---------------, -------- Vision -------~,\n",
            "' Question: Will these magnets attract or repel each |!\n",
            "\n",
            "' other? t\n",
            "\n",
            "my\n",
            "' Context: Two magnets are placed as shown. Hint: 1!\n",
            "\n",
            "'\n",
            "'\n",
            "\n",
            "'\n",
            "\n",
            "'\n",
            "\n",
            "'B iS t\n",
            "\n",
            "' Magnets that attract pull together. Magnets that i '\n",
            "'\n",
            "\n",
            "'\n",
            "\n",
            "'\n",
            "\n",
            "'\n",
            "\n",
            "' repel push apart.\n",
            "\n",
            "| Ontions: (A) attract (B) repel\n",
            "\n",
            "  \n",
            "\n",
            "Reasoning QCM->RA\n",
            "\n",
            " \n",
            "\n",
            "¢ No-CoT predicts the answer directly (QCM—A)\n",
            "Rati le: Will th ts attract 1? To find out, look at which pol 7 7 7 tt\n",
            "closest to each other. The north pole of one magnet is closest to the south pole ofthe | * Reasoning where answer inference is conditioned to the\n",
            "\n",
            "other magnet. Poles that are different attract. So, these magnets will attract each other.\n",
            "\n",
            "Answer: The answer is (A). , rationale (QCM—RA);\n",
            "\n",
            " \n",
            "\n",
            "49 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image is a slide from a presentation titled 'Multimodal CoT Reasoning'. It discusses the impact of rationales on answer prediction, providing an example of a question about magnets attracting or repelling each other. The slide compares two methods: 'No-CoT' and 'Reasoning', with their respective formats and accuracy percentages. The 'No-CoT' method has an accuracy of 81.63%, while the 'Reasoning' method has a lower accuracy of 69.32%. The slide also includes a visual representation of the magnets' poles and a rationale explaining why the magnets attract each other. The source of the information is cited as 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_224.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "¢ Multimodal-CoT is a proposed mechanism to incorporate language (text)\n",
            "and vision (images) modalities into a two-stage framework that\n",
            "\n",
            "separates rationale generation and answer inference.\n",
            "¢ Both stages share the same model structure but differ in the input and output.\n",
            "\n",
            "Vision Rationale\n",
            "\n",
            "- ” Look at each object. For each\n",
            "Py object, decide if it has that\n",
            "property. Potato chips have a\n",
            "\n",
            "“ = salty taste. Both objects are\n",
            "\n",
            "Rationale f Answer\n",
            ". salty. A soft object changes The answer\n",
            "Generation || S21¥,,4 0% object changes inference |) aay\n",
            "\n",
            "The fries are soft, but the\n",
            "\n",
            "Question: Which property do these two cracker is not. The property\n",
            "objects have in common? that both objects have in\n",
            "Context: Select the better answer. common is salty.\n",
            "\n",
            "Options: | (A) soft (B) salty ]\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "50 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image is a slide from a presentation on Multimodal CoT Reasoning. It explains the concept of Multimodal-CoT, which is a proposed mechanism to incorporate language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. Both stages share the same model structure but differ in the input and output. The slide includes a flowchart with three main components: Vision, Rationale Generation, and Answer Inference. The Vision component shows an image of a cracker and fries, the Rationale Generation component contains a text box with a rationale explaining the reasoning process, and the Answer Inference component shows the final answer 'The answer is (B) salty.' The slide also includes a question and context box at the bottom left, asking which property the two objects have in common, with options (A) soft and (B) salty.\n",
            "Caption for /content/drive/MyDrive/pages/page_225.png-- OCR: Paradigms to achieve Multimodal CoT\n",
            "\n",
            "  \n",
            "\n",
            "Question\n",
            "\n",
            "  \n",
            "  \n",
            " \n",
            "\n",
            "Caption\n",
            "Model\n",
            "\n",
            "       \n",
            "\n",
            "(a) Caption-based Reasoning (b) CoT-based Reasoning\n",
            "\n",
            "51 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image displays a slide titled 'Paradigms to achieve Multimodal CoT' with two flowcharts labeled (a) Caption-based Reasoning and (b) CoT-based Reasoning. Each flowchart consists of a sequence of steps starting with 'Image' and 'Question' at the top, followed by 'Caption' or 'Model', then 'Prompted Text' or 'Rationale', leading to 'LLM' or 'Model', and finally resulting in 'Answer'. The flowcharts are color-coded with green, yellow, and purple boxes. The source of the content is cited as 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_226.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "* In addition, multimodality contributes to effective rationales\n",
            "\n",
            "—— One-stage Baseline —-— One-stage Multimodal : . .\n",
            "Baseline: without vision features\n",
            "—+ Two-Stage Baseline -C— Two-Stage Multimodal\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "90\n",
            "gg 2 2 ee B w/o Vision Modality N w/ Vision Modality\n",
            "80\n",
            "> 90.45\n",
            "Qo 90 |\n",
            "5 & 85.31\n",
            "5 70 ~ 83.97\n",
            "9 8\n",
            "s 2 so) 78.57 |\n",
            "60 *\n",
            "70\n",
            "base large\n",
            "1 2 3 4 5 6 7 8 9 10 (200M) (700M)\n",
            "\n",
            "Epoch\n",
            "\n",
            "52 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: Multimodal CoT Reasoning\n",
            "Caption for /content/drive/MyDrive/pages/page_227.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "Problem\n",
            "\n",
            "Question: Which solution has a higher concentration of yellow particles?\n",
            "Context: The diagram below is a model of two solutions. Each yellow ball represents one particle of solute.\n",
            "\n",
            " \n",
            "\n",
            "Options: (A) Solution B | | (B) Solution A | | (C) neither; their concentrations are the same\n",
            "\n",
            " \n",
            "\n",
            "Gold Rationale: In Solution A and Solution B, the yellow particles represent the solute. To figure out which\n",
            "solution has a higher concentration of yellow particles, look at both the number of yellow particles and the volume\n",
            "of the solvent in each container. Use the concentration formula to find the number of yellow particles per milliliter.\n",
            "Solution B has more yellow particles per milliliter. So, Solution B has a higher concentration of yellow particles.\n",
            "\n",
            "Solvent volume: 30 mL. Solvent volume: 30 mL.\n",
            "\n",
            "if\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "'\n",
            "'\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "\n",
            "Solution A Solution B\n",
            "Answer: The answer is (A). Sosa eee eer cee eee ,\n",
            "\n",
            "Baseline\n",
            "\n",
            "Rationale: In Solution A and Solution B, the yellow particles represent Rationale: In Solution A and Solution B, the yellow particles represent\n",
            "the solute. To figure out which solution has a higher concentration of the solute. To figure out which solution has a higher concentration of\n",
            "yellow particles, look at both the number of yellow particles and the yellow particles, look at both the number of yellow particles and the\n",
            "volume of the solvent in each container. Use the concentration formula volume of the solvent in each container. Use the concentration formula\n",
            "to find the number of yellow particles per milliliter. Solution A has more to find the number of yellow particles per milliliter. Solution B has more\n",
            "yellow particles per milliliter. So, Solution A has a higher concentration yellow particles per milliliter. So, Solution B has a higher concentration\n",
            "of yellow particles. of yellow particles.\n",
            "\n",
            "Answer: The answer is (B). Answer: The answer is (A).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "+ Vision Features\n",
            "\n",
            "      \n",
            "\n",
            " \n",
            "\n",
            "53 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image displays a slide from a presentation on Multimodal CoT Reasoning. It includes a problem statement asking which solution has a higher concentration of yellow particles, with two options provided. There are two diagrams representing two solutions with yellow particles, and two text boxes with reasoning and answers. The slide also includes a source citation at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_228.png-- OCR: Multimodal CoT Reasoning\n",
            "\n",
            "Question: Think about the magnetic force between the magnets in each pair. Which of the\n",
            "following statements is true?\n",
            "\n",
            "Context: The images below show two pairs of magnets. The magnets in different pairs do\n",
            "not affect each other. All the magnets shown are made of the same material.\n",
            "\n",
            "Options:\n",
            "\n",
            " \n",
            "\n",
            "(A) The magnitude of the magnetic force is smaller in Pair 2.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "_ (B) The magnitude of the magnetic force is smaller in Pair 1.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "(C) The magnitude of the magnetic force is the same in both pairs.\n",
            "\n",
            " \n",
            "\n",
            "Gold Rationale: The magnets in Pair 1 attract. The magnets in Pair 2 repel. But whether the magnets attract or repel affects only the direction of\n",
            "the magnetic force. It does not affect the magnitude of the magnetic force. Both magnet sizes and distance affect the magnitude of the magnetic\n",
            "force. The sizes of the magnets in Pair 1 are the same as in Pair 2. The distance between the magnets is also the same. So, the magnitude of the\n",
            "\n",
            "magnetic force is the same in both pairs.\n",
            "Answer: The answer is (C).\n",
            "\n",
            "Baseline\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "Rationale: Distance affects the magnitude of the magnetic force.\n",
            "When there is a greater distance between magnets, the magnitude of\n",
            "the magnetic force between them is smaller. There is a greater\n",
            "distance between the magnets in Pair 2 than in Pair 1. So, the\n",
            "magnitude of the magnetic force is smaller in Pair 2 than in Pair 1.\n",
            "Answer: The answer is (B).\n",
            "\n",
            "Rationale: The magnets in Pair 1 attract. The magnets in Pair 2 repel.\n",
            "But whether the magnets attract or repel affects only the direction of\n",
            "the magnetic force. It does not affect the magnitude of the magnetic\n",
            "force. Both magnet sizes and distance affect the magnitude of the\n",
            "magnetic force. The sizes of the magnets in Pair 1 are the same as in\n",
            "Pair 2. The distance between the magnets is also the same. So, the\n",
            "magnitude of the magnetic force is the same in both pairs.\n",
            "\n",
            "Answer: The answer is (C).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image displays a slide from a presentation on Multimodal CoT Reasoning. It includes a problem statement about the magnetic force between magnets in different pairs, with options to choose from. There are two diagrams showing pairs of magnets, one with the magnets attracting and the other with them repelling. The slide also contains a gold rationale explaining why the magnitude of the magnetic force is the same in both pairs, and a baseline rationale that incorrectly states the force is smaller in Pair 2 due to greater distance. The source of the content is cited as 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_229.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "¢ Error Analysis\n",
            "\n",
            "¢ The most prevalent error type is commonsense mistakes, accounting for 80% of the\n",
            "errors. These mistakes occur when the model is faced with questions that require\n",
            "commonsense knowledge, such as interpreting maps, counting objects in images,\n",
            "\n",
            "or utilizing the alphabet.\n",
            "\n",
            "= Commonsense\n",
            "= Logical\n",
            "\n",
            "= Others\n",
            "\n",
            " \n",
            "\n",
            "55 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image is a slide from a presentation titled 'Multimodal CoT Reasoning'. It features a pie chart and a list of bullet points. The pie chart is divided into three segments, representing different types of errors in a model. The largest segment, colored in light blue, represents 'Commonsense' errors, accounting for 80% of the errors. The second segment, in dark blue, represents 'Logical' errors, making up 14% of the errors. The smallest segment, in orange, represents 'Others' and accounts for 6% of the errors. The bullet points list that the most prevalent error type is commonsense mistakes, which occur when the model is faced with questions that require commonsense knowledge, such as interpreting maps, counting objects in images, or utilizing the alphabet. The source of the information is cited as 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_230.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "« Examples of commonsense mistakes: About maps\n",
            "\n",
            "Question: Which of these cities is marked on the map?\n",
            "\n",
            "Context: N/A\n",
            "\n",
            "Options: (A) Las Vegas (B) San Francisco | (C) Phoenix (D) Salt Lake City |\n",
            "Rationale : The city is Las Vegas, Nevada. Salt Lake City, Phoenix, and San Francisco are marked with gray circles on\n",
            "\n",
            "the map below.\n",
            "Answer: The answer is (A).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Rationale: The city is San Francisco, California. Salt Lake City, Las Vegas, and Phoenix are marked with gray circles on the map below.\n",
            "Answer: The answer is (B).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "56 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image displays a slide from a presentation titled 'Multimodal CoT Reasoning' with a focus on 'Examples of commonsense mistakes: About maps'. It includes a problem statement asking which city is marked on a map, with four options provided. Below the problem, there are two sections labeled 'Rationale' and 'Answer', each containing a text box with a statement and a conclusion. The slide also features a small map in the top right corner, showing the United States with a highlighted area indicating the location of the cities mentioned in the problem. The source of the content is cited at the bottom as 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n",
            "Caption for /content/drive/MyDrive/pages/page_231.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            " \n",
            "\n",
            "¢« Examples of commonsense mistakes: Counting numbers in the image\n",
            "A Problem ~\n",
            "\n",
            "Question: Complete the text to describe the diagram. Solute particles moved in both directions across the permeable\n",
            "\n",
            " \n",
            "\n",
            "the particles reached equilibrium.\n",
            "\n",
            "Context: The diagram below shows a solution with one solute. Each solute particle is represented by a green ball. The\n",
            "solution fills a closed container that is divided in half by a membrane. The membrane, represented by a dotted line, is\n",
            "permeable to the solute particles. The diagram shows how the solution can change over time during the process of diffusion.\n",
            "\n",
            "Options: —_(A) to the left than to the right (B) to the right than to the left\n",
            "\n",
            "Rationale : (extract) It shows you how the solution changed during the process of diffusion. Before the solute particles\n",
            "reached equilibrium, there were 3 solute particles on the left side of the membrane and 5 solute particles on the right side\n",
            "of the membrane. When the solute particles reached equilibrium, there were 4 solute particles on each side of the\n",
            "membrane. There was 1 more solute particle on the left side of the membrane than before. So, for the solute particles to\n",
            "reach equilibrium, more solute particles must have moved across the membrane to the left than to the right.\n",
            "\n",
            "Answer: The answer is (A).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Rationale: (extract) It shows you how the solution changed during the process of diffusion. Before the solute particles reached equilibrium, there were\n",
            "5 solute particles on the left side of the membrane and 4 solute particles on the right side of the membrane. When the solute particles reached\n",
            "equilibrium, there were 5 solute particles on each side of the membrane. There was 1 more solute particle on the right side of the membrane than\n",
            "before. So, for the solute particles to reach equilibrium, more solute particles must have moved across the membrane to the right than to the right.\n",
            "Answer: The answer is (B).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "57 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image shows a diagram of a solution with solute particles represented by green balls, separated by a membrane.\n",
            "Caption for /content/drive/MyDrive/pages/page_232.png-- OCR: YJ Multimodal CoT Reasoning\n",
            "mm\n",
            "\n",
            "¢« Examples of commonsense mistakes: About utilizing alphabet\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Question: Which word would you find on a dictionary page with the following guide words? helping - hunter co Vision ---,\n",
            "Context: The diagram below shows a solution with one solute. Each solute particle is represented by a green ball. The ; N/A\n",
            "solution fills a closed container that is divided in half by a membrane. The membrane, represented by a dotted line, is '\n",
            "\n",
            "permeable to the solute particles. The diagram shows how the solution can change over time during the process of diffusion.\n",
            "\n",
            "Options: (A) hospital | (B) hardship\n",
            "Rationale : Put the words in alphabetical order. Since hospital is between the guide words helping - hunter, it would be found on that page.\n",
            "Answer: The answer is (A).\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Rationale: Put the words in alphabetical order. Since hardship is between the guide words helping - hunter, it would be found on that page.\n",
            "\n",
            "Answer: The answer is (B).\n",
            "\n",
            " \n",
            "\n",
            "58 Source: “Multimodal Chain-of-Thought Reasoning in Language Models,” TMLR, 2024.; Caption: The image displays a slide from a presentation titled 'Multimodal CoT Reasoning' with a focus on 'Examples of commonsense mistakes: About utilizing alphabet'. It contains a problem statement about finding a word on a dictionary page with given guide words, and two options for answers. The slide also includes a rationale for each answer, explaining the alphabetical order of words. There is a source citation at the bottom for 'Multimodal Chain-of-Thought Reasoning in Language Models,' TMLR, 2024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_233.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner\n",
            "\n",
            "m Analogical Reasoning\n",
            "\n",
            "= Multimodal Reasoning\n",
            "= Multimodal CoT\n",
            "= Commonsense Knowledge\n",
            "\n",
            "59; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. The slide is numbered 59 and contains a list of topics that will be covered in the lecture. The topics are organized into four main categories: 'Reasoning Basics,' 'Analogical Reasoning,' 'Multimodal Reasoning,' and 'Reasoning Benchmarks.' Under 'Reasoning Basics,' there are two subtopics: 'Chain-of-Thought (CoT)' and 'Zero-shot reasoner,' with 'Few-shot reasoner' as a further subtopic under 'Chain-of-Thought.' 'Analogical Reasoning' and 'Multimodal Reasoning' are listed as standalone categories. 'Multimodal Reasoning' has two subtopics: 'Multimodal CoT' and 'Commonsense Knowledge.' The slide has a simple design with a white background and black text, and there are no images or charts present.\n",
            "Caption for /content/drive/MyDrive/pages/page_234.png-- OCR: * Visual Commonsense Reasoning (VCR): From Recognition to Cognition\n",
            "Why is [person4 fal] pointing at [person ea\n",
            "\n",
            "] that [person1 @ ordered the pancakes.\n",
            "\n",
            " \n",
            "\n",
            "a) He is telling [\n",
            "b) He just told a joke.\n",
            "\n",
            "c) He is feeling accusatory towards [person1 @.\n",
            "d) He is giving [person1 ei directions.\n",
            "\n",
            "a) [person1 ] has the pancakes in front of him.\n",
            "Cho b) Ipeheon ] is taking everyone's order and asked for clarification.\n",
            "bec ane® a) c) )] is looking at the pancakes and both she and\n",
            "Se. are smiling slightly.\n",
            "42))] is delivering food to the table, and she might not\n",
            "know whose order is whose.\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "the rson2\n",
            "dq)\n",
            "\n",
            " \n",
            "\n",
            "How did [person2 fq] get the money that’s in front of her?\n",
            "\n",
            "a) [person2 a ] is selling things on the street.\n",
            "\n",
            "b) [person2 Kal ] earned this money playing music.\n",
            "c) She may work jobs for the mafia.\n",
            "\n",
            "d) She won money playing poker.\n",
            "\n",
            "/\n",
            "bee\"se 4 a) She is playing guitar for money.\n",
            "2s, b) [person2 §@ ] is a professional musician in an orchestra.\n",
            "c) [person2 [)] and [person’ [@ Jare both holding instruments,\n",
            "\n",
            "and were probably busking for that money.\n",
            "d) [person1 @) is putting money in [person2 Aa ]'s tip jar, while\n",
            "she plays music.\n",
            "\n",
            " \n",
            "\n",
            "60 Source: “From Recognition to Cognition: Visual Commonsense Reasoning,” CVPR, 2019.; Caption: The image shows a slide from a presentation on Visual Commonsense Knowledge, specifically discussing Visual Commonsense Reasoning (VCR). It includes two photographs with overlaid text boxes and annotations. The top photo depicts a group of people sitting around a table with food, and the bottom photo shows a person playing a musical instrument. Both photos have annotations indicating the actions and interactions of the people involved. The slide also contains a list of questions and answers related to the actions depicted in the photos, demonstrating how VCR involves reasoning from recognition to cognition.\n",
            "Caption for /content/drive/MyDrive/pages/page_235.png-- OCR: ¢ VisualCOMET: Cognitive Image Understanding via Visual Commonsense Graphs\n",
            "\n",
            " \n",
            "   \n",
            "  \n",
            "   \n",
            "\n",
            " \n",
            "  \n",
            "\n",
            "Save\n",
            "himself from\n",
            "drowning.\n",
            "\n",
            "    \n",
            " \n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "\n",
            "     \n",
            "  \n",
            "\n",
            "Wait for\n",
            "help to arrive.\n",
            "\n",
            "Try to help\n",
            "[Person2].\n",
            "\n",
            "   \n",
            "   \n",
            "\n",
            "Sink in\n",
            "the water.\n",
            "\n",
            "   \n",
            " \n",
            "    \n",
            "  \n",
            "\n",
            "Notice water\n",
            "washing in.\n",
            "\n",
            "   \n",
            " \n",
            "  \n",
            "  \n",
            "\n",
            " \n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            "\n",
            "Swim\n",
            "towards the\n",
            "\n",
            "ON ela M\n",
            "statute.\n",
            "will most likely ... Because Person2\n",
            "\n",
            "wanted to ... —4 Sense his\n",
            "ee own death.\n",
            "\n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            "\n",
            " \n",
            "   \n",
            "  \n",
            "\n",
            "Ld\n",
            "ee\n",
            "\n",
            "  \n",
            "   \n",
            "  \n",
            "\n",
            "Stole eed\n",
            "LIfe TU\n",
            "\n",
            "Person1 wanted\n",
            "\n",
            "needed to\n",
            "\n",
            "  \n",
            "\n",
            "Scream\n",
            "for help.\n",
            "\n",
            "Ee ee oun iel y ON eto\n",
            "Get caught needed to , Taree\n",
            "\n",
            "     \n",
            "\n",
            "in a rush of\n",
            "water.\n",
            "\n",
            "  \n",
            "     \n",
            "  \n",
            "\n",
            " \n",
            "     \n",
            " \n",
            "\n",
            "Be washed\n",
            "away.\n",
            "\n",
            "Start moving\n",
            "against the water.\n",
            "\n",
            "    \n",
            " \n",
            "\n",
            "61 Source: “VisualCOMET: Reasoning About the Dynamic Context of a Still Image,” ECCV, 2020.; Caption: The image displays a flowchart titled 'Visual Commonsense Knowledge' with the subtitle 'VisualCOMET: Cognitive Image Understanding via Visual Commonsense Graphs.' The flowchart consists of various colored circles connected by arrows, each containing text that describes actions or intentions of different individuals in a water-related scenario. The circles are color-coded and contain phrases such as 'Swim to safety,' 'Try to help [Person2],' and 'Get to the top.' The flowchart appears to represent a sequence of events or actions taken by individuals in a situation where someone is drowning. The source of the image is cited as 'VisualCOMET: Reasoning About the Dynamic Context of a Still Image,' ECCV, 2020.\n",
            "Caption for /content/drive/MyDrive/pages/page_236.png-- OCR: YJ Visual Commonsense Knowledge\n",
            "mA\n",
            "\n",
            "  \n",
            "\n",
            "¢ VisualCOMET: Cognitive Image Understanding via Visual Commonsense Graphs\n",
            "\n",
            "se a\n",
            ".\n",
            "\n",
            "Leroy\n",
            "\n",
            " \n",
            "\n",
            ": Event: [Person2] is holding onto a bronze statue :\n",
            "while waves of water crash around him.\n",
            "\n",
            "= Place: Inside a sinking ship :\n",
            "gue EEE\n",
            "\n",
            " \n",
            "\n",
            "Lotte ee ela Pim ed\n",
            "ecco Peace\n",
            "\n",
            "Te ee ead\n",
            "needed to ...\n",
            "\n",
            "‘Swim towards Sense his Wait for help Hold on for Gasp for air. : fi\n",
            "the statute. ‘own death. to arrive. his life. jasp for air. ec for\n",
            "Notice water Save himself from a atk\n",
            "washing in. drowning. }e washed away.\n",
            "\n",
            "62 Source: “VisualCOMET: Reasoning About the Dynamic Context of a Still Image,” ECCV, 2020.; Caption: The image shows a person holding onto a bronze statue while being in a sinking ship.\n",
            "Caption for /content/drive/MyDrive/pages/page_237.png-- OCR: 63\n",
            "\n",
            " \n",
            "\n",
            "What happened?\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children, holding up signs with messages such as 'NO Mandate!', 'My Kids Choice!', 'We are Parents Not Government!', and 'No Domestic Violence'. The setting appears to be outdoors, possibly a park or a public gathering, with trees and grass visible in the background.\n",
            "Caption for /content/drive/MyDrive/pages/page_238.png-- OCR: 64\n",
            "\n",
            " \n",
            "\n",
            "What happened?\n",
            "\n",
            "Yes! A protest.\n",
            "What are they protesting for?\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children, holding signs with messages such as 'NO Mandate!' and 'No Domestic Terrorism.' There is also a sign with a crossed-out syringe, indicating a protest against a mandate. The setting appears to be outdoors, possibly in a park or public space.\n",
            "Caption for /content/drive/MyDrive/pages/page_239.png-- OCR: 65\n",
            "\n",
            " \n",
            "\n",
            "What happened?\n",
            "\n",
            "Yes! A protest.\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children, holding signs with messages against vaccines. There are also images of a syringe and a vaccine vial crossed out with a red 'X'.\n",
            "Caption for /content/drive/MyDrive/pages/page_240.png-- OCR: 66\n",
            "\n",
            " \n",
            "\n",
            "What happened?\n",
            "\n",
            "Yes! A protest.\n",
            "\n",
            "What are they protesting for?\n",
            "\n",
            "\\ we\n",
            "vaccine Cis br\n",
            "\n",
            "BR\n",
            "7\n",
            "\n",
            "negation $¢ @\n",
            "ms\n",
            "\n",
            " \n",
            "\n",
            "Source: “\n",
            "\n",
            " \n",
            "\n",
            "Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023; Caption: The image shows a group of people, including children, holding signs with messages such as 'NO Mandate!' and 'Parents Not Domestic Terrorists.' There are also images of a vaccine, a syringe, and a prohibition sign, indicating a protest against vaccine mandates.\n",
            "Caption for /content/drive/MyDrive/pages/page_241.png-- OCR: 67\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding up signs with messages related to gun control and parental rights.\n",
            "Caption for /content/drive/MyDrive/pages/page_242.png-- OCR: 68\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Vision\n",
            "\n",
            "Object\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding up signs with messages written on them.\n",
            "Caption for /content/drive/MyDrive/pages/page_243.png-- OCR: 69\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Vision\n",
            "\n",
            "Object\n",
            "Relation\n",
            "\n",
            "Scene Graph\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to immigration and children's rights.\n",
            "Caption for /content/drive/MyDrive/pages/page_244.png-- OCR: 70\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Object\n",
            "\n",
            " \n",
            "\n",
            "Relation\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Scene Graph\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to immigration.\n",
            "Caption for /content/drive/MyDrive/pages/page_245.png-- OCR: 71\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Object Entity\n",
            "\n",
            " \n",
            "\n",
            "Relation\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Scene Graph\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to immigration.\n",
            "Caption for /content/drive/MyDrive/pages/page_246.png-- OCR: Object Entity\n",
            "Relation Relation\n",
            "Scene Graph Entity-Relation Graph\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "72 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to immigration.\n",
            "Caption for /content/drive/MyDrive/pages/page_247.png-- OCR: 73\n",
            "\n",
            " \n",
            "\n",
            "Object Entity\n",
            "\n",
            " \n",
            "\n",
            "Tati\n",
            "\n",
            "5 Relation Relation\n",
            "centric\n",
            "\n",
            " \n",
            "\n",
            "Scene Graph Entity-Relation Graph\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to a political stance.\n",
            "Caption for /content/drive/MyDrive/pages/page_248.png-- OCR: 74\n",
            "\n",
            "    \n",
            "    \n",
            "      \n",
            " \n",
            "    \n",
            "\n",
            "Object Entity\n",
            "\n",
            " \n",
            "\n",
            "Tati\n",
            "\n",
            "5 Relation\n",
            "centric\n",
            "\n",
            "Relation\n",
            "\n",
            " \n",
            "\n",
            "Scene Graph Entity-Relation Graph\n",
            "\n",
            " \n",
            "\n",
            "State-of-the-art Captioner\n",
            "\n",
            "Answer\n",
            "a woman holding a sign in front of a group of people.\n",
            "a woman holding a sign while standing in a park.\n",
            "\n",
            "a woman holding a sign in front of a crowd.\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to a political campaign.\n",
            "Caption for /content/drive/MyDrive/pages/page_249.png-- OCR: What happened?\n",
            "\n",
            "Protest\n",
            "\n",
            "Protest\n",
            "\n",
            "  \n",
            "\n",
            "1 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children, holding signs with messages such as 'NO Choice' and 'We are Parents Not Domestic Terrorists.' They appear to be participating in a protest.\n",
            "Caption for /content/drive/MyDrive/pages/page_250.png-- OCR: What happened?\n",
            "Who?\n",
            "Protest\n",
            "\n",
            " \n",
            " \n",
            "  \n",
            "\n",
            "PS e @ banner\n",
            "paren ;\n",
            "child @ e 4 Oe\n",
            "\n",
            "vcrid @, @ Benner lines\n",
            "banner re Tr\n",
            "\n",
            "Protest\n",
            "\n",
            "child\n",
            "\n",
            "   \n",
            "\n",
            "banner\n",
            "\n",
            "No vaccine\n",
            "\n",
            "mandate for kids\n",
            "eo Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a protest with children holding signs. The signs have messages such as 'NO Mandate for Kids' and 'No Vaccine Mandate for Kids'. There is a parent and a child visible in the image.\n",
            "Caption for /content/drive/MyDrive/pages/page_251.png-- OCR: What happened?\n",
            "Who?\n",
            "Protest\n",
            "\n",
            "paren\n",
            "\n",
            "child\n",
            "\n",
            "Protest\n",
            "\n",
            "    \n",
            "\n",
            "Protester\n",
            "\n",
            " \n",
            "\n",
            "Protester child\n",
            "\n",
            "banner\n",
            "\n",
            "No vaccine\n",
            "mandate for kids\n",
            "\n",
            "—[—————\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a protest with children holding signs. The signs have messages such as 'NO' and 'NO Mandate for kids'. There is a table on the left side of the image with the headings 'Event', 'Protester', and 'banner', and the text 'No vaccine mandate for kids' under the 'Event' column. On the right side, there is a diagram with the question 'What happened?' and arrows pointing to 'Protester', 'parent', 'child', and 'banner'. The source of the image is cited as 'Knowledge-Driven Vision-Language Encoding,' Tutorial, CVPR, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_252.png-- OCR: Protest\n",
            "\n",
            "   \n",
            "\n",
            "Protester parent\n",
            "Protester child\n",
            "Tool banner\n",
            "\n",
            "No vaccine\n",
            "mandate for kids\n",
            "\n",
            "— ooo\n",
            "\n",
            "Topic\n",
            "\n",
            "     \n",
            "\n",
            "What happened?\n",
            "Who?\n",
            "\n",
            "Protest\n",
            "\n",
            "     \n",
            "\n",
            "paren\n",
            "\n",
            "child\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children and adults, holding up signs with messages such as 'NO Mandate for Kids' and 'We are Parents Not Government.' The setting appears to be an outdoor event, possibly a protest, with trees and a clear sky in the background. There is also a diagram on the right side of the image that breaks down the components of an event, including the protest, the protesters, the tools used, and the topic being addressed. The bottom of the image includes a source credit to 'Knowledge-Driven Vision-Language Encoding,' CVPR, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_253.png-- OCR: 79\n",
            "\n",
            "Entity\n",
            "\n",
            "Vision\n",
            "\n",
            "Object\n",
            "\n",
            " \n",
            "\n",
            "Pelee\n",
            "\n",
            ". Relation\n",
            "centric\n",
            "\n",
            "Relation\n",
            "\n",
            " \n",
            "\n",
            "Entity-Relation Graph\n",
            "\n",
            "Verb\n",
            "V(eYa\n",
            "\n",
            "Scene Graph\n",
            "\n",
            "Activity\n",
            "\n",
            " \n",
            "\n",
            "centric\n",
            "Event Structure\n",
            "\n",
            " \n",
            "\n",
            "Image Event Graph\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people holding signs with messages related to children's rights and education.\n",
            "Caption for /content/drive/MyDrive/pages/page_254.png-- OCR: State-of-the-art Captioner\n",
            "\n",
            "  \n",
            " \n",
            "    \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "\n",
            "of\n",
            "P yf Answer\n",
            "ea\n",
            "fi a woman holding a sign in front of a group of people.\n",
            "\n",
            "man holding a sign while standing in a park.\n",
            "\n",
            "| | “Sale Stag\n",
            "\n",
            "fex-Vajtatemman holding a sign in front of a crowd.\n",
            "\n",
            "Protest\n",
            "\n",
            "V(eYa\n",
            "centric\n",
            "\n",
            "  \n",
            "\n",
            "80 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a group of people, including children and adults, holding signs with messages such as 'NO Mandate!' and 'We are Parents Not Teachers.' The setting appears to be an outdoor area with trees and grass, suggesting a public gathering or protest.\n",
            "Caption for /content/drive/MyDrive/pages/page_255.png-- OCR: 81\n",
            "\n",
            "Caption Text t\n",
            "\n",
            " \n",
            "\n",
            ": Antigovernment protesters carry an\n",
            ": injured man on a stretcher after\n",
            "\n",
            ": Clashes with riot police on\n",
            "\n",
            ": Independence Square in Kyiv on\n",
            "\n",
            ": February 20, 2014.\n",
            "\n",
            " \n",
            "    \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image displays a slide titled 'Event-Driven Vision-Language Pretraining' with a caption text box and a flow diagram. The caption text box contains a sentence about antigovernment protesters carrying an injured man on a stretcher after clashes with riot police. The flow diagram illustrates the process of 'Text Information Extraction' leading to the identification of 'Event Type,' 'Transport (carry),' 'Agent,' 'Target,' and 'Instrument.' The source of the content is cited as 'Knowledge-Driven Vision-Language Encoding,' Tutorial, CVPR, 2023.\n",
            "Caption for /content/drive/MyDrive/pages/page_256.png-- OCR: Transfer text event knowledge to images\n",
            "cepmon ie\n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "   \n",
            "\n",
            "     \n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            " \n",
            "\n",
            "| Antigovernment protesters carry an Text fAgent | protesters\n",
            ": injured man on a stretcher after Information\n",
            ": Clashes with riot police on Extraction\n",
            "\n",
            ": Independence Square in Kyiv on i\n",
            ": February 20, 2014. :\n",
            "\n",
            "Instrument [stretcher |\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "Image i\n",
            "\n",
            "' i ' geal Tet)\n",
            "ete tae) econ\n",
            "\n",
            "Weakly Supervision\n",
            "\n",
            "eed)\n",
            "\n",
            "TTT)\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a protest scene with individuals labeled as 'Person' and a 'Bench'.\n",
            "Caption for /content/drive/MyDrive/pages/page_257.png-- OCR: Image i\n",
            "tek.\n",
            "u = Person\n",
            "ee Be) Aeon\n",
            "Eola]\n",
            "TUT)\n",
            "83\n",
            "\n",
            "Hard negatives via manipulating event structures\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "gon — [press\n",
            "[Instrument | stretcher\n",
            "\n",
            "Positive\n",
            "Labels\n",
            "\n",
            "Event Type | Arrest |\n",
            "gent protesters\n",
            "\n",
            "Negative — [Agent |\n",
            "\n",
            "(events) [Target injuredman\n",
            "\n",
            " \n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a protest scene with people labeled as 'Person' and a bench labeled as 'Bench'.\n",
            "Caption for /content/drive/MyDrive/pages/page_258.png-- OCR: Hard negatives via manipulating event structures\n",
            "\n",
            "Event Type | Transport (carry)\n",
            "‘Agent [protesters |\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            "  \n",
            "\n",
            "Positive\n",
            "[Instrument [stretcher\n",
            "Event Type | Arre\n",
            "gent protesters\n",
            "Negative — [agent |\n",
            "(events)\n",
            "exe) ]\n",
            "; =i Te)\n",
            "Negative\n",
            "Labels Role\n",
            "(arguments) Switching\n",
            "\n",
            " \n",
            "\n",
            "84 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: This image shows a slide from a presentation discussing 'Hard negatives via manipulating event structures'. It includes a photograph of a protest scene with labeled individuals and objects, and text boxes explaining the concept of positive and negative labels in event structures.\n",
            "Caption for /content/drive/MyDrive/pages/page_259.png-- OCR: Hard negatives via manipulating event structures\n",
            "\n",
            "AN\n",
            "\n",
            "     \n",
            "\n",
            "    \n",
            "     \n",
            "\n",
            "Event Type | Transport (carry) Protester:\n",
            "eS transported\n",
            "Positive [Agent protesters | 3 injureasman\n",
            "Labels Busing\n",
            "[Instrument |stretcher —\n",
            "stretcher\n",
            "Event Type Protesters\n",
            "Negative am arrested\n",
            "° injured man\n",
            "Labels 3 ~ isinga\n",
            "(events) ~ stretcher.\n",
            "ese)\n",
            ". =i Te)\n",
            "Negative ~) (injured man\n",
            "Labels 6 _ transported a\n",
            "(arguments) 3 ~stretcher with\n",
            "S ~ protesters.\n",
            "\n",
            "  \n",
            "\n",
            "85 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: This image shows a slide from a presentation discussing 'Hard negatives via manipulating event structures'. It includes a photograph of a protest scene with labeled individuals and objects, and text boxes explaining the manipulation of event structures to create hard negatives.\n",
            "Caption for /content/drive/MyDrive/pages/page_260.png-- OCR: AN\n",
            "\n",
            "86\n",
            "\n",
            "Hard negatives via manipulating event structures\n",
            "\n",
            "wv\n",
            "\n",
            "eed)\n",
            "\n",
            "Bench\n",
            "\n",
            " \n",
            "\n",
            "4\n",
            "\n",
            "  \n",
            "\n",
            "Positive\n",
            "Labels\n",
            "\n",
            "Negative\n",
            "Labels\n",
            "(events)\n",
            "\n",
            "Negative\n",
            "Labels\n",
            "(arguments)\n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "Protester:\n",
            "transported\n",
            "injured man\n",
            "using a\n",
            "stretcher.\n",
            "\n",
            "Protesters\n",
            "arrested\n",
            "injured man\n",
            "using a\n",
            "stretcher.\n",
            "\n",
            "Injured man\n",
            "transported a\n",
            "stretcher with\n",
            "protesters.\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image shows a protest scene with multiple people, some of whom are labeled as 'Person' and one labeled as 'Bench'. There are also labels indicating 'Protesters' and 'injured man using a stretcher'.\n",
            "Caption for /content/drive/MyDrive/pages/page_261.png-- OCR: AN\n",
            "\n",
            "87\n",
            "\n",
            "   \n",
            "\n",
            "  \n",
            " \n",
            " \n",
            "  \n",
            "  \n",
            "\n",
            "an\n",
            "\n",
            "Person Person ; Person\n",
            "\n",
            "wv\n",
            "\n",
            "ete)\n",
            "\n",
            "Bench\n",
            "\n",
            "4\n",
            "\n",
            "Contrastive Learning on Event Semantics\n",
            "\n",
            "Positive\n",
            "Labels\n",
            "\n",
            "Negative\n",
            "Labels\n",
            "(events)\n",
            "\n",
            "Negative\n",
            "Labels\n",
            "\n",
            "Negative\n",
            "\n",
            "    \n",
            "\n",
            "transported\n",
            "injured man\n",
            "\n",
            "using a\n",
            "\n",
            "stretcher. —-, — ty\n",
            "\n",
            "Protesters m\n",
            "arrested 3 a\n",
            "\n",
            "injured man—» 6 X% — f¢\n",
            "using a 2\n",
            "\n",
            "stretcher.\n",
            "\n",
            "Injured man —> me fp\n",
            "transported a\n",
            "stretcher with\n",
            "\n",
            "(arguments) protesters.\n",
            "\n",
            "Jepooug\n",
            "ebew]\n",
            "\n",
            "Anchor LEARNING\n",
            "Negative\n",
            "ich\n",
            "\n",
            "or a\n",
            "Positive Positive\n",
            "\n",
            " \n",
            "\n",
            "Contrastive\n",
            "s(tov) Learning\n",
            "\n",
            "—> 5(ti,Vv)\n",
            "\n",
            "S(t2,v)\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: This image is a slide from a presentation on Contrastive Learning on Event Semantics.\n",
            "Caption for /content/drive/MyDrive/pages/page_262.png-- OCR: AN\n",
            "\n",
            "88\n",
            "\n",
            "  \n",
            "\n",
            "  \n",
            "\n",
            "MLLM Pretraining with Event Semantic Structures\n",
            "\n",
            " \n",
            "\n",
            "Challenge: Structured Encoding\n",
            "\n",
            " \n",
            "\n",
            "Transport\n",
            "Person\n",
            "\n",
            "   \n",
            "  \n",
            " \n",
            "\n",
            "PER\n",
            "\n",
            "PER\n",
            "police\n",
            "FAC\n",
            "\n",
            "Antigovernment protesters carry an\n",
            "injured man on a stretcher after\n",
            "clashes with riot police on\n",
            "Independence Square in Kyiv on\n",
            "February 20, 2014.\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.\n",
            "\n",
            "Image & Caption; Caption: This image is a slide from a presentation on MLLM Pretraining with Event Semantic Structures. It features a diagram on the left side, which is a structured encoding of an event, and a photograph on the right side, which is an example of the event being described. The diagram includes various elements such as 'Transport Person', 'agent', 'place', and 'event', and shows the relationships between them. The photograph depicts a scene from an event, with annotations that correspond to the elements in the diagram. The slide also includes a caption that provides context for the image, mentioning antigovernment protesters, a stretcher, and a clash with riot police.\n",
            "Caption for /content/drive/MyDrive/pages/page_263.png-- OCR: A\n",
            "\n",
            "89\n",
            "\n",
            "MLLM Pretraining with Event Semantic Structures\n",
            "\n",
            " \n",
            "  \n",
            "   \n",
            " \n",
            "\n",
            "Transport | “\n",
            "Person ,~\n",
            "¢\n",
            "\n",
            "   \n",
            "\n",
            "PER\n",
            "\n",
            "Independence Square buildifig\n",
            "\n",
            "Event Level Alignment\n",
            "\n",
            "Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: The image displays a slide from a presentation titled 'MLLM Pretraining with Event Semantic Structures'. It features a complex diagram with various elements such as 'Transport Person', 'PER', 'agent', 'instrument', 'attack', 'clashes', 'place', 'person', 'protesters', 'COM stretcher', 'PER police', 'FAC', 'Independence Square', 'building', and 'person'. These elements are connected by lines and arrows, indicating relationships and actions between them. The diagram is labeled 'Event Level Alignment' and includes a source citation at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_264.png-- OCR: YJ MLLM Pretraining with Event Semantic Structures\n",
            "mm\n",
            "\n",
            "P P P Transport ,-~\n",
            "Structured Alignment via Optimal Transport [Raza\n",
            "\n",
            "gel me.\n",
            "protesters’ | ~~.\n",
            "PER sA . -\n",
            "\n",
            "   \n",
            "     \n",
            "    \n",
            "    \n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            "Text Event Graph Image Event Graph\n",
            "\n",
            "Independence Square buildifig\n",
            "\n",
            "Event Level Alignment\n",
            "\n",
            "90 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: This image is a slide from a presentation on MLLM Pretraining with Event Semantic Structures. It features a diagram that illustrates the concept of Structured Alignment via Optimal Transport between a Text Event Graph and an Image Event Graph. The diagram includes various elements such as 'Transport Person', 'Instrument', 'Protesters', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person\n",
            "Caption for /content/drive/MyDrive/pages/page_265.png-- OCR: YJ MLLM Pretraining with Event Semantic Structures\n",
            "mm\n",
            "\n",
            ". . . Tr rt 7%\n",
            "Structured Alignment via Optimal Transport Person. *\n",
            "\n",
            "gel me.\n",
            "protesters’ | ~~.\n",
            "PER sA . -\n",
            "\n",
            "   \n",
            "     \n",
            "    \n",
            "    \n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            "Text Event Graph Image Event Graph\n",
            "\n",
            "1 Define cost matrix C (embedding similarity)\n",
            "eee eee\n",
            "\n",
            "Independence Square buildifig\n",
            "\n",
            "Event Level Alignment\n",
            "\n",
            "91 Source: “Knowledge-Driven Vision-Language Encoding,” Tutorial, CVPR, 2023.; Caption: Slide 91: MLLM Pretraining with Event Semantic Structures\n",
            "Caption for /content/drive/MyDrive/pages/page_266.png-- OCR: ‘YJ MLLM Pretraining with Event Semantic Structures\n",
            "= é\n",
            "\n",
            ". . . Tr t 07\n",
            "Structured Alignment via Optimal Transport Person. *\n",
            "\n",
            "gel me.\n",
            "protesters’ | ~~.\n",
            "PER sA . -\n",
            "\n",
            "   \n",
            "     \n",
            "    \n",
            "    \n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            "Text Event Graph Image Event Graph\n",
            "\n",
            "1 Define cost matrix C (embedding similarity)\n",
            "a,\n",
            "\n",
            "2 \\ Optimization Goal: minimize transport distance | The optimal 7 is approximated by a differentiable\n",
            "\\ D(S,T) = minT*C Sinkhorn Knopp algorithm (Sinkhorn, 1964; Cuturi, 2013)\n",
            "\n",
            "T = diag(p) exp(—C/7) diag(q)\n",
            "\n",
            "for 1 = 0,1, 2,... until convergence,\n",
            "ptt =10(Kq'),\n",
            "\n",
            "gt! =10(K pit),\n",
            "\n",
            "92 4 T* := diag(p*) Kdiag(q\")\n",
            "\n",
            "Independence Square buildifig\n",
            "\n",
            "Event Level Alignment; Caption: Slide 92 titled 'MLLM Pretraining with Event Semantic Structures'\n",
            "Caption for /content/drive/MyDrive/pages/page_267.png-- OCR: ‘YJ MLLM Pretraining with Event Semantic Structures\n",
            "= é\n",
            "\n",
            ". . . Tr t 07\n",
            "Structured Alignment via Optimal Transport Person. *\n",
            "\n",
            "gel me.\n",
            "protesters’ | ~~.\n",
            "PER sA . -\n",
            "\n",
            "   \n",
            "    \n",
            "    \n",
            "    \n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            "Text Event Graph Image Event Graph\n",
            "\n",
            "1 Define cost matrix C (embedding similarity)\n",
            "a,\n",
            "\n",
            "Independence Square buildifig\n",
            "\n",
            "Event Level Alignment\n",
            "\n",
            " \n",
            "\n",
            "2 \\ Optimization Goal: minimize transport distance | The optimal 7 is approximated by a differentiable\n",
            "\\ D(S,T) = minT*C Sinkhorn Knopp algorithm (Sinkhorn, 1964; Cuturi, 2013)\n",
            "T = diag(p) exp(—C/7) diag(q)\n",
            "Optimize the transport plan T within k for 1 = 0,1, 2,... until convergence,\n",
            "\\ _4 iterations pit} =10 (Kq‘),\n",
            "\n",
            " \n",
            "\n",
            "gt =1 Q (K'p'*),\n",
            "93 4 TR := diag(p\") Kdiag(q*); Caption: Page 93 of a document.\n",
            "Caption for /content/drive/MyDrive/pages/page_268.png-- OCR: YJ Visual Event Extraction C\n",
            "ys\n",
            "\n",
            "Supporting Zero-shot Vision Event Extraction\n",
            "\n",
            " \n",
            "\n",
            "Vaccination\n",
            "\n",
            "agent target\n",
            "agent\n",
            "\n",
            "   \n",
            " \n",
            "\n",
            "3 4 i; Caption: This image is a presentation slide titled 'Visual Event Extraction' with a subtitle 'Supporting Zero-shot Vision Event Extraction'. It features four photographs, each with a label indicating the type of event depicted: Vaccination, Arrest, Protest, and Celebration. Below each photo, there is a diagram illustrating the roles of the individuals in the event, such as 'agent', 'target', 'detainee', 'demonstrator', and 'attendee'. The slide is designed to show how AI can identify and categorize different types of events from images without prior specific training.\n",
            "Caption for /content/drive/MyDrive/pages/page_269.png-- OCR: ‘YJ Visual Event Extraction\n",
            "ys\n",
            "\n",
            " \n",
            "\n",
            "Supporting Zero-shot Vision Event Extraction\n",
            "\n",
            " \n",
            "\n",
            "Arrest\n",
            "\n",
            "   \n",
            "\n",
            "agent detainee\n",
            "agent | agent; Caption: This image is a presentation slide titled 'Visual Event Extraction' with a subtitle 'Supporting Zero-shot Vision Event Extraction'. It features four photographs, each labeled with a different event type: Vaccination, Arrest, Protest, and Celebration. Each photo has a diagram below it showing the relationships between agents and attendees. The slide is designed to illustrate the concept of extracting visual events from images without prior specific training.\n",
            "Caption for /content/drive/MyDrive/pages/page_270.png-- OCR: Supporting Zero-shot Vision Event Extraction\n",
            "\n",
            "   \n",
            " \n",
            "\n",
            "Celebration\n",
            "\n",
            "rir (Aten; Caption: This image is a presentation slide about Visual Event Extraction, specifically focusing on supporting zero-shot vision event extraction. It includes examples of different events such as vaccination, arrest, protest, and celebration, with corresponding images and annotations.\n",
            "Caption for /content/drive/MyDrive/pages/page_271.png-- OCR: 97\n",
            "\n",
            "¢ Structured: Capturing semantic structure\n",
            "¢ Abstract: Understanding abstract and complicated concepts\n",
            "\n",
            "puree]\n",
            "lignment; Caption: The image displays a slide with the title 'Challenges' at the top, followed by two bullet points. The first bullet point is 'Structured: Capturing semantic structure' and the second is 'Abstract: Understanding abstract and complicated concepts'. Below the bullet points, there is a graphic of two arrows pointing towards each other, with the left arrow labeled 'Structural Alignment' and the right arrow labeled 'Abstract Semantics'. The slide number '97' is visible at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_272.png-- OCR: Action: “Sitting on a sofa” time\n",
            "\n",
            "Vision\n",
            "Better in\n",
            "capturing\n",
            "details with\n",
            "visual\n",
            "features\n",
            "\n",
            " \n",
            "\n",
            "eneath\n",
            "\n",
            "XN person = —-—-—-= ere =—<—<-- beet\n",
            "N in ge of sitting o =\n",
            "\"| to (= —\n",
            "sofa = = sofa = s\n",
            "Spatio-temporal scene graphs; Caption: The image is a slide from a presentation discussing challenges in AI, specifically structure-aware encoding. It features a diagram on the left side with two main components: 'Text' and 'Vision'. The 'Text' component is described as having a strong ability in reasoning and semantic structure understanding, while the 'Vision' component is noted for its better ability in capturing details with visual features. On the right side, there is a sequence of images depicting a person sitting on a sofa, with annotations indicating the person's position relative to the sofa at different times. The annotations include terms like 'person', 'next to', 'in front of', 'beneath', and 'sitting on', which are connected to the sofa in a circular flow, suggesting a temporal sequence. Below the images, there is a label 'Spatio-temporal scene graphs' indicating the type of data representation used.\n",
            "Caption for /content/drive/MyDrive/pages/page_273.png-- OCR: Abstract\n",
            "\n",
            " \n",
            "\n",
            "99\n",
            "\n",
            "Love\n",
            "\n",
            "Happiness\n",
            "\n",
            "Emotion > Music; Caption: The image shows a slide from a presentation with the title 'Challenges: Abstract Semantics'. It is divided into three sections, each with a different image and label. The first section on the left shows a silhouette of two hands forming a heart shape against a sunset, labeled 'Love'. The middle section shows a couple dancing in a forest, labeled 'Happiness'. The third section on the right shows a child wearing headphones, labeled 'Emotion -> Music'. The slide number '99' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_274.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Reasoning Basics\n",
            "m Chain-of-Thought (CoT)\n",
            "= Zero-shot reasoner\n",
            "= Few-shot reasoner\n",
            "\n",
            "m Analogical Reasoning\n",
            "= Multimodal Reasoning\n",
            "= Multimodal CoT\n",
            "= Commonsense Knowledge\n",
            "= Reasoning Benchmarks\n",
            "\n",
            "100; Caption: The image displays a slide from a presentation with the title 'This Lecture – Agenda' at the top. Below the title, there is a list of topics that will be covered in the lecture. The topics are organized into four main categories: 'Reasoning Basics,' 'Analogical Reasoning,' 'Multimodal Reasoning,' and 'Reasoning Benchmarks.' Each category has sub-points. For example, under 'Reasoning Basics,' there are two sub-points: 'Chain-of-Thought (CoT)' and 'Zero-shot reasoner,' with 'Few-shot reasoner' as a further sub-point under 'Chain-of-Thought.' The slide number '100' is visible at the bottom, indicating its position within the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_275.png-- OCR: 101\n",
            "\n",
            "MMStar [NeurlPS 2024]\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    \n",
            "\n",
            ">\n",
            "\n",
            "ScienceQA™*: question-1009\n",
            "Answer: C\n",
            "\n",
            "®\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "   \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "\n",
            "the capital of Nebraska?\n",
            "\n",
            ":Providence\n",
            "\n",
            "B: Saint Paul\n",
            "\n",
            "C: Lincoin\n",
            "\n",
            "D: Kansas City\n",
            "\n",
            "‘ap The image does nothing,\n",
            "\n",
            "it‘s the same as asking\n",
            "\n",
            "LLM me with a text question\n",
            "directly.\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ry\n",
            "\n",
            "“3°\n",
            "\n",
            " \n",
            "\n",
            "Soft Mot:\n",
            "Experts Choice\n",
            "Dense\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "vo! 7 wo 1\n",
            "\n",
            "ie\n",
            "‘Toul Taning PU 3ays Tout Tang oD\n",
            "(b) ImageNet 10-shot Accuracy\n",
            "\n",
            "MathVista: question-565 Answer: A\n",
            "#Correct LLMs : 16/22 (72.7%)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Which model can achieve the best\n",
            "ImageNet 10-shot Accuracy score?\n",
            "\n",
            " \n",
            "  \n",
            "\n",
            "xperts Choice\n",
            "C: Tokens Choice\n",
            "D: Dense\n",
            "\n",
            "      \n",
            " \n",
            " \n",
            " \n",
            "     \n",
            "      \n",
            " \n",
            "   \n",
            "\n",
            "MMMU*“!: question-2407 Answer: E\n",
            "#LLM-LVLM™* Pairs : 9/16 (56.3%)\n",
            "\n",
            " \n",
            "\n",
            "is the shape of the round dirt\n",
            "\n",
            "quare\n",
            "ngle\n",
            "\n",
            "D: Diamond\n",
            "\n",
            "Which cell type is pictured?\n",
            "A: Eosinophil\n",
            "B: Thrombocyte\n",
            "\n",
            "uM @& © = LWiM™\n",
            "° @; Caption: The image displays a slide from a presentation titled 'Various reasoning benchmarks are emerging' with a focus on 'MMStar [NeurIPS 2024]'. It includes multiple examples of questions and answers related to reasoning benchmarks, such as geography, shape recognition, and cell type identification. There are also graphs showing the accuracy of different models in answering questions, and a section discussing the performance of LLMs in text-based reasoning tasks. The slide is visually divided into sections with different background colors and contains both text and graphical elements.\n",
            "Caption for /content/drive/MyDrive/pages/page_276.png-- OCR: - MMBench [ECCV 2024]\n",
            "\n",
            "Image Style\n",
            "\n",
            "Image Topic\n",
            "\n",
            "   \n",
            "\n",
            "10z\n",
            "\n",
            " \n",
            "\n",
            "Q: Which category does this\n",
            "image belong to?\n",
            "\n",
            "A. Oil Paiting\n",
            "\n",
            "B. Sketch\n",
            "\n",
            "C. Digital art\n",
            "\n",
            "D. Photo\n",
            "\n",
            "GT: A\n",
            "\n",
            "Q: Which of the following\n",
            "\n",
            "captions best describes this\n",
            "\n",
            "image?\n",
            "\n",
            "A. A group of people playing\n",
            "soccer in a field\n",
            "\n",
            "B. A woman walking her dog on\n",
            "a beach\n",
            "\n",
            "Cc. Aman riding a bicycle on a\n",
            "mountain trail\n",
            "\n",
            "D. A child playing with a ball in\n",
            "\n",
            "rk\n",
            "\n",
            "Coarse Perception\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Q: Which category does this\n",
            "image belong to?\n",
            "\n",
            "A. Oil Paiting\n",
            "\n",
            "B. Sketch\n",
            "\n",
            "C. Digital art\n",
            "\n",
            "D. Photo\n",
            "\n",
            "GT: B\n",
            "\n",
            "Q: Which of the following\n",
            "\n",
            "captions best describes this\n",
            "\n",
            "image?\n",
            "\n",
            "A. A group of people playing\n",
            "soccer in a field\n",
            "\n",
            "B. A woman walking her dog on\n",
            "a beach\n",
            "\n",
            "Cc. Aman riding a bicycle ona\n",
            "mountain trail\n",
            "\n",
            "D. A child playing with a ball in\n",
            "a park\n",
            "\n",
            "GT: B; Caption: The image displays a slide from a presentation, featuring the title 'Various reasoning benchmarks are emerging' and a subtitle 'MMBench [ECCV 2024]'. It includes a section titled 'Coarse Perception' with three sets of questions and answers related to image style, image topic, and image captions. Each set has two options, A and B, with the correct answer indicated as 'GT: A'. The images shown are a famous painting, a grayscale image of a cube, a soccer game, and a person playing with a ball on a beach.\n",
            "Caption for /content/drive/MyDrive/pages/page_277.png-- OCR: - MMBench [ECCV 2024]\n",
            "\n",
            "Object Localization\n",
            "\n",
            " \n",
            "\n",
            "OCR\n",
            "\n",
            " \n",
            "\n",
            "103\n",
            "\n",
            "(single-instance)\n",
            "\n",
            "Q: How many apples are there in\n",
            "the image? And how many\n",
            "bananas are there?\n",
            "\n",
            "A. 4 apples and 2 bananas\n",
            "\n",
            "B. 3 apples and 3 banana\n",
            "\n",
            "C. 2 apples and 4 bananas\n",
            "\n",
            "D. 4 apples and 1 bananas\n",
            "\n",
            "GT: A\n",
            "\n",
            "Q: What does this outdoor\n",
            "billboard mean?\n",
            "\n",
            "A. Smoking is prohibited here.\n",
            "B. Something Is on sale.\n",
            "\n",
            "C. No photography allowed\n",
            "D. Take care of your speed.\n",
            "GT: B\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Fine-grained Perception\n",
            "\n",
            "Q: Which corner is the juice?\n",
            "A. Up\n",
            "\n",
            "B. Down\n",
            "\n",
            "Cc. Left\n",
            "\n",
            "D. Right\n",
            "\n",
            "GT: D\n",
            "\n",
            "Q: What does this picture want\n",
            "\n",
            "to express?\n",
            "\n",
            "A. We are expected to cave for\n",
            "green plants.\n",
            "\n",
            "B. We ave expected to cave for\n",
            "the earth.\n",
            "\n",
            "C. We ave expected to stay\n",
            "positive.\n",
            "\n",
            "D. We are expected to work\n",
            "hard.\n",
            "\n",
            "GT: D; Caption: The image displays a slide from a presentation, which includes a title 'Various reasoning benchmarks are emerging' and a subtitle 'MMBench [ECCV 2024]'. Below the title, there are three sections, each with a question and multiple-choice answers related to object localization, OCR (Optical Character Recognition), and fine-grained perception. The first section shows a picture of fruits with a question about the number of apples and bananas. The second section shows a billboard with a question about the meaning of the outdoor sign. The third section shows a picture of a hand with a question about the message the picture wants to express. Each section has a correct answer indicated in green. The slide number '103' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_278.png-- OCR: ¢ MMBench [ECCV 2024] Fine-grained Perception\n",
            "\n",
            "(cross-instance)\n",
            "\n",
            "Spatial Relationship\n",
            "\n",
            "Q: Which country is north of the\n",
            "country circled in blue?\n",
            "\n",
            "A. Laos\n",
            "\n",
            "B. Thailand\n",
            "\n",
            "C. China\n",
            "\n",
            "D. indonesia\n",
            "\n",
            "GT: C\n",
            "\n",
            " \n",
            "\n",
            "Attribute Comparison\n",
            "\n",
            "Q: Are the two arrows in the same\n",
            "direction in the picture?\n",
            "\n",
            "A. Same\n",
            "\n",
            "B. Not the same\n",
            "\n",
            "C. Can't judge\n",
            "\n",
            "GT: B\n",
            "\n",
            "104\n",
            "\n",
            " \n",
            "\n",
            "Which country is the southernmost\n",
            "of all the countries shown in the\n",
            "picture?\n",
            "\n",
            "A. Australia\n",
            "\n",
            "Q: Are the candies in the two jars\n",
            "in the picture the same color?\n",
            "\n",
            "A. Same\n",
            "\n",
            "B. Not the same\n",
            "\n",
            "C. Can't judge\n",
            "\n",
            "GT: B; Caption: The image is a slide from a presentation discussing various reasoning benchmarks, specifically focusing on the MMBench benchmark from ECCV 2024. It includes examples of fine-grained perception tasks, such as identifying a country on a map, determining the southernmost country among a group, comparing the direction of arrows, and assessing the color of candies in jars. Each task is accompanied by multiple-choice questions and the correct answers are indicated.\n",
            "Caption for /content/drive/MyDrive/pages/page_279.png-- OCR: Various reasoning benchmarks are emerging\n",
            "\n",
            "  \n",
            "\n",
            "¢ MMBench [ECCV 2024] Attribute Reasoning\n",
            "\n",
            "Physical Property Reasoning\n",
            "\n",
            "pak nyse eorgen thot hs et\n",
            "\n",
            "Is the hardest naturally occurring Q: The object shown in this figure:\n",
            "\n",
            " \n",
            "\n",
            "105\n",
            "\n",
            "c\n",
            "\n",
            "substance on Earth.\n",
            "\n",
            "Conducts electricity well at room\n",
            "temperature.\n",
            "\n",
            "Is typically found in igneous rocks\n",
            "like basalt and granite.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Is one kind of metal that is liquid\n",
            "at the room temperature.\n",
            "\n",
            "Can be easily dissolved in water.\n",
            "Has a low boiling point compared\n",
            "to other metals.\n",
            "\n",
            "D. Has a low melting point compared Bi attracted to magnets.\n",
            "to other minerals.\n",
            "\n",
            "GT: A\n",
            "Q: What's the function of the What's the function of the\n",
            "demonstrated object? a object?\n",
            "A. Cut vegetables A. Separating\n",
            ". stir an B. Clamping\n",
            "\n",
            ". Water purification c. drill\n",
            "D. Boiling water se tae\n",
            "a D. incise\n",
            "\n",
            "GT: A; Caption: The image displays a slide from a presentation, featuring the title 'Various reasoning benchmarks are emerging' and the subtitle 'MMBench [ECCV 2024]'. It is divided into two main sections: 'Attribute Reasoning' and 'Function Reasoning'. Under 'Attribute Reasoning', there are two questions each with four answer options related to physical properties and functions of objects. The first question is about a diamond, and the second is about a metal sphere. The 'Function Reasoning' section has one question about the function of a mixer. Each question has a 'GT' (graphical answer) indicated. The slide is numbered 105 at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_280.png-- OCR: ¢ MMBench [ECCV 2024] Relation Reasoning\n",
            "\n",
            "Social_Relation\n",
            "\n",
            "Q: What can be the relationship\n",
            "between the two persons in this image?\n",
            "A. Father and daughter\n",
            "\n",
            "B. Mother and son\n",
            "\n",
            "C. Brother and sister\n",
            "\n",
            "D. Husband and wife\n",
            "\n",
            "GT: D\n",
            "\n",
            "Q: What can be the relationship\n",
            "between the two persons in this image?\n",
            "A. Father and daughter\n",
            "\n",
            "B. Grandfather and granddaughter\n",
            "\n",
            "C. Brother and sister\n",
            "\n",
            "D. Husband and wife\n",
            "\n",
            "GT: B\n",
            "\n",
            "  \n",
            "\n",
            "Nature Relation\n",
            "\n",
            "Q: In nature, what's the relationship\n",
            "between these two creatures?\n",
            "\n",
            "A. Predatory relationshi|\n",
            "\n",
            "B. Competitive relationships\n",
            "\n",
            "C. Parasitic relationships\n",
            "\n",
            "D. Symbiotic relationship\n",
            "\n",
            "GT: B\n",
            "\n",
            "Q: In nature, what's the relationship\n",
            "between these two creatures?\n",
            "\n",
            "A. Predatory relationships\n",
            "\n",
            "B. Competitive relationships\n",
            "\n",
            "C. Parasitic relationships\n",
            "\n",
            "D. Symbiotic relationship\n",
            "\n",
            "GT: D\n",
            "\n",
            "  \n",
            "\n",
            "106; Caption: The image displays a slide from a presentation, which is titled 'Various reasoning benchmarks are emerging' and is related to 'MMBench [ECCV 2024]'. The slide is divided into two sections, one for 'Social Relation' and the other for 'Nature Relation'. Each section contains a question about the relationship between two individuals or creatures in the image, followed by four multiple-choice answers and the correct answer indicated. The 'Social Relation' section has an image of two people sitting at a table, while the 'Nature Relation' section has an image of a lion and a cheetah. The slide is numbered 106.\n",
            "Caption for /content/drive/MyDrive/pages/page_281.png-- OCR: Various reasoning benchmarks are emerging\n",
            "\n",
            " \n",
            "\n",
            "- MMBench [ECCV 2024]\n",
            "\n",
            "Future Prediction\n",
            "\n",
            "Q: What will happen next?\n",
            "\n",
            "A. this person is gonna cry\n",
            "\n",
            "B. this person is gonna laugh\n",
            "\n",
            "C. this person is gonna get mad\n",
            "D. both A,B, and C\n",
            "\n",
            "GT:A\n",
            "\n",
            " \n",
            "\n",
            "Structuralized Image-text Understanding\n",
            "\n",
            "Kid's Favourte Fruts Q: According to this it , which\n",
            "“ fruit did the most kids like?\n",
            "\n",
            "& A. Orange\n",
            "‘ B. Banana\n",
            "; 1 C. Pear\n",
            "D. Apple\n",
            "oe M de GT: A\n",
            "\n",
            "107\n",
            "\n",
            " \n",
            "\n",
            "Logic Reasoning\n",
            "\n",
            "Q: What will happen next?\n",
            "\n",
            "A. the motorcyle is gonna go forward\n",
            "\n",
            "B. the motorcyle is gonna crash\n",
            "\n",
            "C. the motorcyle is gonna go\n",
            "backward\n",
            "\n",
            "D. both A,B, and C\n",
            "\n",
            "GT: B\n",
            "\n",
            " \n",
            "\n",
            "ee resi A Q: According to oe en\n",
            "hobby is liked the least?\n",
            "\n",
            "A. Reading\n",
            "\n",
            "B. Singing\n",
            "\n",
            "C. Painting\n",
            "\n",
            "D. Dancing\n",
            "\n",
            "GT: C; Caption: The image displays a slide from a presentation titled 'Various reasoning benchmarks are emerging' with a focus on 'MMBench [ECCV 2024] Logic Reasoning'. It includes three examples of reasoning benchmarks: 'Future Prediction', 'Structuralized Image-text Understanding', and a pie chart representing 'Sarah asked her friends to vote on their favorite hobby'. Each example has a question and multiple-choice answers, with the correct answer indicated. The slide is numbered 107.\n",
            "Caption for /content/drive/MyDrive/pages/page_282.png-- OCR: - MMVet [ICML 2024]\n",
            "\n",
            "Me: I'll do it at 8\n",
            "Time: 8.05\n",
            "Me: looks like | gotta wait till 9 now\n",
            "\n",
            "5\n",
            "\n",
            "      \n",
            "\n",
            "Q: What will the girl on the right write Q: Where is this photo taken? Q: Can you explain this meme?\n",
            "on the board? GT: Qatar GT: This meme is a humorous take on\n",
            "GT: 14\n",
            "\n",
            "procrastination and the tendency to delay\n",
            "Required capabilities: Required capabilities: tasks until a specific time ...\n",
            "R itio Recognitio Required capabilities:\n",
            "\n",
            "lal awareness lowledg\n",
            "\n",
            " \n",
            "\n",
            "108; Caption: The image shows a slide from a presentation with the title 'Various reasoning benchmarks are emerging' and a subtitle 'MMVet [ICML 2024]'. There are three separate sections on the slide, each with a question and answer format. The first section has a question about what a girl on a chalkboard will write, with the answer being '14' and the required capabilities listed as Recognition, Spatial awareness, OCR, and Math. The second section asks where a photo is taken, with the answer being 'Qatar' and the required capabilities listed as Recognition and Knowledge. The third section contains a humorous meme with a caption about procrastination and a delay in starting a task, with the required capabilities listed as OCR, Recognition, Knowledge, and Language generation. The slide number '108' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_283.png-- OCR: AN\n",
            "\n",
            "109\n",
            "\n",
            "Various reasoning benchmarks are emerging\n",
            "\n",
            "MathVista [ICLR 2024]\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Question: Find the missing value\n",
            "in this math puzzle.\n",
            "\n",
            "Solution:\n",
            "(5-48 =1\n",
            "(7-3 = 64\n",
            "(8-2 =216\n",
            "\n",
            "Similarly, (11 - 8)3 = 27.\n",
            "\n",
            "So the missing value is 27.\n",
            "Answer: 27\n",
            "\n",
            "Category: Math-targeted\n",
            "\n",
            "Task: Figure question answering\n",
            "Context: Puzzle test\n",
            "\n",
            "10\n",
            "\n",
            " \n",
            "\n",
            "fix)\n",
            "\n",
            "00\n",
            "\n",
            "0s\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ANS ra\n",
            "\n",
            "Question: Which function is monotonic\n",
            "in range [0, pi]?\n",
            "\n",
            "Choices:\n",
            "\n",
            "(A) the red one (B) the blue one\n",
            "\n",
            "(C) both (D) none of them\n",
            "\n",
            "Answer: (B) the blue one\n",
            "\n",
            "Category: Math-targeted\n",
            "\n",
            "Task: Textbook question answering\n",
            "Context: Function plot\n",
            "\n",
            "4\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "\n",
            "gpt-3.5-turbo\n",
            "text-davinci-003\n",
            "claude-instant.\n",
            "text-davinci-002\n",
            "text-bison-001\n",
            "chatglm2\n",
            "openchat-13b\n",
            "\n",
            "API-based\n",
            "LLMs\n",
            "\n",
            "wizardIm-13b Open-sourced\n",
            "\n",
            "- es LLMs\n",
            "wizardcoder-15b\n",
            "\n",
            "dolly-v2-12b\n",
            "\n",
            " \n",
            "\n",
            "AgentBench Overall Score\n",
            "\n",
            "Question: What is the performance gap in\n",
            "the AgentBench Overall Score between\n",
            "the worst API-based LLM and the best\n",
            "open-sourced LLM?\n",
            "\n",
            "Answer: 0.16\n",
            "\n",
            "Category: Math-targeted\n",
            "\n",
            "Task: Figure question answering\n",
            "Context: Scientific figure\n",
            "\n",
            " \n",
            "\n",
            "Grade: Elementary school Grade: College Grade: College\n",
            "Math: Logical reasoning Math: Algebraic reasoning Math: Scientific reasoning\n",
            "(a) IQTest (b) FunctionQA (c) PaperQA; Caption: The image displays a slide from a presentation titled 'Various reasoning benchmarks are emerging' with a focus on MathVista [ICLR 2024]. It includes three sections: IQTest, FunctionQA, and PaperQA, each with a question and answer related to mathematical reasoning. The IQTest section shows a math puzzle with a missing value to be found, the FunctionQA section presents a question about a monotonic function in a range, and the PaperQA section discusses the performance gap in the AgentBench Overall Score between API-based LLMs and open-sourced LLMs. Additionally, there is a bar graph comparing the scores of various LLMs, with API-based LLMs generally scoring higher than open-sourced LLMs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_284.png-- OCR: AN\n",
            "\n",
            "110\n",
            "\n",
            "Various reasoning benchmarks are emerging\n",
            "\n",
            "HallusionBench [CVPR 2024]\n",
            "\n",
            "Visual Dependent\n",
            "\n",
            "Illusion o\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "     \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "e\n",
            "Question: Question:\n",
            "Is the right orange circle the same size as the According to parallel lines theorem, is angle 1 +\n",
            "left orange circle? angle 2 > 180?\n",
            "Is the right orange cirele larger than the left According to parallel lines theorem, is angle 1 +\n",
            "orange circle? angle 2 = 180?\n",
            "Is the right orange circle smaller than the left According to parallel lines theorem, is angle | +\n",
            "orange circle? angle 2 < 180?\n",
            "Figure / Other\n",
            "2 Re\n",
            "\n",
            "           \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Question:\n",
            "(Onernont Are all the characters in this figure from the\n",
            "Does the image show \"Beijing Roast Duck\"? ‘manga series One Piece?\n",
            "Does the image show \"Guangxi Roast Are there any characters in this figure from the\n",
            "Duck\"? manga series Detective Conan?\n",
            "\n",
            "Video\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "‘Question:\n",
            "According to the positive sequence images, does Homer Simpson disappear into the bushes?\n",
            "\n",
            "According to the positive sequence images, does Homer Simpson come out of the bushes?\n",
            "\n",
            "Homer Simpson disappears into the bushes. According to the positive sequence, are they in the correct order?\n",
            "Homer Simpson comes out of the bushes. According to the positive sequence, are they in the correct order?\n",
            "\n",
            " \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Visual Supplement\n",
            "\n",
            "Table\n",
            "\n",
            "Chart\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Question: Question:\n",
            "Does China have the most gold medals in In 2017, was Tencent the company with the\n",
            "2008 beijing olympic? highest revenue from video games, with Sony as\n",
            "Does USA have the most gold medals in Pete eee\n",
            "2008 beijing olympic? ,\n",
            "IDSES Ricsa ee Ae meet ld neal In 2017, did Apple generate higher revenue\n",
            "2008 beijing olympic? ‘from video games compared to Google?\n",
            "\n",
            "OCR\n",
            "\n",
            "No Visual\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "G*6.67428 * 10°\" mi’ kg™*\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "No Visual\n",
            "c\n",
            "Question:\n",
            "- According to the image, does the value of\n",
            "Question: Gravity constant 'G' range from 6.66 *\n",
            "\n",
            "Based on the map, did the Democratic Party\n",
            "win Texas in the 2020 elections?\n",
            "Based on the map, did the Republican Party\n",
            "win Texas in the 2020 elections?\n",
            "\n",
            "10-11 10 6.68 * 1011?\n",
            "According to the image, does the value of\n",
            "Gravity constant 'G' range from 6.68 *\n",
            "10*-11 to 6.70 * 10-11?; Caption: The image displays a slide from a presentation titled 'Various reasoning benchmarks are emerging' with a focus on 'HallusionBench [CVPR 2024]'. The slide is divided into two main sections: 'Visual Dependent' and 'Visual Supplement'. The 'Visual Dependent' section includes examples of reasoning questions related to 'Illusion', 'Math', 'Poster', 'Figure / Other', and 'Video'. Each example is accompanied by a visual representation, such as a circle pattern, a math problem, a poster, a manga figure, and a video sequence. The 'Visual Supplement' section contains examples of reasoning questions related to 'Table', 'Chart', 'Map', and 'OCR', each with a corresponding visual aid, such as a table with text, a bar chart, a map, and a text with OCR output. The slide is designed to illustrate different types of reasoning questions that can be used in benchmarks for evaluating AI reasoning capabilities.\n",
            "Caption for /content/drive/MyDrive/pages/page_285.png-- OCR: | CD\n",
            "mm\n",
            "\n",
            "111\n",
            "\n",
            "Reasoning is one of the fundamental intelligent behaviors of\n",
            "human beings, which requires understanding and analyzing given\n",
            "conditions and background knowledge to derive a new conclusion\n",
            "logically and rationally.\n",
            "On-going Directions of Research\n",
            "\n",
            "¢ MLLM Architectures\n",
            "\n",
            "¢ Efficiency and Scalability of MLLM Training Recipes\n",
            "\n",
            "¢ Long-context Support\n",
            "\n",
            "¢ Instruction Fine-tuning (IFT) Data\n",
            "\n",
            "¢ Reinforcement Learning for Multimodal Reasoning\n",
            "\n",
            "¢ Evaluation Benchmarks; Caption: The image is a slide from a presentation, featuring a title 'Remarks' at the top. Below the title, there is a paragraph explaining the concept of reasoning as a fundamental intelligent behavior of human beings. It emphasizes the need for understanding and analyzing given conditions and background knowledge to derive a new conclusion logically and rationally. The slide then lists 'On-going Directions of Research' which include MLLM Architectures, Efficiency and Scalability of MLLM Training Recipes, Long-context Support, Instruction Fine-tuning (IFT) Data, Reinforcement Learning for Multimodal Reasoning, and Evaluation Benchmarks. The slide number '111' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_286.png-- OCR: Learning to Act\n",
            "\n",
            "Wen-Huang Cheng (2832 2)\n",
            "\n",
            "National Taiwan University\n",
            "\n",
            " \n",
            "\n",
            "wenhuang@csie.ntu.edu.tw; Caption: The image displays a presentation slide with a blue background. At the top, there is a title \"Artificial Intelligence\" in white text. Below the title, the phrase \"Learning to Act\" is prominently displayed in large white letters. In the center of the slide, there is a logo of National Taiwan University, and beneath it, the name \"Wen-Huang Cheng (郭文皇)\" is written in white text, followed by the university's name and an email address \"wenhuang@csie.ntu.edu.tw\". On the right side of the slide, there is a graphic of a human head with the letters \"AI\" inside it, and a network of lines and nodes that represent a digital or technological theme. The slide number \"1\" is visible at the bottom left corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_287.png-- OCR: Ne. <\n",
            "\n",
            "Transformers without Normalization\n",
            "\n",
            "Jiachen Zhu’? ¢ Xinlei Chen’ ¢ Kaiming He? ¢ Yann LeCun’? ¢ Zhuang Liu!“t\n",
            "\n",
            "T Project Lead\n",
            "° 2 , Wie mane “2 PRINCETON\n",
            "OO Meta ° NYU ° I li TecmRaIee ° Beers\n",
            "\n",
            "CVPR 2025; Caption: The image is a slide from a presentation titled 'AI Weekly' with the main topic 'Transformers without Normalization'. It lists the names of five authors: Jiachun Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu, with the last name marked as the project lead. Below the authors' names, there are four logos representing different institutions: Meta, NYU, Massachusetts Institute of Technology (MIT), and Princeton University. At the bottom of the slide, there is a reference to 'CVPR 2025'. The slide is numbered '2' at the bottom left corner.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_288.png-- OCR: original block\n",
            "\n",
            "Attention / FFN\n",
            "\n",
            "Layer Norm\n",
            "\n",
            " \n",
            "\n",
            "wav2vec LN output\n",
            "\n",
            "ViT LN output\n",
            "\n",
            "®\n",
            "8\n",
            "\n",
            "DiT LN output\n",
            "\n",
            "x\n",
            "8\n",
            "\n",
            "°\n",
            "\n",
            "20\n",
            "\n",
            "-20\n",
            "\n",
            "LN output (y axis) vs. LN input (« axis)\n",
            "\n",
            "ND <\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "VIT LN layer 5 ViT LN layer 15 VITLNlayer20, VIT LN layer 25\n",
            "i ; Ss |\n",
            "| ra [ ae {>\n",
            "7\n",
            "10 0 10 -100 0 100-100 0 100-100 0 100\n",
            "wav2vec LN layer 5 wav2vec LN layer 30 wav2vec LN layer 35 wav2vec LN layer 40\n",
            "A E |\n",
            "40 0 10 -50 0 50-10 0 10-100 0 100\n",
            "DIT LN layer 5 DIT LN layer 13, DIT LN layer 21 DIT LN layer 28\n",
            "a a\n",
            "-200 0 200-1000 0 1000-1000 0 1000 -2000 0 2000\n",
            "LN input\n",
            "\n",
            "We introduce Dynamic Tanh (DyT)\n",
            "\n",
            "DyT(a) = tanh(aa),; Caption: The image appears to be a slide from a presentation, possibly related to artificial intelligence or machine learning, as indicated by the 'AI Weekly' text. It includes a flowchart on the left side, which seems to represent a process involving attention or feedforward neural networks (FFN), layer normalization, and scaling and shifting operations. On the right side, there are multiple graphs labeled 'VIT LN layer' and 'wav2vec LN layer', each showing a scatter plot with the x-axis labeled 'LN input' and the y-axis labeled 'LN output'. The graphs are color-coded in blue and show different patterns of data distribution across various layers. At the bottom of the slide, there is a mathematical expression 'DyT(x) = tanh(ax)', which suggests the introduction of a Dynamic Tanh function in the context of the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_289.png-- OCR: Ne <\n",
            "\n",
            "block with DyT\n",
            "\n",
            " \n",
            "\n",
            "original block\n",
            "\n",
            "Attention / FFN\n",
            "\n",
            "Layer Norm\n",
            "\n",
            " \n",
            "\n",
            "inference training\n",
            "LLaMA 7B layer model layer model\n",
            "RMSNorm 2.1s 14.1s 8.38 42.6s\n",
            "DyT 1.0s 13.0s 4.88 39.1s\n",
            "152.4% {7.8% {42.2% 18.2%\n",
            "\n",
            "reduction; Caption: The image is a slide from a presentation titled 'AI Weekly'. It compares the inference and training times of two different methods, RMSNorm and DyT, for a specific task. The slide is divided into two main sections, each illustrating a different method. The left section shows the 'original block' with components like 'Attention / FFN', 'Layer Norm', 'scale & shift', and 'normalize'. The right section shows the 'block with DyT' with an additional 'DyT' component and a 'tanh(αx)' function. Below the diagrams, there is a table comparing the inference and training times for LLaMA 7B, with RMSNorm and DyT. The table shows a significant reduction in time for both inference and training when using DyT compared to RMSNorm.\n",
            "Caption for /content/drive/MyDrive/pages/page_290.png-- OCR: CVPR 2025 (Rating: 555)\n",
            "\n",
            "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective\n",
            "\n",
            "Shaobo Wang!?  Yicun Yang? Zhiyuan Liu? ~Chenghao Sun?\n",
            "Xuming Hu? ConghuiHe*  Linfeng Zhang!?*\n",
            "1School of Artificial Intelligence, Shanghai Jiao Tong University\n",
            "?EPIC Lab, Shanghai Jiao Tong University\n",
            "3Hong Kong University of Science and Technology, Guangzhou\n",
            "4Shanghai Artificial Intelligence Laboratory\n",
            "{shaobowang1009, zhanglinfeng}@sjtu.edu.cn; Caption: The image displays a slide from a presentation with the title 'CVPR 2025 (Rating: 555)'. Below the title, there is a subtitle 'Dataset Distillation with Neural Characteristic Function: A Minmax Perspective'. The slide lists the names of six authors, their affiliations, and their email addresses. The affiliations include the School of Artificial Intelligence, EPIC Lab, Hong Kong University of Science and Technology, and Shanghai Artificial Intelligence Laboratory. The slide number '5' is also visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_291.png-- OCR: 50K Real Training Images\n",
            "oye . Ce BRMA Se Es\n",
            "Dataset Distillation G=Pineh |\n",
            "fas\n",
            "eee: Baca Distillation\n",
            "* Dataset distillation has emerged as\n",
            "a powerful approach for reducing\n",
            "\n",
            "data requirements in deep learning jain 0 Synthetic Training Images\n",
            "* i.e., condensing a large, real dataset wi ao AA\n",
            "\n",
            "into a smaller, synthetic one\n",
            "\n",
            "=\n",
            "\n",
            " \n",
            "\n",
            "Similar Test Performance; Caption: The image is a slide from a presentation, titled 'Dataset Distillation'. It explains the concept of dataset distillation as a method to reduce data requirements in deep learning by condensing a large, real dataset into a smaller, synthetic one. The slide includes a diagram illustrating the process, where 50,000 real training images are distilled into 10 synthetic training images, and both are used to train a model. The model's performance is similar on both the real and synthetic datasets.\n",
            "Caption for /content/drive/MyDrive/pages/page_292.png-- OCR: Achieved performances of this paper\n",
            "\n",
            "¢ 20.5% accuracy boost on ImageSquawk [CVPR 2022]\n",
            "* GPU memory usage reduction by over 300x\n",
            "¢ 20x faster processing speeds compared to state-of-the-art methods\n",
            "\n",
            "¢ The first work to achieve lossless compression of CIFAR-100 ona\n",
            "single NVIDIA 2080 Ti GPU using only 2.3 GB of memory; Caption: The image is a slide from a presentation, titled 'Achieved performances of this paper'. It lists four bullet points highlighting the key achievements of the paper. The first point mentions a 20.5% accuracy boost on ImageSquawk [CVPR 2022]. The second point states a GPU memory usage reduction by over 300x. The third point indicates that the processing speeds are 20x faster compared to state-of-the-art methods. The final point notes that this is the first work to achieve lossless compression of CIFAR-100 on a single NVIDIA 2080 Ti GPU using only 2.3 GB of memory. The slide number is indicated at the bottom right corner as '7'.\n",
            "Caption for /content/drive/MyDrive/pages/page_293.png-- OCR: Achieved performances of this paper\n",
            "\n",
            "    \n",
            "   \n",
            "       \n",
            "      \n",
            "\n",
            "Full Dataset\n",
            "\n",
            "a oOo\n",
            "oOo Oo\n",
            "\n",
            "Speed (second / iteration)\n",
            "\\ IPC 1 10 50\n",
            "' NCFM|0.8 0.7 1.0\n",
            "\n",
            "100\n",
            "0.8\n",
            "\n",
            "Test Accuracy (%)\n",
            "wo p\n",
            ">} oO\n",
            "\n",
            "NO\n",
            "oO\n",
            "\n",
            "1 10 50 100\n",
            "Image Per Class (IPC); Caption: The image is a graph displaying the achieved performances of a paper, specifically focusing on test accuracy as a function of image per class (IPC). The graph compares the performance of two methods, NCFM (Ours) and DATM, across different GPU memory capacities, ranging from 2G to 300G. The x-axis represents the IPC, while the y-axis shows the test accuracy percentage. The graph also includes a table that provides additional details on the speedup achieved by NCFM over DATM at different IPC values.\n",
            "Caption for /content/drive/MyDrive/pages/page_294.png-- OCR: Distribution Matching (DM)\n",
            "\n",
            "¢ Among various methods, distribution matching (DM) based\n",
            "approaches stand out for their balance of computational efficiency\n",
            "and strong performance.\n",
            "\n",
            "° A key challenge in DM lies in defining an effective metric to measure\n",
            "distributional discrepancies between real and synthetic datasets.; Caption: The image is a slide from a presentation with the title 'Distribution Matching (DM)'. It contains two bullet points of text. The first bullet point states that among various methods, distribution matching (DM) based approaches are notable for their balance of computational efficiency and strong performance. The second bullet point highlights a key challenge in DM, which is defining an effective metric to measure the discrepancies between real and synthetic datasets. The slide number '9' is visible at the bottom right corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_295.png-- OCR: DM: feature point matching methods\n",
            "\n",
            "¢ Early methods primarily employed Mean Squared Error (MSE) to\n",
            "compare point-wise features, which operates in Euclidean space, Zp.\n",
            "\n",
            "O Synthetic Data * Real Data\n",
            "\n",
            "peel a\n",
            "\n",
            "Luse = Epa |llf(@) — f(@)I\"]\n",
            "\n",
            "S ~ \\\n",
            "{ min L(D, D,|Zp) if L(D, D2|Zp 1 -\n",
            "f ( - a) . - ‘ » ( 3 - 7 ; where f denotes the feature extractor network, D and D\n",
            "i ~~ * represent the real and synthetic data distributions, respec-\n",
            "i o a 1 tively, x and & are samples drawn from D and D.\n",
            "! 1 !\n",
            "(e°) ft we] |\n",
            "(e°) it\n",
            "\n",
            "Oe i ee\n",
            "\n",
            "(a) Previous caesar optimize D to minimize the distance within Z.; Caption: The image is a slide from a presentation discussing feature point matching methods in the context of data matching. It includes a diagram illustrating the concept of Mean Squared Error (MSE) in Euclidean space, with synthetic and real data points, and a mathematical equation defining the MSE.\n",
            "Caption for /content/drive/MyDrive/pages/page_296.png-- OCR: DM: feature point matching methods\n",
            "\n",
            "However, MSE directly matches pixel-level or patch-level information\n",
            "without capturing the semantic structures embedded in highdimensional\n",
            "manifolds, which falls short for distribution comparison.\n",
            "\n",
            "O Synthetic Data * Real Data\n",
            "\n",
            "MSE aa fonn, -n--LZa_\n",
            "\n",
            "( ms D, |Z) ‘ ( oC D2|Zp, 4\n",
            "aN ‘8 ge |\n",
            "6 ¥ | |\n",
            "1 * ! ' :\n",
            "6) Le ve :\n",
            "\n",
            "a i end\n",
            "\n",
            "ee Previous ee olipa optimize D to minimize the distance within Z.; Caption: The image shows a slide from a presentation discussing feature point matching methods in the context of distribution comparison. It includes a diagram illustrating the previous paradigm of optimizing a variable to minimize the distance within a set Z, and a text explaining the limitations of the Mean Squared Error (MSE) method in capturing semantic structures in high-dimensional manifolds.\n",
            "Caption for /content/drive/MyDrive/pages/page_297.png-- OCR: DM: moment matching methods\n",
            "\n",
            "¢ Later methods employ Maximum Mean Discrepancy (MMD), aligning\n",
            "moments of distributions in a latent Hilbert space, Z,,.\n",
            "\n",
            "@ Synthetic Data * RealData Z: Latent MMD-=— w: Parameterized Network\n",
            "\n",
            "a Ea 6 i D2|Zp, min p= ae ‘ f 3 Dp|2s¢\n",
            "ors\n",
            "\n",
            "ew 4 / Gee\n",
            "\n",
            "we a * Rs\n",
            "*\n",
            "\n",
            "+\n",
            "\n",
            "wTmt ee ee ee ee ee er ee an a a ean a a are ee eee eee\n",
            "\n",
            "© Previous ee olipa optimize D to oi etediniae the distance within Z.; Caption: The image shows a diagram explaining moment matching methods in data analysis, specifically focusing on Maximum Mean Discrepancy (MMD) and its application in aligning moments of distributions in a latent Hilbert space.\n",
            "Caption for /content/drive/MyDrive/pages/page_298.png-- OCR: DM: moment matching methods\n",
            "\n",
            "While distributional equivalence implies moment equivalence, the\n",
            "converse is not necessarily true: aligning moments alone does not\n",
            "guarantee full distributional matching.\n",
            "\n",
            "@ Synthetic Data * RealData Z: Latent MMD-=— w: Parameterized Network\n",
            "\n",
            "ee a i D2|Zp, Ss me ae |e A Dp|2s¢\n",
            "eee (3 (%\n",
            "\n",
            "!\n",
            "\\\n",
            ";\n",
            "\n",
            "an\n",
            "\n",
            "ioe Xk\n",
            "\n",
            "oe\n",
            "—\n",
            "&\n",
            "on\n",
            "\n",
            "wom ee ee ee ee er ww mw eww eww ee ee Se em mmm mmm ew en eZ Meee rrr rrr\n",
            "\n",
            "ee Previous ee olipa optimize D to minimize the distance within Z.; Caption: The image shows a slide from a presentation discussing moment matching methods in the context of data alignment. It includes a title, a brief explanatory text, and two diagrams illustrating the concepts of MSE (Mean Squared Error) and MMD (Maximum Mean Discrepancy). The diagrams depict the process of aligning synthetic data with real data in a latent space, using a parameterized network to minimize the distance within the latent space.\n",
            "Caption for /content/drive/MyDrive/pages/page_299.png-- OCR: DM: moment matching methods\n",
            "\n",
            "¢ MMD-based methods align feature moments in the embedded\n",
            "domain but may not effectively align the overall distributions.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "(a) From Real to Complex (b) Distribution Matching (DM) Real Synthetic\n",
            "ed : minimizing MMD doesn’t effectively make the distributions similar\n",
            "\n",
            "Real\n",
            "—> eit f) , 90°\n",
            "\n",
            "x~DdD=>\n",
            "\n",
            "  \n",
            "\n",
            "77 FR — ORO ae elit F%))\n",
            "X¥~D LY’\n",
            "Synthetic Feature Net\n",
            "\n",
            "Euler’s Formula\n",
            "e/4x = cos(a,(t)) + j sin(ax(€))\n",
            "\n",
            "Complex Plane\n",
            "\n",
            " \n",
            "\n",
            "©,(t) = Ex-ple#’!] = |, (Olei@x©\n",
            "\n",
            "Iteration 0 Iteration 5000 Iteration 10000\n",
            "\n",
            "V 4\n",
            "amplitude phase\n",
            "\n",
            "14; Caption: The image is a slide from a presentation discussing moment matching methods (MMD) in the context of aligning feature moments in the embedded domain. It includes two main sections: (a) From Real to Complex and (b) Distribution Matching (DM). The first section illustrates the transition from real to complex domain using frequency argument and Euler's formula, while the second section shows three polar plots representing the distribution matching process at different iterations. The slide also contains mathematical expressions related to the topic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_300.png-- OCR: Adversarial Distribution Matching\n",
            "\n",
            "¢ This paper proposes a novel approach that reformulates distribution\n",
            "matching as an adversarial minmax optimization problem.\n",
            "\n",
            "¢ We maximize the discrepancy measure to define a robust discrepancy metric,\n",
            "parameterized by a neural network wW.\n",
            "\n",
            "Concurrently, we minimize the discrepancy between the synthetic dataset D~\n",
            "and the real dataset D by optimizing the synthetic data distribution D~.\n",
            "\n",
            "Formally, this minmax optimization problem is expressed as:\n",
            "\n",
            "min max L(D, D, f, v)\n",
            "D YW\n",
            "\n",
            "where L denotes the discrepancy measure, f is the feature extractor network,\n",
            "and w is the network learning the discrepancy metric.\n",
            "\n",
            "This minmax framework seeks the optimal synthetic data distribution D~ that\n",
            "minimizes L while network W maximizes L to establish a robust metric.; Caption: The image is a slide from a presentation titled 'Adversarial Distribution Matching'. It contains text explaining a novel approach to distribution matching as an adversarial minmax optimization problem. The slide outlines the process of maximizing a discrepancy measure to define a robust discrepancy metric, parameterized by a neural network. It also describes the process of minimizing the discrepancy between a synthetic dataset and a real dataset by optimizing the synthetic data distribution. The slide includes a formal expression of the minmax optimization problem, where 'L' denotes the discrepancy measure, 'f' is the feature extractor network, and 'ψ' is the network learning the discrepancy metric. The slide concludes by stating that this framework seeks the optimal synthetic data distribution that minimizes 'L' while the network 'ψ' maximizes 'L' to establish a robust metric. The slide number is indicated as 15.\n",
            "Caption for /content/drive/MyDrive/pages/page_301.png-- OCR: Adversarial Distribution Matching\n",
            "\n",
            "max “ee By.) if 7c Di|Zy,) 1 min L(D, D;|Zy,) pi Zee | BalZy,)\n",
            "\n",
            "* > 77 BRN i oo ek » - be\n",
            "(@%) i ra es\n",
            "Ce) H@°o) y (e°) te\n",
            "\n",
            "eee\n",
            "\n",
            "ecco\n",
            "\n",
            "Seocemeeeeeeees’?® Sseseeeeemeeeeres”® 8 Sseeeeeeeeece ==\n",
            "\n",
            ") Our minmax ie first opumize w to maximize the distance in\n",
            "parameterized space Zy,, then optimize D to minimize the distance within Zy.\n",
            "\n",
            "a estas Ga eee; Caption: The image depicts a process called Adversarial Distribution Matching, specifically illustrating a minimax paradigm. It shows a sequence of steps involving the optimization of parameters to maximize and minimize distances within parameterized spaces.\n",
            "Caption for /content/drive/MyDrive/pages/page_302.png-- OCR: Distribution Matching with NCFD\n",
            "\n",
            "¢ CF-based methods directly compare distributions by balancing the\n",
            "amplitude and phase in the complex plane, enhancing distributional\n",
            "similarity.\n",
            "\n",
            " \n",
            "\n",
            "Characteristic Function Matching (Ours) Real Synthetic\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "minimizing CF makes the distributions similar\n",
            "\n",
            " \n",
            "\n",
            "Iteration 0 Iteration 5000 Iteration 10000; Caption: The image displays a slide titled 'Distribution Matching with NCFD'. It explains that CF-based methods compare distributions by balancing amplitude and phase in the complex plane, which enhances distributional similarity. The slide includes a caption 'Characteristic Function Matching (Ours)' and a statement 'minimizing CF makes the distributions similar'. There are three radar charts labeled 'Iteration 0', 'Iteration 5000', and 'Iteration 10000', showing the comparison between 'Real' and 'Synthetic' data distributions. The charts show the progression of the distribution similarity over iterations, with the 'Real' data represented in blue and the 'Synthetic' data in red. The slide is numbered 17.\n",
            "Caption for /content/drive/MyDrive/pages/page_303.png-- OCR: Dataset Distillation with NCFM\n",
            "\n",
            "* Real and synthetic data points are sampled and embedded through a feature\n",
            "extractor network. The synthetic data is optimized by minimizing the\n",
            "distributional discrepancy between real and synthetic data, measured via the\n",
            "Neural Characteristic Function Discrepancy (NCFD) in the complex plane.\n",
            "Additionally, an auxiliary network learns an optimal sampling distribution for the\n",
            "frequency arguments of the characteristic function.\n",
            "\n",
            "Real Dataset D Complex Plane\n",
            "\n",
            "Feature Network 0\n",
            "x w\n",
            "— “f@ 7\n",
            "\n",
            "Synthetic Dataset D -\n",
            "\n",
            "“\\— DSS fo. oi\n",
            "yf |  . 5 Ave\n",
            "x P £=(0,(H) —O())(O:1 - Fx) Lor \\\n",
            "— ~ f@® = se Y . 2 eee 2 Maximize L to learn a\n",
            "Minimize L to match the distribution which samples\n",
            "\n",
            "©,(t) = Ey-ple*' | Sampling Network\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "real dataset distribution * @x(t) = Eyple/’ | frequency arguments t\n",
            "\n",
            " \n",
            "\n",
            "- —> V5 L£ Gradient for Synthetic Dataset min LUFC), F@; t= Wo(e))\n",
            "D\n",
            "\n",
            "- —> Vo £ Gradient for Sampling Network max £(f (x), f@); te Pole)\n",
            "\n",
            "18; Caption: Slide 18: Dataset Distillation with NCFM\n",
            "Caption for /content/drive/MyDrive/pages/page_304.png-- OCR: The scope of this course\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ME) | 8:2 B=\n",
            "\n",
            "MLLM\n",
            "Agent Single Agent : Agent-Agent\n",
            "\n",
            "Agent-Human\n",
            "\n",
            " \n",
            "\n",
            "Applications\n",
            "\n",
            "Symbolic\n",
            "Agent\n",
            "\n",
            "    \n",
            "    \n",
            "\n",
            "Deep\n",
            "Reinforcement\n",
            "Learning ean\n",
            "Agent Nace eee eceeeceeeeneeeeene Human & Resources a)\n",
            "\n",
            " \n",
            "\n",
            "°\n",
            "Ee Simulated Agent Society; Caption: The image is a slide from a presentation, titled 'The scope of this course'. It outlines the main topics covered in the course, which include MLLM Agent, Single Agent, Agent-Agent, Agent-Human, Symbolic Agent, and Deep Reinforcement Learning Agent. The slide also features a diagram labeled 'Applications' that illustrates the interaction between agents, their environment, and human resources. The diagram includes elements such as a simulated agent society, virtual and physical environments, and the perception of human and resources. The slide is numbered 19, indicating its position within the presentation.\n",
            "Caption for /content/drive/MyDrive/pages/page_305.png-- OCR: YJ This Lecture — Agenda C\n",
            "mm\n",
            "\n",
            "= Markov Decision Processes (MDP)\n",
            "= Defining MDP\n",
            "m Policy Methods\n",
            "= Reinforcement Learning\n",
            "m Passive Reinforcement Learning\n",
            "= Active Reinforcement Learning\n",
            "= Approximate Reinforcement Learning\n",
            "m Case Study\n",
            "e Language Model Self-improvement\n",
            "\n",
            "20; Caption: The image is a slide from a presentation, specifically the agenda for a lecture. It includes a list of topics that will be covered, such as Markov Decision Processes (MDP), Reinforcement Learning, and a Case Study on Language Model Self-improvement.\n",
            "Caption for /content/drive/MyDrive/pages/page_306.png-- OCR: YJ Some slides of this lecture credit to: C\n",
            "mm\n",
            "\n",
            "University of California, Berkeley - CS 188: Artificial Intelligence\n",
            "\n",
            "Berkeley\n",
            "\n",
            "UNIVERSITY OF CALIFORNIA\n",
            "\n",
            " \n",
            "\n",
            "21; Caption: The image displays a slide from a lecture, specifically slide number 21. It contains text and two logos. The text at the top of the slide reads 'Some slides of this lecture credit to:' followed by 'University of California, Berkeley - CS 188: Artificial Intelligence.' Below the text, there are two logos side by side. On the left is the 'CS188' logo, which features a stylized robot with a blue and yellow color scheme. On the right is the 'Berkeley University of California' logo, which is a blue and gold emblem with the university's name. The slide has a simple layout with a white background and the text and logos are centrally aligned.\n",
            "Caption for /content/drive/MyDrive/pages/page_307.png-- OCR: Non-Deterministic Search; Caption: The image depicts a cartoon of an astronaut in a distressed state, with a hole in the ground and fire on one side, and a blue diamond on the other side, suggesting a search for the diamond.\n",
            "Caption for /content/drive/MyDrive/pages/page_308.png-- OCR: Example: Racing; Caption: The image shows a blue car racing on a curved road.\n",
            "Caption for /content/drive/MyDrive/pages/page_309.png-- OCR: Example: Racing\n",
            "\n",
            " \n",
            "\n",
            "= Arobot car wants to travel far, quickly\n",
            "m Three states: Cool, Warm, Overheated\n",
            "m= Twoactions: Slow, Fast\n",
            "\n",
            "m Going faster gets double reward\n",
            "\n",
            "0.5\n",
            "\n",
            "    \n",
            "  \n",
            "\n",
            "Slow\n",
            "\n",
            "   \n",
            "\n",
            "40 Overheated; Caption: This image is a slide from a presentation, titled \"Example: Racing\". It contains a diagram illustrating the states and actions of a robot car in a racing scenario. The diagram is divided into three main parts, each representing a different state of the car: Cool, Warm, and Overheated. Each state is connected by arrows indicating the actions that can be taken, such as \"Slow\" or \"Fast\", and the resulting state transitions. The diagram also includes numerical values indicating the reward or penalty associated with each action and state transition. The slide text provides additional context, explaining that the robot car aims to travel far quickly, and that going faster results in double the reward.\n",
            "Caption for /content/drive/MyDrive/pages/page_310.png-- OCR: Example: Grid World\n",
            "\n",
            " \n",
            "\n",
            "a Amazeike problem\n",
            "m Theagent lives in agrid\n",
            "am Walls block the agent's path\n",
            "\n",
            "= Noisy movement: actions do not always go as planned\n",
            "um 80% of the time, the action North takes the agent North\n",
            "\n",
            "(if there is no wall there)\n",
            "10% of the time, North takes the agent West; 10% East\n",
            "\n",
            "u_ If there is awall in the direction the agent would have\n",
            "been taken, the agent stays put\n",
            "\n",
            " \n",
            "\n",
            "m Theagent receives rewards each time step\n",
            "= Small “living” reward each step (can be negative)\n",
            "m_ Big rewards come at the end (good or bad)\n",
            "\n",
            "= Goal: maximize sum of rewards; Caption: Slide with information about Grid World.\n",
            "Caption for /content/drive/MyDrive/pages/page_311.png-- OCR: Grid World Actions\n",
            "\n",
            "Deterministic Grid World Stochastic Grid World; Caption: The image is a slide titled 'Grid World Actions' and is divided into two sections: 'Deterministic Grid World' and 'Stochastic Grid World'. The 'Deterministic Grid World' section shows a grid with a fire and a sprinkler, with an arrow pointing downwards, indicating a direct action. The 'Stochastic Grid World' section shows a grid with a fire and a sprinkler, with a question mark and three arrows pointing downwards, each with a probability value (0.1, 0.8, 0.1), indicating a probabilistic action.\n",
            "Caption for /content/drive/MyDrive/pages/page_312.png-- OCR: Markov Decision Processes\n",
            "\n",
            " \n",
            "\n",
            "mg An MDPis defined by:\n",
            "\n",
            "= Aset of statesseS\n",
            "\n",
            "ms Asetofactionsa A\n",
            "\n",
            "= Atransition function T(s, a, s’)\n",
            "= Probability that a from s leads to s’, i.e., P(s’| s, a)\n",
            "= Also called the model or the dynamics\n",
            "\n",
            "= Areward function R(s, a, s’)\n",
            "= Sometimes just R(S) or R(s’)\n",
            "\n",
            "a Astart state\n",
            "\n",
            "mu Maybe aterminal state; Caption: Slide titled 'Markov Decision Processes'.\n",
            "Caption for /content/drive/MyDrive/pages/page_313.png-- OCR: What is Markov about MDPs?\n",
            "\n",
            " \n",
            "\n",
            "mu “Markov” generally means that given the present state, the\n",
            "future and the past are independent\n",
            "\n",
            "m For Markov decision processes, “Markov” means action\n",
            "outcomes depend only on the current state\n",
            "\n",
            "P(St41 = s'|S¢ = St, At = at, S¢—1 = St—1, At-1, ... 50 = Ty)\n",
            "\n",
            " \n",
            "\n",
            "Andrey Markov\n",
            "P(St41 = 8’|S; = 54, Ap = at) (1856-1922)\n",
            "\n",
            "m Thisis just like search, where the successor function could only\n",
            "depend on the current state (not the history); Caption: The image is a slide from a presentation with the title 'What is Markov about MDPS?' It contains text explaining the concept of Markov decision processes (MDPs) and includes a portrait of a man with the name 'Andrey Markov (1856-1922)' below it. The slide text defines 'Markov' in the context of MDPs, stating that given the present state, the future and the past are independent. It further explains that for Markov decision processes, 'Markov' means that action outcomes depend only on the current state. The slide also includes mathematical expressions related to the probability of a state transition given the current state and action. The slide text concludes by comparing this to search, where the successor function could only depend on the current state, not the history.\n",
            "Caption for /content/drive/MyDrive/pages/page_314.png-- OCR: Policies\n",
            "\n",
            " \n",
            "\n",
            "  \n",
            "      \n",
            "\n",
            "m In deterministic single-agent search problems,\n",
            "we wanted an optimal plan, or sequence of\n",
            "actions, from start to a goal\n",
            "\n",
            " \n",
            "   \n",
            "\n",
            "wae\n",
            "\n",
            "Wr\n",
            "Te\n",
            "\n",
            "      \n",
            "\n",
            "= For MDPs, we want an optimal policy z*: SA\n",
            "m Apolicy zgives an action for each state\n",
            "\n",
            "= Anoptimal policy is one that maximizes\n",
            "expected utility if followed\n",
            "\n",
            "a Anexplicit policy defines a reflex agent; Caption: The image shows a hand holding a paper with a grid of squares, each containing a diamond symbol and a skull symbol, with arrows pointing from the diamonds to the skulls.\n",
            "Caption for /content/drive/MyDrive/pages/page_315.png-- OCR: Optimal Policies\n",
            "\n",
            "3 °\n",
            "9S N\n",
            "ll Il\n",
            "— “nh\n",
            ") n\n",
            "—\n",
            "S S\n",
            "oO 7\n",
            "Il\n",
            "Ml DB\n",
            "A\n",
            "a G\n",
            "G\n",
            "2 vd; Caption: The image displays a slide titled 'Optimal Policies' with four separate grids, each containing a combination of blue arrows pointing in different directions and blue squares with either a plus or minus sign. Below each grid, there is a value labeled 'R(s)'. The grids are arranged in a 2x2 format, and the arrows seem to represent different actions or decisions within a policy framework.\n",
            "Caption for /content/drive/MyDrive/pages/page_316.png-- OCR: MDP Search Trees\n",
            "\n",
            "m Each MDP state projects an expectimax-like search tree\n",
            "\n",
            " \n",
            "\n",
            "In expectimax, or a general MDP. it forms a computation\n",
            "tree that considers all of the possible actions so that you\n",
            "can max over them and the:\n",
            "\n",
            " \n",
            "\n",
            "(s, a)isa\n",
            "q-state\n",
            "\n",
            "  \n",
            " \n",
            "\n",
            "(s,a,s’) called a transition\n",
            "T(s,a,s’) = P(s’|s,a)\n",
            "\n",
            "R(s,a,s’) oe; Caption: Slide titled \"MDP Search Trees\" with a diagram and text explaining MDP states and search trees.\n",
            "Caption for /content/drive/MyDrive/pages/page_317.png-- OCR: Utilities of Sequences; Caption: The image shows a robot with a headlamp and a line connecting it to a pile of colorful gems.\n",
            "Caption for /content/drive/MyDrive/pages/page_318.png-- OCR: Utilities of Sequences\n",
            "mg What preferences should an agent have over reward sequences?\n",
            "m Moreorless? [1,2,2] or [2,3,4]\n",
            "\n",
            "= Noworlater? [0,0,1] or [1,0,0] .; Caption: The image displays a slide titled 'Utilities of Sequences' with bullet points discussing what preferences an agent should have over reward sequences. It poses questions about whether an agent should prefer more or less rewards and whether it should prefer immediate or delayed rewards, providing examples of sequences for each. Additionally, there is a cartoon illustration of a robot on the right side of the slide.\n",
            "Caption for /content/drive/MyDrive/pages/page_319.png-- OCR: Discounting\n",
            "\n",
            " \n",
            "\n",
            "mg It’s reasonable to maximize the sum of rewards\n",
            "mg It’s also reasonable to prefer rewards now to rewards later\n",
            "m One solution: values of rewards decay exponentially\n",
            "\n",
            "SC “7\n",
            "VG Vi he.\n",
            "\n",
            "1 7 7\n",
            "Worth Now Worth Next Step Worth In Two Steps; Caption: The image is a slide titled 'Discounting' with three bullet points explaining the concept. Below the text, there are three images of diamonds with different levels of decay, labeled 'Worth Now', 'Worth Next Step', and 'Worth In Two Steps'.\n",
            "Caption for /content/drive/MyDrive/pages/page_320.png-- OCR: Discounting\n",
            "\n",
            " \n",
            "\n",
            "m Howto discount? |\n",
            "m Each time we descend a level, we WJ (ew\n",
            "\n",
            "multiply in the discount once YP 1 4\n",
            "\n",
            "m Why discount?\n",
            "m= Sooner rewards probably do have\n",
            "\n",
            "if\n",
            "higher utility than later rewards y a\n",
            "\n",
            "a Also helps our algorithms converge\n",
            "\n",
            "m= Example: discount of 0.5 5 _ oa\n",
            "mw U((1,2,3]) =1*1 +0.5*2 + 0.25*3 =2.75 J\n",
            "m U([3,2,1]) =1*3 +0.5*2 +0.25*1 =5.25 Gb. -\n",
            "a U((1,2,3]) <U([3,2,1]) L; Caption: Discounting\n",
            "Caption for /content/drive/MyDrive/pages/page_321.png-- OCR: Quiz: Discounting\n",
            "\n",
            " \n",
            "\n",
            "a Given: 10 1\n",
            "a b c d e\n",
            "= Actions: East, West, and Exit (only available in exit states a, e)\n",
            "m Transitions: deterministic\n",
            "\n",
            " \n",
            "\n",
            "m Quiz 1: For y = 1, what is the optimal policy? 10\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "m Quiz 2: Fory=0.1, what is the optimal policy? 10\n",
            "\n",
            "m Quiz 3: For which y are West and East equally good when in state d?; Caption: Given: \n",
            "- Actions: East, West, and Exit (only available in exit states a, e)\n",
            "- Transitions: deterministic\n",
            "Quiz 1: For γ = 1, what is the optimal policy?\n",
            "Quiz 2: For γ=0.1, what is the optimal policy?\n",
            "Quiz 3: For which γ are West and East equally good when in state d?\n",
            "Caption for /content/drive/MyDrive/pages/page_322.png-- OCR: Infinite Utilities?!\n",
            "\n",
            " \n",
            "\n",
            "m Problem: What if the game lasts forever? Do we get infinite rewards?\n",
            "\n",
            "= Solutions:\n",
            "g Finite horizon: (similar to depth-limited search)\n",
            "u Terminate episodes after a fixed T steps (e.g. life)\n",
            "m Gives nonstationary policies (depends on time left); Caption: The image shows a slide from a presentation with the title 'Infinite Utilities?!' It discusses a problem related to games lasting forever and the possibility of infinite rewards. The slide lists two solutions: 'Finite horizon' and 'Infinite horizon.' The 'Finite horizon' solution is compared to depth-limited search and involves terminating episodes after a fixed number of steps, giving nonstationary policies that depend on the time left. The 'Infinite horizon' solution is not elaborated upon. There is also a cartoon illustration of a car driving on a road that loops back on itself, symbolizing an infinite loop. Additionally, there is a photograph of a track and field event, specifically a relay race, with athletes in motion.\n",
            "Caption for /content/drive/MyDrive/pages/page_323.png-- OCR: Infinite Utilities?!\n",
            "\n",
            " \n",
            "\n",
            "m Problem: What if the game lasts forever? Do we get infinite rewards?\n",
            "\n",
            "m Solutions:\n",
            "g Finite horizon: (similar to depth-limited search)\n",
            "= Terminate episodes after a fixed T steps (e.g. life)\n",
            "u Gives nonstationary policies (depends on time left)\n",
            "\n",
            " \n",
            "\n",
            "u Discounting: use 0 <y <1\n",
            "CO\n",
            "U([ro,--- Pool) = So y'r¢ < Rmax/(1 — 7)\n",
            "t=0\n",
            "= Smaller y means smaller “horizon” — shorter term focus\n",
            "\n",
            "m Absorbing state: guarantee that for every policy, a terminal state will eventually\n",
            "be reached (like “overheated” for racing); Caption: Slide with information about infinite utilities.\n",
            "Caption for /content/drive/MyDrive/pages/page_324.png-- OCR: Recap: Defining MDPs\n",
            "\n",
            "= Markov decision processes:\n",
            "w Set of states S\n",
            "m Start state so ae\n",
            "m= Set of actions A “\n",
            "m Transitions P(s'|s,a) (or T(s,a,s’)) uo\n",
            "m Rewards R(s,a,s’) (and discount y) 8,88\n",
            "\n",
            " \n",
            "\n",
            "= MDP quantities so far:\n",
            "= Policy = Choice of action for each state\n",
            "= Utility =sum of (discounted) rewards; Caption: Slide with a recap of defining Markov decision processes.\n",
            "Caption for /content/drive/MyDrive/pages/page_325.png-- OCR: Solving MDPs\n",
            "\n",
            "   \n",
            "\n",
            "=a\n",
            "Aref; Caption: The image shows a hand holding a piece of paper with a grid of squares, some of which contain symbols like a diamond, a skull, and arrows pointing in different directions.\n",
            "Caption for /content/drive/MyDrive/pages/page_326.png-- OCR: Optimal Quantities\n",
            "\n",
            "m The value (utility) of a state s:\n",
            "V*(s) = expected utility starting in sand\n",
            "\n",
            ", ‘ sisa\n",
            "acting optimally state\n",
            "m= The value (utility) of a q-state (s,a): ote a\n",
            "Q’(s,a) = expected utility starting out\n",
            "having taken action a from state sand ' (as)isa\n",
            "(thereafter) acting optimally transition\n",
            "\n",
            " \n",
            "\n",
            "m The optimal policy:\n",
            "1 (Ss) = optimal action from state s; Caption: Slide titled \"Optimal Quantities\" with bullet points explaining the value of a state and a q-state, and the optimal policy. There is also a diagram illustrating a state, action, and transition with expected utilities.\n",
            "Caption for /content/drive/MyDrive/pages/page_327.png-- OCR: Example — Gridworld V* Values\n",
            "\n",
            " \n",
            "\n",
            "Values after 100 iterations; Caption: The image displays a grid of values, specifically a 4x3 grid, with each cell containing a numerical value. The grid is color-coded, with most cells in green, one cell in red, and one cell in gray. The red cell contains the value -1.00, and the gray cell contains the value 0.57. The green cells contain various other numerical values. The title 'Example – Gridworld V* Values' suggests that this grid represents values in a gridworld model after 100 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_328.png-- OCR: Example — Gridworld Q* Values\n",
            "\n",
            "PAPAS\n",
            "Ww ON\n",
            "a ..\\\n",
            "\n",
            "en; Caption: The image displays a grid of numbers representing Q* values after 100 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_329.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) =\n",
            "\n",
            " \n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "(s,a)isa\n",
            "g-state\n",
            "\n",
            "(s,a,s’))isa\n",
            "transition; Caption: Values of States\n",
            "Caption for /content/drive/MyDrive/pages/page_330.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            " \n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "(s,a)isa\n",
            "g-state\n",
            "\n",
            "(s,a,s’))isa\n",
            "transition; Caption: Values of States\n",
            "Caption for /content/drive/MyDrive/pages/page_331.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "(s,a)isa\n",
            "Q*(s,a) = R(s, a, s') o's V*(s') q-state\n",
            "\n",
            "(s,a,s’))isa\n",
            "transition; Caption: Values of States\n",
            "Caption for /content/drive/MyDrive/pages/page_332.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            "Q*(s,a) = S°T(s, a, s') [R(s, a, s')+ yV*(s')|\n",
            "\n",
            " \n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "(s,a)isa\n",
            "g-state\n",
            "\n",
            "(s,a,s’))isa\n",
            "transition; Caption: Values of States\n",
            "Caption for /content/drive/MyDrive/pages/page_333.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "i _ (s,a)isa\n",
            "Q*(s,a) = VIT(s,a, 8!) [RCs 4,8\") + 9V*(s')) g-state\n",
            "(s,a,s’))isa\n",
            "transition\n",
            "\n",
            "V*(s) = max) | T(s,4, s') [R(s, a, s')+ yV*(s')|; Caption: Values of States\n",
            "Caption for /content/drive/MyDrive/pages/page_334.png-- OCR: Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "(s,a)isa\n",
            "Oo\", a) DTCs: a, 5’) [R(s, a, s’) + yV*(s')| q-state\n",
            "s\n",
            "(s,a,s’))isa\n",
            "transition\n",
            "\n",
            "V*(s) = max) | T(s,4, s') [R(s, a, s')+ yV*(s')|\n",
            "\n",
            " \n",
            "\n",
            "= But how do we solve these equations?; Caption: Values of States\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_335.png-- OCR: Recall: Racing\n",
            "\n",
            " \n",
            "\n",
            "= Arobot car wants to travel far, quickly\n",
            "m Three states: Cool, Warm, Overheated\n",
            "m= Twoactions: Slow, Fast\n",
            "\n",
            "m Going faster gets double reward\n",
            "\n",
            "0.5\n",
            "\n",
            "    \n",
            "  \n",
            "\n",
            "Slow\n",
            "\n",
            "   \n",
            "\n",
            "40 Overheated; Caption: This image is a slide from a presentation titled \"Recall: Racing\". It features a diagram illustrating the states and actions of a robot car in a racing scenario. The diagram includes three states: Cool, Warm, and Overheated, and two actions: Slow and Fast. Each state and action is connected by arrows indicating the transition probabilities and rewards. The Cool state can transition to Warm or Overheated with a probability of 0.5 each, and to Cool with a probability of 1.0. The Warm state can transition to Overheated with a probability of 0.5 and back to Cool with a probability of 0.5. The Overheated state can transition back to Warm with a probability of 0.5 and to Cool with a probability of 0.5. The rewards are +1 for transitioning to Cool, +2 for transitioning to Warm, and -10 for transitioning to Overheated. The slide also includes a note that going faster gets double the reward.\n",
            "Caption for /content/drive/MyDrive/pages/page_336.png-- OCR: Racing Search Tree; Caption: The image displays a Racing Search Tree.\n",
            "Caption for /content/drive/MyDrive/pages/page_337.png-- OCR: Racing Search Tree\n",
            "\n",
            " \n",
            "\n",
            "mw We're doing way too much\n",
            "\n",
            "QD\n",
            "work with expectimax! Se\n",
            "&s\n",
            "m Problem: States are repeated\n",
            "m Idea: Only compute needed\n",
            "quantities once, cache the rest in > & ® &® &® € ® &\n",
            "~~ fh, fh \\ (}, fh { fh, Q\n",
            "m Problem: Tree goes on forever on\n",
            "m Idea: Do a depth-limited\n",
            "computation, but with increasing ft\n",
            "depths until change is small\n",
            "\n",
            "= Note: deep parts of the tree CWAWUT VWUWA EMME CWA CWA ad\n",
            "eventually don’t matter if y <1; Caption: The image is a slide titled 'Racing Search Tree' with a diagram illustrating a search tree. The tree has nodes with cars at the top, and arrows pointing downwards to smaller nodes with cars and arrows. The slide text discusses issues with using 'expector' and suggests solutions like caching and limiting tree depth.\n",
            "Caption for /content/drive/MyDrive/pages/page_338.png-- OCR: Time-Limited Values\n",
            "\n",
            " \n",
            "\n",
            "m Key idea: time-limited values\n",
            "\n",
            "mu Define V,(s) to be the optimal value of sif the game ends\n",
            "ink more time steps\n",
            "a Equivalently, it’s what a depth-k expectimax would give from s; Caption: Time-Limited Values\n",
            "Caption for /content/drive/MyDrive/pages/page_339.png-- OCR: Computing Time-Limited Values\n",
            "\n",
            "CD\n",
            "\n",
            "V3( >) V3( @) V3( 4% )\n",
            "\n",
            "  \n",
            "\n",
            "Vo( >) Vo( @) )| SoS & SS & & &\n",
            "\n",
            "Vile) Vile) Vi(@s)| Co\n",
            "\n",
            "a a\n",
            "\n",
            "Vo( @ ) Vo( @) Vo( 4% ) WAMHY HAMMAM WAM Ca; Caption: Computing Time-Limited Values\n",
            "Caption for /content/drive/MyDrive/pages/page_340.png-- OCR: Value Iteration; Caption: The image shows a cartoon robot with a chef's hat, holding a slice of cake with the letter 'n' on it, and standing next to a stack of three cakes with 'n-1', 'n-2', and 'n' written on them respectively.\n",
            "Caption for /content/drive/MyDrive/pages/page_341.png-- OCR: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            "m Start with Vo(s) = 0: no time steps left means an expected reward sum of zero\n",
            "\n",
            "m Given vector of V(S) values, do one step of expectimax from each state:\n",
            "Veils) — max S> T(s, a, 8’) [R(s, a,s’) + ¥ VieCs)|\n",
            "sg!\n",
            "\n",
            "m Repeat until convergence, which yields V*; Caption: Slide titled Value Iteration.\n",
            "Caption for /content/drive/MyDrive/pages/page_342.png-- OCR: Example: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            "Overheated\n",
            "\n",
            "Vi\n",
            "\n",
            "Vatils) — max) /T(s, a, s’) [RCs, a,s’) + 1Vels’)]\n",
            "\n",
            "s; Caption: The image shows a diagram of value iteration with three states, V0, V1, and V2, and a formula for calculating the next state value. There are also three cars with different conditions and arrows indicating the transitions between states.\n",
            "Caption for /content/drive/MyDrive/pages/page_343.png-- OCR: Example: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            "B& EB S&S Sew\n",
            "\n",
            "y\n",
            "0.5 -\n",
            "Q\n",
            "Va Slow 4\n",
            "> Fast 0.5 +2\n",
            "‘ool 0.5\n",
            "+2\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "Overheated\n",
            "\n",
            "Cc\n",
            "\n",
            "Assume no discount!\n",
            "\n",
            "s!\n",
            "\n",
            "a=slow: 1(1+0)=1\n",
            "\n",
            "a=fast: 0.5(2 +0) +0.5(2 +0) =2; Caption: The image shows a diagram of value iteration with three states represented by cars in different conditions. The states are labeled V0, V1, and V2. The diagram includes arrows indicating the transitions between states with associated probabilities and rewards. There is also a mathematical formula for value iteration and two calculations for the value of states when the action is slow or fast.\n",
            "Caption for /content/drive/MyDrive/pages/page_344.png-- OCR: Example: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            "B& EB S&S Sew\n",
            "\n",
            "y\n",
            "0.5 -\n",
            "Q\n",
            "Va Slow 4\n",
            "> Fast 0.5 +2\n",
            "‘ool 0.5\n",
            "+2\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "Overheated\n",
            "\n",
            "Cc\n",
            "\n",
            "Assume no discount!\n",
            "\n",
            "s!\n",
            "\n",
            "a=slow: 0.5(1 +0) +0.5(1 +0) =1\n",
            "\n",
            "a=fast: 1(-10 +0) =-10; Caption: The image is a slide from a presentation titled \"Example: Value Iteration\". It includes a diagram with three cars in different states, labeled V2, V1, and V0. The cars are depicted in various conditions such as fast, slow, warm, and overheated. There are arrows indicating the transition probabilities and rewards for each action (slow or fast) taken in each state. The slide also includes a mathematical formula for value iteration and two calculations for the value of being in a slow or fast state.\n",
            "Caption for /content/drive/MyDrive/pages/page_345.png-- OCR: Example: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "SS G\n",
            "°\n",
            "Vi 2 1 0 : Assume no discount!\n",
            "Vi-ti(s) — max >> T(s,a,s\") |R(s,a, 8’) +7 Ve(s’)]\n",
            "\n",
            "a=fast 0.5(2 +2) +0.5(2+1)=3.5; Caption: The image shows a diagram explaining the concept of value iteration in reinforcement learning.\n",
            "Caption for /content/drive/MyDrive/pages/page_346.png-- OCR: Example: Value Iteration\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Assume no discount!\n",
            "\n",
            "2 1 0\n",
            "Yo a=slow: 0.5(1 + 2) +0.5(1 + 1) =2.5\n",
            "\n",
            "a=fast: 1(-10 +0) =-10\n",
            "\n",
            "Vatils) — max) /T(s, a, s’) [RCs, a,s’) + 1Vels’)]\n",
            "\n",
            "s; Caption: The image is a slide from a presentation titled \"Example: Value Iteration\". It includes a table with three rows labeled V0, V1, and V2, and three columns labeled ?, ?, and 0. There are also three cars depicted with different statuses: a blue car labeled \"Cool\", a red car labeled \"Overheated\", and a grey car with a question mark. The slide contains mathematical expressions and calculations related to value iteration in reinforcement learning.\n",
            "Caption for /content/drive/MyDrive/pages/page_347.png-- OCR: Example: Value Iteration\n",
            "\n",
            "2 & &\n",
            "% | 35 25 0\n",
            "\n",
            " \n",
            "\n",
            "Assume no discount!\n",
            "\n",
            "Vatils) — max) /T(s, a, s’) [RCs, a,s’) + 1Vels’)]\n",
            "\n",
            "s; Caption: The image shows a diagram of value iteration with three states and their corresponding values. The states are represented by cars in different conditions: cool, warm, and overheated. The values are updated based on the actions taken (slow, fast, warm) and the resulting rewards and transitions.\n",
            "Caption for /content/drive/MyDrive/pages/page_348.png-- OCR: k=0 (initialize)\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "VALUES AFTER O ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with values after 0 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_349.png-- OCR: k= 1 (set rewards)\n",
            "\n",
            " \n",
            "\n",
            "VALUES AFTER 1 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid with numerical values and color-coded cells.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_350.png-- OCR: =1\n",
            "\n",
            "0.9(0.8*0+0.1*0+0.1*1)=\n",
            "1.00 0.09\n",
            "a=l:\n",
            "\n",
            "0.9(0.8*0+0.1*1+0.1*0)=\n",
            "0.09\n",
            "\n",
            " \n",
            "\n",
            "a=:\n",
            "\n",
            "0.9(0.8*0+0.1*0+0.1*0)=\n",
            "\n",
            "0\n",
            "\n",
            "a=>:\n",
            "0.9(0.8*1+0.1*0+0.1*0)=\n",
            "0.72\n",
            "\n",
            "VALUES AFTER 1 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with a 3x3 grid, where each cell contains a numerical value. The top left cell has a value of 0.00, the top middle cell has a value of 0.00, and the top right cell has a value of 1.00. The middle left cell has a value of 0.00, the middle cell has a value of -1.00, and the middle right cell has a value of 0.00. The bottom left cell has a value of 0.00, the bottom middle cell has a value of 0.00, and the bottom right cell has a value of 0.00. The image also includes a formula for calculating the average value (a) of the cells, with the values of a=↑, a=↓, a=←, and a=→. The formula for a=↑ is 0.9(0.8*0+0.1*0+0.1*1)=0.09, the formula for a=↓ is 0.9(0.8*0+0.1*1+0.1*0)=0.09, the formula for a=← is 0.9(0.8*1+0.1*0+0.1*0)=0.72, and the formula for a=→ is 0.9(0.8*1+0.1*0+0.1*0)=0.72. The image also includes the values of noise, discount, and living reward.\n",
            "Caption for /content/drive/MyDrive/pages/page_351.png-- OCR: VALUES AFTER 2 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with a value table after 2 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_352.png-- OCR: k=2\n",
            "\n",
            "VALUES AFTER 2 ITERATIONS\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ast:\n",
            "0.9(0.8*0.72+0.1*0+0.1*-1)=\n",
            "0.43\n",
            "\n",
            "a=l:\n",
            "0.9(0.8*0+0.1*-1+0.1*0)=\n",
            "-0.09\n",
            "\n",
            "0.9(0.8*0+0.1*0+0.1*0.72)=\n",
            "0.06\n",
            "\n",
            "a=:\n",
            "0.9(0.8*-1+0.1*0.72+0.1*0)=\n",
            "-0.66\n",
            "\n",
            "Noise = 0.2\n",
            "Discount = 0.9\n",
            "Living reward =0; Caption: The image displays a gridworld display with a value table after 2 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_353.png-- OCR: VALUES AFTER 3 ITERATIONS Noise =0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with a value table after 3 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_354.png-- OCR: VALUES AFTER 4 ITERATIONS Noise =0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid with numerical values in each cell, indicating the results after 4 iterations of a process. The grid is color-coded with green, black, and red cells, each representing different values. The top left cell has a value of 0.37, the top middle cell has 0.66, the top right cell has 0.83, and the bottom right cell has 1.00. The bottom left cell has a value of 0.00, the middle left cell has 0.51, and the middle right cell has -1.00. The bottom middle cell has a value of 0.31, and the bottom right cell has 0.00. The red cell in the middle right corner indicates a negative value. The slide also includes text indicating that this is 'k=4', and there are additional parameters listed: Noise = 0.2, Discount = 0.9, and Living reward = 0.\n",
            "Caption for /content/drive/MyDrive/pages/page_355.png-- OCR: VALUES AFTER 5 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with numerical values in each cell, indicating the state values after 5 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_356.png-- OCR: k=\n",
            "\n",
            " \n",
            "\n",
            "VALUES AFTER 6 ITERATIONS Noise =0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid of values after 6 iterations, with a total of 9 cells. The top left cell has a value of 0.59, the top middle cell has a value of 0.73, the top right cell has a value of 0.85, and the bottom right cell has a value of 1.00. The bottom left cell has a value of 0.41, the bottom middle cell has a value of 0.57, and the bottom right cell has a value of 0.19. The center cell is gray and has a value of 0.21. The bottom middle cell has a value of 0.31, and the bottom right cell has a value of 0.43. The bottom left cell has a value of 0.21, and the bottom middle cell has a value of 0.31. The bottom right cell has a value of 0.19. The top right cell has a value of 1.00, and the bottom right cell has a value of 0.19. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of 0.57. The top right cell has a value of 0.85, and the bottom right cell has a value of 0.19. The top left cell has a value of 0.59, and the bottom left cell has a value of 0.41. The top middle cell has a value of 0.73, and the bottom middle cell has a value of \n",
            "Caption for /content/drive/MyDrive/pages/page_357.png-- OCR: VALUES AFTER 7 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid with numerical values in each cell, indicating the values after 7 iterations. The grid is color-coded with green, grey, and red cells.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_358.png-- OCR: VALUES AFTER 8 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid of values after 8 iterations, with a total of 9 cells. The top left cell has a value of 0.63, the top middle cell has a value of 0.74, the top right cell has a value of 0.85, and the bottom right cell has a value of 1.00. The bottom left cell has a value of 0.53, the bottom middle cell has a value of 0.57, and the bottom right cell has a value of 0.26. The center cell is grey and has a value of 0.39. The bottom middle cell has a value of 0.46. The top right cell has a red border and a value of -1.00. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top left cell has a green border and a value of 0.53. The top middle cell has a green border and a value of 0.74. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a green border and a value of 0.26. The top right cell has a green border and a value of 0.85. The bottom left cell has a green border and a value of 0.42. The bottom middle cell has a green border and a value of 0.39. The bottom right cell has a\n",
            "Caption for /content/drive/MyDrive/pages/page_359.png-- OCR: VALUES AFTER 9 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a gridworld display with numerical values in each cell, indicating the state values after 9 iterations.\n",
            "Caption for /content/drive/MyDrive/pages/page_360.png-- OCR: k=10\n",
            "\n",
            " \n",
            "\n",
            "VALUES AFTER 10 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid with numerical values in each cell, indicating some form of data or results after 10 iterations. The grid is color-coded with green, grey, and red cells, and there are arrows pointing to some of the cells.\n",
            "Caption for /content/drive/MyDrive/pages/page_361.png-- OCR: k=11\n",
            "\n",
            " \n",
            "\n",
            "VALUES AFTER 11 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid of numerical values, with some cells highlighted in green and one cell in red.\n",
            "Caption for /content/drive/MyDrive/pages/page_362.png-- OCR: VALUES AFTER 12 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid with numerical values in each cell, indicating the results after 12 iterations of a process. The grid is color-coded, with most cells in green, one in red, and one in gray. The red cell contains a negative value, while the gray cell has a value of zero. The top of the image has a title 'k=12' and the bottom provides additional information about the noise, discount, and living reward values used in the iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_363.png-- OCR: VALUES AFTER 100 ITERATIONS Noise = 0.2\n",
            "Discount = 0.9\n",
            "\n",
            "Living reward =0; Caption: The image displays a grid of values after 100 iterations, with a total of 9 squares. The top left square has a value of 0.64, the top middle has 0.74, the top right has 0.85, and the bottom right has 1.00. The bottom left square has a value of 0.57, the middle left has 0.57, the middle right has 0.43, and the bottom middle has 0.48. The bottom right square has a value of 0.28. The middle square is highlighted in red with a value of -1.00. The top right square is highlighted in green with a value of 1.00. The bottom left square is highlighted in green with a value of 0.49, and the bottom middle square is highlighted in green with a value of 0.43. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0.57, and the middle left square is highlighted in green with a value of 0.57. The middle square is highlighted in green with a value of 0.57, and the bottom middle square is highlighted in green with a value of 0.48. The bottom right square is highlighted in green with a value of 0.28. The top left square is highlighted in green with a value of 0.64, and the top middle square is highlighted in green with a value of 0.74. The top right square is highlighted in green with a value of 0.85, and the bottom right square is highlighted in green with a value of 0.28. The bottom left square is highlighted in green with a value of 0\n",
            "Caption for /content/drive/MyDrive/pages/page_364.png-- OCR: Policy Methods; Caption: The image displays two cartoon robots holding papers with various symbols and arrows on them. The robot on the left has a green eye and is smiling, while the robot on the right has a more complex design with a helmet and is holding a paper with a grid of symbols and arrows. The title 'Policy Methods' is written at the top of the image.\n",
            "Caption for /content/drive/MyDrive/pages/page_365.png-- OCR: Policy Evaluation\n",
            "\n",
            "2:; Caption: The image depicts a cartoon character resembling a computer server with a magnifying glass, examining a document.\n",
            "Caption for /content/drive/MyDrive/pages/page_366.png-- OCR: Fixed Policies\n",
            "\n",
            " \n",
            "\n",
            "Do the optimal action Do what 7 says to do\n",
            "\n",
            " \n",
            "\n",
            "“SAS\n",
            "\n",
            " \n",
            "\n",
            "As\n",
            "\n",
            "m Expectimax trees max over all actions to compute the optimal values\n",
            "\n",
            "m If we fixed some policy 7(s), then the tree would be simpler — only one action per state\n",
            ".. though the tree’s value would depend on which policy we fixed; Caption: The image is a slide titled 'Fixed Policies' with two diagrams comparing 'Do the optimal action' and 'Do what π says to do'. The diagrams illustrate decision-making processes in a state-action space, with arrows indicating possible actions and states. The slide text explains that expectimax trees are used to compute optimal values and that fixing a policy simplifies the tree but the value depends on the fixed policy.\n",
            "Caption for /content/drive/MyDrive/pages/page_367.png-- OCR: Utilities for a Fixed Policy\n",
            "\n",
            " \n",
            "\n",
            "m Define the utility of astate s, under a fixed policy z:\n",
            "V\"(s) = expected total discounted rewards starting in s and following zz\n",
            "\n",
            "mw VWhatis the recursive relation?\n",
            "\n",
            " \n",
            "\n",
            "Vi(s) = max) T(s, a, s') [R(s, a, s') +7V*(s')|; Caption: Slide 17: Utilities for a Fixed Policy\n",
            "Caption for /content/drive/MyDrive/pages/page_368.png-- OCR: Utilities for a Fixed Policy\n",
            "\n",
            " \n",
            "\n",
            "m Define the utility of astate s, under a fixed policy z:\n",
            "V\"(s) = expected total discounted rewards starting in s and following zz\n",
            "\n",
            "mw VWhatis the recursive relation?\n",
            "\n",
            " \n",
            "\n",
            "Vi(s) = max) T(s, a, s') [R(s, a, s') +7V*(s')|\n",
            "\n",
            "mw Answer:\n",
            "\n",
            "V\"(s) = S>T(s, 7(s), s')[R(s, (s), 8’) + yV7(s')]; Caption: Slide 17: Utilities for a Fixed Policy\n",
            "Caption for /content/drive/MyDrive/pages/page_369.png-- OCR: Policy Evaluation\n",
            "\n",
            " \n",
            "\n",
            "m How dowe calculate the V's for a fixed policy 7?\n",
            "\n",
            "m Idea 1: Tum recursive equations into updates (like value iteration) 1(S)\n",
            "\n",
            "  \n",
            "\n",
            "S, 7(S)\n",
            "Vo (s) = 0\n",
            "\n",
            "Veii(s) — 27(s,1(s), 5\") [R(s, (5), 8\") £yVE(s)] -S7US)S\n",
            "\n",
            "m Idea 2: Without the maxes, the equations are just alinear system\n",
            "- Solve with your favorite linear system solver\n",
            "\n",
            " \n",
            "\n",
            "V\"(s1)\n",
            "X| V*(s2) | =; Caption: Slide titled \"Policy Evaluation\" with text and diagrams.\n",
            "Caption for /content/drive/MyDrive/pages/page_370.png-- OCR: Example: Policy Evaluation\n",
            "\n",
            "Always Go Right Always Go Forward; Caption: The image shows two examples of policy evaluation in a game-like scenario. On the left, there is a policy labeled 'Always Go Right' and on the right, a policy labeled 'Always Go Forward'. Both policies are evaluated in a game environment with a fire hazard, and the outcomes are depicted below each policy.\n",
            "Caption for /content/drive/MyDrive/pages/page_371.png-- OCR: Example: Policy Evaluation\n",
            "\n",
            "Always Go Right Always Go Forward; Caption: The image displays two sets of colored boxes with numerical values, representing different policy evaluation strategies.\n",
            "Caption for /content/drive/MyDrive/pages/page_372.png-- OCR: Policy Extraction; Caption: The image depicts a process of 'Policy Extraction' where a robot is analyzing a grid with various symbols and arrows, and extracting a policy from it.\n",
            "Caption for /content/drive/MyDrive/pages/page_373.png-- OCR: Computing Actions from Values\n",
            "\n",
            "m Let's imagine we have the optimal values V*(s)\n",
            "ag It's not obvious!\n",
            "— |\n",
            "m Ve need to do a mini-expectimax (one step) =\n",
            "\n",
            "ex:\n",
            "n*(s) = argmax)_T(s,a,s’)[R(s,a, 8’) + yV*(s/)] max (0.5, 1.7, 1.21= 1.7\n",
            "a\n",
            "sg!\n",
            "\n",
            " \n",
            "\n",
            "argmax [0.5, 1.7, 1.2] =1\n",
            "\n",
            "m Thisis called policy extraction, since it gets the policy implied by the values; Caption: The image is a slide from a presentation titled 'Computing Actions from Values'. It contains text and a grid of colored squares with numerical values.\n",
            "Caption for /content/drive/MyDrive/pages/page_374.png-- OCR: Computing Actions from Q-Values\n",
            "m Let's imagine we have the optimal q-values:\n",
            "\n",
            "m How should we act?\n",
            "= Completely trivial to decide!\n",
            "\n",
            "m*(s) = argmaxQ*(s,a)\n",
            "\n",
            " \n",
            "\n",
            "= Important lesson: actions are easier to select from q-values than values!; Caption: The image displays a slide titled 'Computing Actions from Q-Values' with a grid of numbers representing q-values in different colors.\n",
            "Caption for /content/drive/MyDrive/pages/page_375.png-- OCR: Policy Iteration; Caption: The image depicts a stylized illustration of a hand holding a red pen, writing on a large paper with a grid of squares. Each square contains a different symbol, such as a skull, a diamond, and arrows pointing in various directions. There is also a small bottle labeled 'White Out' next to the hand. The title 'Policy Iteration' is displayed at the top of the image.\n",
            "Caption for /content/drive/MyDrive/pages/page_376.png-- OCR: Policy Iteration\n",
            "\n",
            " \n",
            "\n",
            "m Evaluation: For fixed current policy z,find values with policy evaluation:\n",
            "- |terate until values converge:\n",
            "\n",
            "Vitis) — OT (s, mils), 8’) [R(s, mils), 8) 4 V8.)\n",
            "\n",
            "- Endup with value function V7\n",
            "= Improvement: For fixed values, get a better policy using policy extraction\n",
            "- QOne-step look-ahead:\n",
            "\n",
            "m41(s) = argmax ) > T(s, as!) [R(s,a, 8!) +7V™(s')]\n",
            "\n",
            "m Repeat steps until policy converges; Caption: The image is a slide titled 'Policy Iteration' with text and mathematical equations.\n",
            "Caption for /content/drive/MyDrive/pages/page_377.png-- OCR: Comparison\n",
            "\n",
            " \n",
            "\n",
            "= Both value iteration and policy iteration compute the same thing (all optimal values)\n",
            "\n",
            "m In value iteration:\n",
            "m Every iteration updates both the values and (implicitly) the policy\n",
            "a We don't track the policy, but taking the max over actions implicitly recomputes it\n",
            "\n",
            "m In policy iteration:\n",
            "\n",
            "m We do several passes that update utilities with fixed policy (each pass is fast because we\n",
            "consider only one action, not all of them)\n",
            "\n",
            "u After the policy is evaluated, a new policy is chosen (slow like a value iteration pass)\n",
            "m= Thenew policy will be better (or we’re done)\n",
            "\n",
            "m Both are dynamic programs for solving MDPs; Caption: Comparison\n",
            "Caption for /content/drive/MyDrive/pages/page_378.png-- OCR: Summary: MDP Algorithms\n",
            "\n",
            " \n",
            "\n",
            "m Soyou want to....\n",
            "= Compute optimal values: use value iteration or policy iteration\n",
            "\n",
            "Value Iteration or Policy Iteration (v\")\n",
            "= Compute values for a particular policy: use policy evaluation\n",
            "Policy Evaluation\n",
            "\n",
            "a Turn your values into a policy: use policy extraction (one-step lookahead)\n",
            "\n",
            "(v) Policy Extraction @; Caption: Summary: MDP Algorithms\n",
            "Caption for /content/drive/MyDrive/pages/page_379.png-- OCR: Reinforcement Learning; Caption: The image depicts a cartoon robot with a confused expression, surrounded by a green diamond and a purple lightning bolt, with a question mark above its head.\n",
            "Caption for /content/drive/MyDrive/pages/page_380.png-- OCR: nature\n",
            "\n",
            "Article | Published: 30 October 2019\n",
            "\n",
            "Grandmaster level in StarCraft II using multi-agent\n",
            "reinforcement learning\n",
            "\n",
            "Actions limit ~22 per 5s ff Peawosies delay ~200 ms\n",
            "+ Ean ©\n",
            "a\n",
            "\n",
            " \n",
            "\n",
            "processing delay 30 ms\n",
            "\n",
            "ing delay 80 ms; Caption: The image is a screenshot from a computer game, specifically StarCraft II, showing a battlefield with various units and structures. There is also a graphical representation of a reinforcement learning process with a flowchart and a clock indicating processing delay.\n",
            "Caption for /content/drive/MyDrive/pages/page_381.png-- OCR: ; Caption: The image displays a stylized representation of a soccer field with two vertical white lines on each side, a dashed line down the middle, and two rectangular shapes at the top corners. There is also a circular shape with the word 'nature' inside it in the top right corner.\n",
            "Caption for /content/drive/MyDrive/pages/page_382.png-- OCR: A Walk in the Park: Learning to Walk in 20 Minutes\n",
            "With Model-Free Reinforcement Learning\n",
            "\n",
            "Laura Smith*!, Ilya Kostrikov*!, Sergey Levine!\n",
            "“Equal contribution ‘Berkeley AI Research, UC Berkeley\n",
            "{smithlaura, kostrikov}@berkeley.edu, svlevine@eecs.berkeley.edu\n",
            "\n",
            "a” Ai ASL or A 1,\n",
            "28 yA Dy) ‘s w\n",
            "\n",
            "     \n",
            "\n",
            "peti?\n",
            "\n",
            "16 Aug 2022\n",
            "\n",
            "https://arxiv.org/pdf/2208.07860.pdf; Caption: The image displays a title 'A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning' followed by the authors' names and affiliations. There are three circular images with pictures of a robot walking in different environments. Below the images, there is a date '16 Aug 2022' and a URL link to a PDF document.\n",
            "Caption for /content/drive/MyDrive/pages/page_383.png-- OCR: ; Caption: The image depicts a scene in a natural setting, likely a park or a forested area. There is a dirt path or road that runs through the scene, and it appears to be a sunny day given the shadows cast by the trees. On the right side of the path, there is a person sitting on the ground, possibly resting or observing the surroundings. This person is wearing a blue jacket and jeans. In the foreground, there is a black object that looks like a toy or a piece of equipment, possibly a remote-controlled vehicle or a toy car. The background is filled with trees and greenery, indicating a wooded area. The overall atmosphere seems calm and serene, with no visible signs of activity or movement apart from the person sitting on the ground.\n",
            "Caption for /content/drive/MyDrive/pages/page_384.png-- OCR: Reinforcement Learning: Overview\n",
            "\n",
            " \n",
            "\n",
            "u Passive Reinforcement Leaming: how to learn from already given experiences\n",
            "= Model-based: leam the MDP model from experiences, then solve the MDP\n",
            "= Model-free: forego learning the MDP model, directly learn V or Q\n",
            "= Value learning: learns value of a fixed policy\n",
            "m 2 approaches: Direct Evaluation & TD Learning\n",
            "= Qlearning: learns Q values of the optimal policy (Q version of TD Learning)\n",
            "\n",
            "= Active Reinforcement Leaming: how to collect new experiences\n",
            "\n",
            "= Approximate Reinforcement Leaming: to handle large state spaces; Caption: The image is a slide from a presentation titled 'Reinforcement Learning: Overview'. It contains text that explains different types of reinforcement learning. The slide is divided into three main sections: Passive Reinforcement Learning, Active Reinforcement Learning, and Approximate Reinforcement Learning. Each section has bullet points explaining the concepts. The text is in black with blue headings, and there are no images or other visual elements present.\n",
            "Caption for /content/drive/MyDrive/pages/page_385.png-- OCR: Reinforcement Learning\n",
            "\n",
            " \n",
            "\n",
            "m Still assume a Markov decision process (MDP):\n",
            "\n",
            "= Asetof statessES 0H\n",
            "= Aset of actions (per state) A ex oN\n",
            "\n",
            "a Amodel T(s,a,s’)\n",
            "\n",
            "= Areward function R(s,a,s’) > EA,\n",
            "# Still looking for a policy m(s) oN . Overheted; Caption: Reinforcement Learning\n",
            "Caption for /content/drive/MyDrive/pages/page_386.png-- OCR: Reinforcement Learning\n",
            "\n",
            " \n",
            "\n",
            "m Still assume a Markov decision process (MDP):\n",
            "\n",
            "gw Still looking for a policy z(s)\n",
            "\n",
            "w AsetofstatessES\n",
            "= Aset of actions (per state) A E&\n",
            "\n",
            "a Amodel T(s,a,s’) Warm wae\n",
            "a Areward function R(s,a,s’) GB EL,\n",
            "\n",
            "# New twist: don’t know Tor R\n",
            "\n",
            "g i.e. we don’t know which states are good or what the actions do\n",
            "= Must actually try out actions and states to learn; Caption: The image is a slide from a presentation on Reinforcement Learning. It includes text and illustrations. The text explains the concept of a Markov decision process (MDP) and introduces a new twist where the reward function R is unknown. The illustrations depict a car in three states: cool, warm, and overheated, with corresponding actions.\n",
            "Caption for /content/drive/MyDrive/pages/page_387.png-- OCR: Reinforcement Learning\n",
            "\n",
            " \n",
            "\n",
            "m Still assume a Markov decision process (MDP):\n",
            "\n",
            "gw Still looking for a policy z(s)\n",
            "\n",
            "# New twist: don’t know Tor R\n",
            "\n",
            "w Asetof statessES\n",
            "\n",
            "= Aset of actions (per state) A\n",
            "a Amodel T(s,a,s’)\n",
            "\n",
            "a Areward function R(s,a,s’) Reward: r\n",
            "\n",
            "g i.e. we don’t know which states are good or what the actions do\n",
            "= Must actually try out actions and states to learn; Caption: Reinforcement Learning\n",
            "Caption for /content/drive/MyDrive/pages/page_388.png-- OCR: Offline (MDPs) vs. Online (RL)\n",
            "\n",
            " \n",
            "\n",
            "Offline Solution: Online Learning:\n",
            "Compute policy ahead Compute policy as\n",
            "of time experience comes in; Caption: The image depicts a comparison between offline (MDPs) and online (RL) learning methods. On the left side, there is a character wearing a gas mask and a hard hat, standing on a platform with flames underneath, representing the offline solution where the policy is computed ahead of time. On the right side, there is a similar character, but this time it is falling off the platform with flames, representing the online learning method where the policy is computed as experience comes in.\n",
            "Caption for /content/drive/MyDrive/pages/page_389.png-- OCR: Passive Reinforcement Learning\n",
            "\n",
            " \n",
            "\n",
            "= Simplified task: policy evaluation\n",
            "m Input: a fixed policy z(s)\n",
            "m You don't know the transitions T(s,a,s’)\n",
            "m You don't know the rewards R(s,a,s’)\n",
            "= Goal: learn the state values\n",
            "\n",
            "g In this case:\n",
            "m Leamer is “along for the ride”\n",
            "m No choice about what actions to take\n",
            "m Just execute the policy and learn from experience\n",
            "m Thisis NOT offline planning! You actually take actions in the world.; Caption: The image depicts a hand holding a paper with a grid of squares, each containing arrows pointing in different directions. Some squares have a skull symbol, and others have a diamond symbol. The arrows are green, red, and blue, indicating different directions or actions.\n",
            "Caption for /content/drive/MyDrive/pages/page_390.png-- OCR: Model-Based Learning; Caption: The image shows two cartoon robots sitting at a table. The robot on the left is wearing a helmet and is holding a purple object, possibly a book or a tablet. The robot on the right is without a helmet and is holding a purple pen or stylus. Both robots have a friendly and curious expression on their faces. The background is plain white, and the title 'Model-Based Learning' is displayed at the top of the image.\n",
            "Caption for /content/drive/MyDrive/pages/page_391.png-- OCR: Model-Based Learning\n",
            "\n",
            " \n",
            "\n",
            "m Model-Based Idea:\n",
            "m Leaman approximate model based on experiences\n",
            "m Solve for values as if the learned model were correct\n",
            "\n",
            "m Step 1: Learn empirical MDP model\n",
            "= Count outcomes s’ for each s, a\n",
            "a Normalize to give an estimate of T'(s, a, s’)\n",
            "m Discover each /2(s, a, s’) when we experience (s, a, s’)\n",
            "\n",
            " \n",
            "\n",
            "m Step 2: Solve the learned MDP\n",
            "\n",
            "= For example, use value iteration, as before; Caption: The image shows a slide titled 'Model-Based Learning' with a list of steps and a cartoon illustration of a character with headphones and a book.\n",
            "Caption for /content/drive/MyDrive/pages/page_392.png-- OCR: Example: Model-Based Learning\n",
            "\n",
            " \n",
            "\n",
            "Input Policy zc Observed (Ss, a, s’, R) Transitions Leamed Model\n",
            "Episode 1 Episode 2 T(s, a, 8’)\n",
            "B, east, C, -1 B, east, C, -1 T(B, east, C)= 1.00\n",
            "\n",
            "C, east, D, -1 C, east, D, -1 re east, D) 0.75\n",
            "D, exit, x, +10 D, exit, x, +10 (C, north, A) = 0.\n",
            "\n",
            " \n",
            "\n",
            "Episode 3 Episode 4 R(s, a, s’)\n",
            "\n",
            "E, north, C,-1 E, north, C,-1 Re east oe\n",
            "- east, D) =-\n",
            "C,east, D,-1 C, north, A, -1 ROD. exit, x) = +10\n",
            "\n",
            "Assume: y =1 D, exit, x, +10 A, exit, x, -10; Caption: This image is a slide from a presentation titled 'Example: Model-Based Learning'. It is divided into three main sections: 'Input Policy π', 'Observed (s, a, s', R) Transitions', and 'Learned Model'. The 'Input Policy π' section shows a grid with four squares labeled A, B, C, and D, with arrows indicating movement directions. The 'Observed (s, a, s', R) Transitions' section lists four episodes of observed transitions with states, actions, next states, and rewards. The 'Learned Model' section presents two models, 'T' and 'R', with transition and reward functions for different states and actions. The slide also includes a note at the bottom stating 'Assume: γ = 1'.\n",
            "Caption for /content/drive/MyDrive/pages/page_393.png-- OCR: Analogy: Expected Age\n",
            "\n",
            " \n",
            "\n",
            "Goal: Compute expected age of current students in NTU CSIE\n",
            "\n",
            "SoP(a)-a  -=0.385 x 204...\n",
            "\n",
            " \n",
            "\n",
            " \n",
            " \n",
            "   \n",
            " \n",
            " \n",
            "\n",
            "   \n",
            " \n",
            "  \n",
            "  \n",
            "\n",
            " \n",
            "\n",
            "Why does this (a) = Why does this\n",
            "? ?\n",
            "\n",
            "work? Because N EIA] © i Soa work? Because\n",
            "\n",
            "eventually you N& samples appear\n",
            "\n",
            "learn the right x ‘ with the right\n",
            "\n",
            "model. frequencies.; Caption: The image is a slide titled 'Analogy: Expected Age' with the goal to compute the expected age of current students in NTU CSIB. It presents a mathematical equation for expected age with known probability distribution P(A), and then discusses two scenarios: 'Unknown P(A): Model Based' and 'Unknown P(A): Model Free'. Each scenario has a corresponding equation and a question 'Why does this work?' with an explanation. The slide uses a combination of text, equations, and colored boxes to convey the information.\n",
            "Caption for /content/drive/MyDrive/pages/page_394.png-- OCR: Model-Free Learning; Caption: The image depicts a robot with a thought bubble containing coins, next to a slot machine with the text 'DOUGLE $2 $0 OR NOTHING'. The title 'Model-Free Learning' is displayed at the top.\n",
            "Caption for /content/drive/MyDrive/pages/page_395.png-- OCR: Direct Evaluation\n",
            "\n",
            " \n",
            "\n",
            "= Goal: Compute values for each state under zr\n",
            "\n",
            "m Idea: Average together observed sample values\n",
            "w Act according to 7\n",
            "\n",
            "mw Every time you visit a state, write down what the\n",
            "sum of discounted rewards turned out to be from\n",
            "that state until the end of the episode (called\n",
            "the expected reward-to-go):\n",
            "\n",
            "samplei(s) = R(s) +yR(s')+y?R(s_)+ «.\n",
            "\n",
            "DOUBLE\n",
            "\n",
            " \n",
            "\n",
            "m Average those samples: V(s) =- ~) sample;(s)\n",
            "\n",
            "# This is called direct or Monte-Cano evaluation; Caption: The image displays a slot machine with a sign that reads 'DOUBLE $2 OR NOTHING'.\n",
            "Caption for /content/drive/MyDrive/pages/page_396.png-- OCR: Example: Direct Evaluation\n",
            "\n",
            "Observed (s, a, $s’, R) Transitions Output Values\n",
            "\n",
            "Episode 1 Episode 2\n",
            "\n",
            "B, east, C, -1\n",
            "C, east, D, -1\n",
            "\n",
            "Input Policy zr\n",
            "\n",
            "B, east, C, -1\n",
            "C, east, D, -1\n",
            "\n",
            "D, exit, x, +10 D, exit, x, +10\n",
            "\n",
            " \n",
            "\n",
            "Episode 3\n",
            "\n",
            "E, north, C, -1\n",
            "C, east, D, -1\n",
            "Assume: =1 D, exit, x, +10\n",
            "\n",
            "E, north, C, -1\n",
            "C,east, A, -1\n",
            "A, exit, x, -10; Caption: Slide shows an example of direct evaluation.\n",
            "Caption for /content/drive/MyDrive/pages/page_397.png-- OCR: Example: Direct Evaluation\n",
            "\n",
            "Input Policy zr Observed (S, a, Ss’, R) Transitions Output Values\n",
            "\n",
            "Episode 1 Episode 2\n",
            "\n",
            "B, east, C, -1 B, east, C, -1\n",
            "C, east, D, -1 C, east, D, -1\n",
            "D, exit, x, +10 D, exit, x, +10; Caption: Slide shows an example of direct evaluation.\n",
            "Caption for /content/drive/MyDrive/pages/page_398.png-- OCR: Example: Direct Evaluation\n",
            "\n",
            " \n",
            "\n",
            "Input Policy zr Observed (S, a, Ss’, R) Transitions Output Values\n",
            "\n",
            "Episode 1 Episode 2\n",
            "\n",
            "B, east, C, -1\n",
            "C, east, D, -1\n",
            "\n",
            "B, east, C, -1\n",
            "C, east, D, -1\n",
            "\n",
            "D, exit, x, +10 D, exit, x, +10\n",
            "\n",
            " \n",
            "\n",
            "Episode 3\n",
            "\n",
            "E, north, C, -1\n",
            "C, east, D, -1\n",
            "Assume: =1 D, exit, x, +10\n",
            "\n",
            " \n",
            "\n",
            "E, north, C, -1\n",
            "C,east, A, -1\n",
            "A, exit, x, -10\n",
            "\n",
            "  \n",
            "\n",
            "V(s) is sum of discounted rewards from s until the end, averaged over all encounters of s; Caption: Slide shows an example of direct evaluation.\n",
            "Caption for /content/drive/MyDrive/pages/page_399.png-- OCR: Problems with Direct Evaluation\n",
            "\n",
            " \n",
            "\n",
            "a What's good about direct evaluation? Output Values\n",
            "a It’s easy to understand\n",
            "m It doesn’t require any knowledge of T,R\n",
            "\n",
            "a It eventually computes the correct average values,\n",
            "using just sample transitions\n",
            "\n",
            "a What bad about it?\n",
            "\n",
            "a It wastes information about state connections\n",
            "\n",
            "= Need to have all episodes ahead of time (cannot\n",
            "“stream” in transitions)\n",
            "\n",
            " \n",
            "\n",
            "If Band Eboth go to C\n",
            "under this policy, how can\n",
            "their values be different?; Caption: Slide title: Problems with Direct Evaluation. Bullet points: What’s good about direct evaluation? It’s easy to understand, It doesn’t require any knowledge of T, R, It eventually computes the correct average values, using just sample transitions. What bad about it? It wastes information about state connections, Need to have all episodes ahead of time (cannot “stream” in transitions).\n",
            "Caption for /content/drive/MyDrive/pages/page_400.png-- OCR: Problems with Direct Evaluation\n",
            "\n",
            "Observed (s, a, s’, R) Transitions:\n",
            "\n",
            "Episode 1\n",
            "E, north, C,0\n",
            "C,east, D,0\n",
            "D, exit, x, +10\n",
            "\n",
            "Episode 2\n",
            "\n",
            "   \n",
            "\n",
            "Is Babad state?; Caption: The image is a slide titled 'Problems with Direct Evaluation'. It contains a grid with four labeled cells A, B, C, and D, each with a number: -10, 0, +10, and 0 respectively. There are also four labeled cells E, B, C, and D, each with a number: 0, 0, 0, and 0 respectively. The slide poses a question at the bottom: 'Is B a bad state?'\n",
            "Caption for /content/drive/MyDrive/pages/page_401.png-- OCR: Why Not Use Policy Evaluation?\n",
            "\n",
            " \n",
            "\n",
            "= Simplified updates calculate V for a fixed policy:\n",
            "m Each round, replace V with a one-step-look-ahead layer over V\n",
            "\n",
            "Vo (s) =0\n",
            "\n",
            "Vir (s) us m(s), s’)[R(s, 1(s), 8’) + VV\" (s')]\n",
            "\n",
            "m This approach fully exploited the connections between the states\n",
            "m Unfortunately, we need T and Rto do it!\n",
            "\n",
            "m Key question: how can we do this update to V without knowing T and R?\n",
            "= In other words, how to we take a weighted average without knowing the weights?; Caption: The image is a slide from a presentation titled 'Why Not Use Policy Evaluation?'. It contains text and mathematical equations related to policy evaluation in the context of reinforcement learning.\n",
            "Caption for /content/drive/MyDrive/pages/page_402.png-- OCR: Sample-Based Policy Evaluation?\n",
            "\n",
            "mw We want to improve our estimate of V by computing these averages:\n",
            "Vizi(s) — SOT (s,7(s), 8) [R(s, 7(s), 8!) + VE (s')]\n",
            "\n",
            "s' However, T is not available!\n",
            "\n",
            "m Idea: Take samples of outcomes s’ (by doing the action!) and average\n",
            "\n",
            "sample, = R(s,7(s), 84) + Vii (84)\n",
            "Known P(A):\n",
            "soning = = R(s,7(s), 82) + We (sd) B[A] = 57 P(a)-a\n",
            "\n",
            "samplen = = R(s,7(s), sp) + VE (si)\n",
            "\n",
            "Unknown P(A): “Model Free”\n",
            "\n",
            "De\n",
            "\n",
            " \n",
            "\n",
            "1\n",
            "Via (s) — 7 S° sample;\n",
            "a; Caption: Sample-Based Policy Evaluation?\n",
            "Caption for /content/drive/MyDrive/pages/page_403.png-- OCR: Sample-Based Policy Evaluation?\n",
            "mw We want to improve our estimate of V by computing these averages:\n",
            "Vii Cs) — x T(s,7(s),8')[R(s, 7(s), 8’) + Vi (s')]\n",
            "m Idea: Take samples of outcomes s’ (by doing the action!) and average\n",
            "\n",
            "sample, = R(s,7(s), 84) + Vir (84)\n",
            "samplea = = R(s,7(s), 85) + We (85)\n",
            "\n",
            "samplen = = R(s,7(s), sp) + VE (si)\n",
            "\n",
            "  \n",
            "\n",
            "1\n",
            "Ve +) le;\n",
            "44+1(8) = sample;\n",
            "Unfortunately, “n samples” here are not always available!; Caption: Slide with text and a cartoon image.\n",
            "Caption for /content/drive/MyDrive/pages/page_404.png-- OCR: Temporal Difference Learning\n",
            "\n",
            " \n",
            "\n",
            "m Big idea: learn from every experience!\n",
            "m Update V(s) each time we experience a transition (s, a, s’, r)\n",
            "u Likely outcomes s’ will contribute updates more often\n",
            "\n",
            "= Temporal difference learning of values\n",
            "= Policy still fixed, still doing evaluation!\n",
            "= Move values toward value of whatever successor occurs: running average\n",
            "Sample of V(s): sample = R(s,7(s), 8’) + yV™(s’)\n",
            "Update to V(s): V\"(s) + (1 -—a)V“%(s) + (a) sample O<a<1\n",
            "\n",
            "Same update: V\"™(s) © V“(s) + a(sample — V\"(s)); Caption: The image is a slide titled 'Temporal Difference Learning'. It contains text explaining the concept of learning from every experience, updating values each time a transition occurs, and the idea of temporal difference learning of values. The slide also includes mathematical expressions related to the updating of values in a learning process.\n",
            "Caption for /content/drive/MyDrive/pages/page_405.png-- OCR: Exponential Moving Average\n",
            "\n",
            " \n",
            "\n",
            "m Traditional Average: ave(x) =*¥n xn\n",
            "\n",
            "= Need to have all N samples at once (cannot “stream” in samples)\n",
            "\n",
            "m Exponential moving average\n",
            "= Therunning interpolation update: 7, = (1 —a@)-Fn_1 + Q- Ln 0<a<1\n",
            "\n",
            "m Makes recent samples more important:\n",
            "\n",
            "In +(1—a@)+an_1 4+ (l—a@)? -ayn_o +...\n",
            "1+ (l1—a)+(1-a)?+...\n",
            "= Forgets about the past samples (how quickly depends on a)\n",
            "\n",
            " \n",
            "\n",
            "Ln =; Caption: The image is a slide from a presentation titled 'Exponential Moving Average'. It contains text and mathematical formulas explaining the concept of Exponential Moving Average (EMA) in comparison to the Traditional Average. The slide has a blue header with the title, followed by bullet points and mathematical equations. The equations are presented in a clear, black font on a white background.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_406.png-- OCR: Example: Temporal Difference Learning\n",
            "\n",
            " \n",
            "\n",
            "States Observed Transitions\n",
            "B, east, [ Beast, C,-2 | -2 [C.east, D,-2 | east, D, -2\n",
            "\n",
            "i oT oan oon\n",
            "\n",
            "Assume: y = 1, a= 1/2 V7(s) — (1—a)V\"(s) +0 [R(s, 7(s), 8’) + V7(s')); Caption: The image is a slide from a presentation titled 'Example: Temporal Difference Learning'. It is divided into two main sections: 'States' and 'Observed Transitions'. The 'States' section shows a grid with five squares labeled A, B, C, D, and E. The 'Observed Transitions' section shows three smaller grids, each representing a transition from one state to another with an associated reward. The transitions are from B to C, C to D, and C to D, with rewards of -2, -2, and -1 respectively. There are red dots indicating the current state in the transition grids. At the bottom of the slide, there is an equation that appears to be related to the temporal difference learning algorithm, with variables and parameters such as γ (gamma), α (alpha), and V^π(s) (value function under policy π at state s).\n",
            "Caption for /content/drive/MyDrive/pages/page_407.png-- OCR: Problems with TD Value Learning\n",
            "\n",
            "m ID value leaning is a model-free way to do policy evaluation\n",
            "\n",
            "= However, if we want to turn values into a (new) policy, we’re stuck:\n",
            "m(s) = arg max Q(s, a)\n",
            "Q(s,a) = S°T(s, a, 8’) [R(s, a, s')+ 7V(s')|\n",
            "\n",
            "mg What can we do?\n",
            "\n",
            "m= Leam Q-values, not values\n",
            "u Makes action selection model-free too!; Caption: The image is a slide from a presentation titled 'Problems with TD Value Learning'. It contains text and mathematical equations. The text discusses the concept of TD value learning as a model-free way to do policy evaluation and highlights a problem when trying to turn values into a new policy. The slide poses a question about what can be done to address this issue and suggests learning Q-values instead of values to make action selection model-free. The slide also includes two equations related to the topic.\n",
            "Caption for /content/drive/MyDrive/pages/page_408.png-- OCR: Q-Learning\n",
            "\n",
            " \n",
            "\n",
            "m QLeaming: sample-based Q-value iteration\n",
            "\n",
            "Qrp4+1(s,a) — S°T(s, a, s') RCs, a,s')++¥ max Q;,(s’,a’)\n",
            "\n",
            " \n",
            "\n",
            "m Leam Q(s,a) values as you go\n",
            "m Receive a sample (s,a,s’,r)\n",
            "= Consider your old estimate: Q(s, a)\n",
            "m Consider your new sample estimate:\n",
            "\n",
            "no longer policy\n",
            "\n",
            "sample = R(s,a,s') + 7 max Q(s',a’) evaluation!\n",
            "\n",
            "u Incorporate the new estimate into a running average:\n",
            "\n",
            "Q(s,a) — (1 — a)Q(s,a) + (a) [sample]; Caption: Q-Learning\n",
            "Caption for /content/drive/MyDrive/pages/page_409.png-- OCR: The Story So Far: Reinforcement Learning\n",
            "\n",
            "g We still assume an MDP:\n",
            "= Asetof statesseES\n",
            "= Aset of actions (per state) A\n",
            "= Amodel T(s,a,$’)\n",
            "= Areward function R(s,a,s’)\n",
            "@ Still looking for a policy z(s)\n",
            "\n",
            " \n",
            "\n",
            "= New twist: don’t know Tor R, so must try out actions\n",
            "\n",
            "= Big idea: Compute all averages over T using sample outcomes; Caption: The image shows a slide from a presentation titled 'The Story So Far: Reinforcement Learning'. The slide contains text and an illustration of a purple fish with a snorkel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_410.png-- OCR: The Story So Far: MDPs and RL\n",
            "\n",
            " \n",
            "\n",
            "Known MDP: Offline Solution\n",
            "\n",
            "Goal Technique\n",
            "\n",
            "Compute V*, Q*, 7:* Value / policy iteration\n",
            "\n",
            "Evaluate a fixed policy 7c Policy evaluation\n",
            "\n",
            " \n",
            "\n",
            "Unknown MDP: Model-Based Unknown MDP: Model-Free\n",
            "Goal Technique Goal Technique\n",
            "\n",
            "Compute V\", Q*, 1\" VI/PI on approx. MDP Compute V\", Q®, 7° Qlearning\n",
            "\n",
            "Evaluate a fixed policy 7 FEon approx. MDP Evaluate a fixed policy 7 Value Learning; Caption: The image is a slide titled 'The Story So Far: MDPs and RL'. It is divided into two main sections. The top section is labeled 'Known MDP: Offline Solution' and contains two columns titled 'Goal' and 'Technique'. Under 'Goal', there are two bullet points: 'Compute V*, Q*, π*' and 'Evaluate a fixed policy π'. Under 'Technique', there are two bullet points: 'Value / policy iteration' and 'Policy evaluation'. The bottom section is divided into two parts, each labeled 'Unknown MDP'. The left part is labeled 'Unknown MDP: Model-Based' and the right part is labeled 'Unknown MDP: Model-Free'. Each part has two columns titled 'Goal' and 'Technique'. Under 'Goal', there are two bullet points: 'Compute V*, Q*, π*' and 'Evaluate a fixed policy π'. Under 'Technique', there are two bullet points: 'VI/PI on approx. MDP' and 'PE on approx. MDP' for the Model-Based section, and 'Q-learning' and 'Value Learning' for the Model-Free section.\n",
            "Caption for /content/drive/MyDrive/pages/page_411.png-- OCR: Active Reinforcement Learning\n",
            "\n",
            "  \n",
            "\n",
            "Acting according to policy 2*(s) = argmax Q(s,a)\n",
            "a; Caption: The image depicts a sequence of three panels showing a character, which appears to be a robot, falling into a pit of flames. The character is shown in different stages of falling, with the flames getting larger as it descends. The text 'Active Reinforcement Learning' is displayed at the top, and below the panels, there is a mathematical expression 'Acting according to policy π*(s) = argmax Q(s,a)'.\n",
            "Caption for /content/drive/MyDrive/pages/page_412.png-- OCR: Exploration vs. Exploitation; Caption: The image depicts a comparison between 'The Usual Place' and 'Grand Opening!' with a robot in between, suggesting a contrast between exploration and exploitation.\n",
            "Caption for /content/drive/MyDrive/pages/page_413.png-- OCR: How to Explore?\n",
            "\n",
            " \n",
            "\n",
            "m Several schemes for forcing exploration\n",
            "\n",
            "= Simplest: random actions (e-greedy)\n",
            "m Every time step, flip a coin\n",
            "= With (small) probability e, act randomly\n",
            "= With (large) probability 1-e, act on current policy\n",
            "\n",
            "= Problems with random actions?\n",
            "\n",
            "= You do eventually explore the space, but keep\n",
            "thrashing around once learning is done\n",
            "\n",
            "= One solution: lower < over time\n",
            "= Another solution: exploration functions; Caption: The image shows a slide from a presentation titled 'How to Explore?' It discusses several schemes for forcing exploration, specifically focusing on the simplest scheme which involves random actions. The slide outlines a method where every time step, a coin is flipped with a small probability ε to act randomly, and with a large probability 1-ε, to act on the current policy. It also mentions problems with random actions, such as eventually exploring the space but keeping thrashing around once learning is done, and suggests solutions like lowering ε over time or using exploration functions. The slide includes a cartoon image of a robot with a coin in its hand, appearing to be in a state of confusion or contemplation.\n",
            "Caption for /content/drive/MyDrive/pages/page_414.png-- OCR: Exploration Functions\n",
            "\n",
            " \n",
            "\n",
            "m When to explore?\n",
            "\n",
            "m Random actions: explore a fixed amount\n",
            "= Better idea: explore areas whose badness is not\n",
            "(yet) established, eventually stop exploring\n",
            "\n",
            "   \n",
            "\n",
            "= Exploration function constant\n",
            "m Takes a value estimate u and a visit count n, and \\ 7\n",
            "returns an optimistic utility, e.g. f(u,n) =u+k/n\n",
            "\n",
            "Regular Q-Update: Q(s,a) <a R(s,a, 8’) + ymax Q(s‘, a’)\n",
            "Modified Q-Update: Q(s,a) <a R(s,a,s’) + ymax f(Q(s',a\"), N(s‘,a’))\n",
            "\n",
            "x <q visshorthand for x — (1—a)x+av; Caption: The image displays a slide titled 'Exploration Functions' with bullet points discussing when to explore, the exploration function, and two types of Q-Update formulas. There is also a cartoon illustration of a robot with a 'DO NOT ENTER' sign and a menacing figure with red eyes.\n",
            "Caption for /content/drive/MyDrive/pages/page_415.png-- OCR: How Can we Evaluate Exploration Methods?; Caption: The image shows a robot standing on a platform with flames on the ground, and a smaller image of the same robot with a red 'X' over it, indicating a failed attempt or a wrong method.\n",
            "Caption for /content/drive/MyDrive/pages/page_416.png-- OCR: Regret\n",
            "\n",
            " \n",
            "\n",
            "m Evenif you learn the optimal policy,\n",
            "you still make mistakes along the way’\n",
            "\n",
            "= Regret is a measure of your total\n",
            "mistake cost:\n",
            "\n",
            "a Difference between all your (expected)\n",
            "rewards, including youthful suboptimality,\n",
            "and optimal (expected) rewards\n",
            "\n",
            "= Minimizing regret goes beyond\n",
            "learning to be optimal — it requires\n",
            "optimally learning to be optimal\n",
            "\n",
            "= For example: random exploration and\n",
            "exploration functions both end up\n",
            "optimal, but random exploration has\n",
            "higher regret; Caption: The image shows a robot with a blue body and white limbs, standing on a platform with flames underneath. The robot appears to be in a state of distress or regret, as indicated by the red cross mark on its chest and the flames.\n",
            "Caption for /content/drive/MyDrive/pages/page_417.png-- OCR: Are We Done?\n",
            "\n",
            "m Large and complex state spaces are still a problem!; Caption: The image displays a slide with a title 'Are We Done?' followed by a bullet point stating 'Large and complex state spaces are still a problem!'\n",
            "Caption for /content/drive/MyDrive/pages/page_418.png-- OCR: Approximate Q-Learning; Caption: The image shows a cartoon representation of a computer with a ghost-like figure interacting with it.\n",
            "Caption for /content/drive/MyDrive/pages/page_419.png-- OCR: Generalizing Across States\n",
            "\n",
            " \n",
            "\n",
            "m Basic Q-Leaming keeps a table of all g-values\n",
            "\n",
            "m Inrealistic situations, we cannot possibly learn\n",
            "about every single state!\n",
            "m= Too many states to visit them all in training\n",
            "m= Too many states to hold the q-tables in memory\n",
            "\n",
            "m Instead, we want to generalize:\n",
            "m Leam about some small number of training states from\n",
            "experience\n",
            "= Generalize that experience to new, similar situations\n",
            "a This is a fundamental idea in machine leaming, and we'll\n",
            "see it over and over again; Caption: The image is a slide from a presentation titled 'Generalizing Across States'. It contains text and two illustrations. The text explains the concept of Q-Learning in machine learning, highlighting the limitations of learning about every single state and the need to generalize from a small number of training states. The illustrations depict a robot reading a book and a robot looking at a bookshelf, symbolizing the process of learning and generalization.\n",
            "Caption for /content/drive/MyDrive/pages/page_420.png-- OCR: Example: Pacman\n",
            "\n",
            " \n",
            "\n",
            "Let's say we discover In naive g-learning,\n",
            "through experience we know nothing\n",
            "that this state is bad: about this state:\n",
            "\n",
            "*\n",
            "*\n",
            "\n",
            "*\n",
            "\n",
            ":_ ee |\n",
            "e\n",
            "\n",
            "r\n",
            "ee ee\n",
            "\n",
            "A\n",
            "oy\n",
            "oy\n",
            "A\n",
            "A\n",
            "oy\n",
            "*\n",
            "A\n",
            "e; Caption: The image shows two panels with maze-like graphics. The left panel has a blue outline and the right panel has a black outline. Both panels have a similar layout with paths, dead ends, and a few obstacles. The left panel has a Pacman character, a ghost, and a coin, while the right panel has a ghost, a coin, and a power pellet.\n",
            "Caption for /content/drive/MyDrive/pages/page_421.png-- OCR: Feature-Based Representations\n",
            "\n",
            " \n",
            "\n",
            "m Solution: describe a state using a vector of\n",
            "features (properties) f;, fa, ...\n",
            "= Features are functions from states to real numbers\n",
            "(often 0/1) that capture important properties of the\n",
            "state\n",
            "m Example features:\n",
            "Distance to closest ghost\n",
            "Distance to closest dot\n",
            "Number of ghosts\n",
            "1/ (dist to dot)2\n",
            "Is Pacman in a tunnel? (0/1)\n",
            "\n",
            "m Canalso describe a q-state (s, a) with features (e.g.\n",
            "action moves closer to food); Caption: The image displays a slide titled 'Feature-Based Representations' with a list of bullet points explaining the concept. It includes a visual representation of a maze game with Pacman and ghosts.\n",
            "Caption for /content/drive/MyDrive/pages/page_422.png-- OCR: Approximate Q-Learning\n",
            "\n",
            " \n",
            "\n",
            "m Using a feature representation f;, fo, ...we can write aq function (or value function)\n",
            "for any state using a few weights wz, Wa, ...:\n",
            "\n",
            "V(s) = wifi(s) + wofa(s) +... + wnfn(s)\n",
            "Q(s,a) = wifi(s, a)+wofa(s,a)+...+unfn(s, a)\n",
            "m Advantage: our experience is summed up in a few powerful numbers wi, Wa, ...\n",
            "\n",
            "m Disadvantage: states may share features but actually be very different in value!\n",
            "m Ex these two states would have the same value if we don’t include ghost positions as a feature:; Caption: The image displays a slide titled 'Approximate Q-Learning' with text and two small graphics.\n",
            "Caption for /content/drive/MyDrive/pages/page_423.png-- OCR: YJ Reinforcement Learning with Neural Network\n",
            "yi\n",
            "\n",
            "m Deep Q-Learning is a reinforcement learning technique\n",
            "that combines Q-Learning with deep neural networks.\n",
            "mw Why Deep Q-Learning?\n",
            "\n",
            "m Imagine an environment with 10,000 states and 1,000 actions per state.\n",
            "This would create a table of 10 million cells.\n",
            "\n",
            " \n",
            "\n",
            "State-Action Val\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "[| avaue |\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "[sete\n",
            "[Aeon\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "elololojojoljoloso/z\n",
            "o\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Q Learning\n",
            "\n",
            "138; Caption: The image is a slide from a presentation on Reinforcement Learning with Neural Network. It explains Deep Q-Learning as a reinforcement learning technique that combines Q-Learning with deep neural networks. The slide poses a question about the necessity of Deep Q-Learning and provides a hypothetical scenario to illustrate the complexity of Q-Learning. It mentions an environment with 10,000 states and 1,000 actions per state, which would result in a table of 10 million cells. The slide includes a diagram showing the relationship between 'State', 'Action', and 'Q-Value' with arrows pointing from 'State' and 'Action' to 'Q-Value', indicating the process of Q-Learning. The slide is numbered 138.\n",
            "Caption for /content/drive/MyDrive/pages/page_424.png-- OCR: ‘YJ Deep Q-Learning Networks (DQNs) C\n",
            "mm\n",
            "\n",
            "m By using a neural network to approximate the Q-function, which estimates\n",
            "the expected cumulative reward for each action in a given state, Deep Q-\n",
            "Learning can handle environments with large state spaces.\n",
            "\n",
            "m Itaims to enable agents to learn optimal actions in complex, high-\n",
            "dimensional environments.\n",
            "\n",
            " \n",
            "\n",
            "[state |\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Deep Q Learning\n",
            "\n",
            "139; Caption: The image is a slide from a presentation on Deep Q-Learning Networks (DQNs). It includes a title, two bullet points explaining the concept, and a diagram illustrating the architecture of a Deep Q-Learning network. The diagram shows a neural network with multiple layers and connections, representing the state input leading to multiple Q-value outputs for different actions.\n",
            "Caption for /content/drive/MyDrive/pages/page_425.png-- OCR: L A Comparison Between\n",
            "-_ Q-learning & Deep Q-learning\n",
            "\n",
            "Q-Table So Si S2 S3 S4\n",
            "\n",
            "+4.21 +3.24 +1.84 +2.33 +3.73\n",
            "\n",
            "  \n",
            "\n",
            "Ql |+2.53 +7.44 +3.34 +531 +6.22\n",
            "\n",
            "DQN\n",
            "\n",
            "Q(s,a) > Q(3,1) > 5s\n",
            "\n",
            "140; Caption: This image is a slide from a presentation comparing Q-learning and Deep Q-learning.\n",
            "Caption for /content/drive/MyDrive/pages/page_426.png-- OCR: YJ Training DQNs C\n",
            "mm\n",
            "\n",
            "= With the neural network taking the place of the Q-table, we can\n",
            "simplify the Q-learning algorithm.\n",
            "\n",
            "[| Current Q-table value we are updating\n",
            "\n",
            "[| Learning rate\n",
            "[| Reward\n",
            "[| Discount\n",
            "\n",
            "[| Estimated reward from our next action\n",
            "141; Caption: The image displays a slide titled 'Training DQNs' with a bullet point explaining that with a neural network replacing the Q-table, the Q-learning algorithm can be simplified. Below the text, there is a mathematical equation representing the Q-learning update rule, with each term color-coded and labeled to correspond with the components of the equation. The slide number '141' is visible at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_427.png-- OCR: YJ Training DQNs\n",
            "mm\n",
            "\n",
            "  \n",
            "\n",
            "= With the neural network taking the place of the Q-table, we can\n",
            "simplify the Q-learning algorithm.\n",
            "\n",
            "* The learning rate is no longer\n",
            "+1 tearing rate needed, as our back-propagating\n",
            "\n",
            " \n",
            "\n",
            "optimizer will already have that.\n",
            "[| Reward * Once the learning rate is removed,\n",
            "\n",
            "you realize that you can also\n",
            "[| Discount remove the two Q(s, a) terms, as\n",
            "\n",
            "they cancel each other out after\n",
            "[| Estimated reward from our next action getting rid of the learning rate.\n",
            "\n",
            "142; Caption: The image is a slide from a presentation on 'Training DQNs' which stands for Deep Q-Networks. It features a mathematical equation that represents the Q-learning algorithm, which is a method used in reinforcement learning. The equation is Q(st, at) = Q(st, at) + α[r+1 + λmax Q(st+1, a) - Q(st, at)], where Q(st, at) is the current Q-table value being updated, α is the learning rate, r+1 is the reward, λ is the discount factor, and Q(st+1, a) is the estimated reward from the next action. The slide also includes a legend explaining the colors of the terms in the equation: the current Q-table value, learning rate, reward, discount, and estimated reward from the next action. Additionally, there are bullet points explaining that the learning rate is no longer needed as the back-propagating optimizer will already have that, and once the learning rate is removed, the two Q(st, a) terms can also be removed as they cancel each other out after getting rid of the learning rate.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_428.png-- OCR: yN Challenges in Deep Reinforcement Learning C\n",
            "ye\n",
            "\n",
            "= Non-stationary or unstable learning target\n",
            "\n",
            "caer a\n",
            "\n",
            "The learning target is sontinuously changing with\n",
            "each iteration. In deep learning, the target variable\n",
            "does not change and hence the training is stable,\n",
            "which is just not true for reinforcement learning.\n",
            "\n",
            "= To summarize, we often depend on the policy or value functions in reinforcement\n",
            "learning to sample actions. However, this is frequently changing as we continuously\n",
            "learn what to explore. As we play out the game, we get to know more about the\n",
            "\n",
            "ground truth values of states and actions and hence, the output is also changing.\n",
            "143; Caption: The image is a slide from a presentation titled 'Challenges in Deep Reinforcement Learning'. It discusses the concept of a non-stationary or unstable learning target in the context of deep reinforcement learning. The slide includes a mathematical equation that represents the learning target, which is continuously changing with each iteration. The equation is Q(st, at) = Q(st, at) + α[r(st+1) + λmax Q(st+1, a)], where Q(st, at) is the learning target, α is the learning rate, r(st+1) is the reward, and λ is the discount factor. The slide also contains bullet points explaining that in deep learning, the target variable does not change, which is not true for reinforcement learning. It further explains that in reinforcement learning, the policy or value functions are used to sample actions, but these functions change as the learning progresses, leading to a non-stationary learning target.\n",
            "Caption for /content/drive/MyDrive/pages/page_429.png-- OCR: Tricks for handling a constantly changing input and output\n",
            "\n",
            "  \n",
            "\n",
            "AN\n",
            "\n",
            "= Target Network\n",
            "e Since the same network is calculating the predicted value and the target\n",
            "\n",
            "value, there could be a lot of divergence between these two. So, instead of\n",
            "using one neural network for learning, we can use two.\n",
            "\n",
            "e We could use a separate network to estimate the target. This target\n",
            "network has the same architecture as the function approximator but with\n",
            "frozen parameters. For every C iterations (a hyperparameter), the\n",
            "parameters from the prediction network are copied to the target network.\n",
            "\n",
            "e This leads to more stable training because it keeps the target function fixed\n",
            "(for a while).\n",
            "\n",
            "144; Caption: The image is a slide from a presentation, specifically slide number 144. It is titled 'Tricks for handling a constantly changing input and output' and focuses on the concept of a 'Target Network' in the context of neural networks. The slide contains bullet points explaining the use of a separate network to estimate the target value, which has the same architecture as the function approximator but with frozen parameters. It also mentions that this approach leads to more stable training because the target function is kept fixed for a while. The slide is part of a larger presentation, as indicated by the slide number.\n",
            "Caption for /content/drive/MyDrive/pages/page_430.png-- OCR: ‘Y (cmon jvm)\n",
            "mm\n",
            "\n",
            "Target Prediction\n",
            "\n",
            " \n",
            "\n",
            "     \n",
            "\n",
            "Parameter update at every\n",
            "C iterations\n",
            "\n",
            "Q’ Q\n",
            "\n",
            "Target Network Prediction Network\n",
            "\n",
            "145; Caption: This image shows a diagram of a neural network architecture with a target network and a prediction network.\n",
            "Caption for /content/drive/MyDrive/pages/page_431.png-- OCR: Tricks for handling a constantly changing input and output\n",
            "\n",
            "  \n",
            "\n",
            "AN\n",
            "\n",
            "m Experience Replay\n",
            "\n",
            "e Instead of running Q-learning on state/action pairs as they occur during\n",
            "simulation or the actual experience, we ‘replay’ the stored data [state,\n",
            "action, reward, next_state].\n",
            "\n",
            "e For example, suppose we are trying to build a video game bot where each\n",
            "frame of the game represents a different state. During training, we could\n",
            "sample a random batch of 64 frames from the last 100,000 frames to train\n",
            "our network.\n",
            "\n",
            "146; Caption: The image is a slide from a presentation with the title 'Tricks for handling a constantly changing input and output'. It discusses the concept of 'Experience Replay' in the context of Q-learning. The slide explains that instead of running Q-learning on state/action pairs as they occur during simulation or the actual experience, the data is 'replayed'. It provides an example of building a video game bot where each frame of the game represents a different state, and during training, a random batch of frames is sampled to train the network. The slide is numbered 146.\n",
            "Caption for /content/drive/MyDrive/pages/page_432.png-- OCR: YJ A Case Study C\n",
            "mm\n",
            "\n",
            "= Task: Language Model Self-Improvement (LMSI)\n",
            "\n",
            "e To improve language models without requiring external\n",
            "supervision, in an unsupervised training manner\n",
            "\n",
            "e Two notable LMSI works\n",
            "\n",
            "* RLCAI (arXiv 2022) RLCAI employs a model to evaluate which of\n",
            "\n",
            "* RLAIF (arXiv 2023) the two answers better adheres toa set of\n",
            "rules or principles, subsequently creating a\n",
            "dataset of Al preferences. The system then\n",
            "trains using reinforcement learning, with the\n",
            "preference model as the reward signal.\n",
            "\n",
            "147; Caption: The image is a slide from a presentation, featuring a case study on Language Model Self-Improvement (LMSI). The slide is titled 'A Case Study' and is numbered 147. It outlines the task of improving language models without external supervision in an unsupervised training manner. The slide lists two notable works on LMSI: RLCAI (arXiv 2022) and RLAIF (arXiv 2023). It provides a brief explanation of RLCAI, stating that it employs a model to evaluate which of the two answers better adheres to a set of rules or principles, subsequently creating a dataset of AI preferences. This dataset is then used to train the system using reinforcement learning, with the preference model serving as the reward signal.\n",
            "Caption for /content/drive/MyDrive/pages/page_433.png-- OCR: AN\n",
            "\n",
            "148\n",
            "\n",
            "Reinforcement Learning Contemplation (RLC)\n",
            "\n",
            " \n",
            "\n",
            "RLC is a novel LMSI\n",
            "method, which is\n",
            "grounded on the\n",
            "observation that it is\n",
            "simpler for a\n",
            "language model to\n",
            "evaluate a sentence\n",
            "than to generate it.\n",
            "\n",
            "Text generation task\n",
            "\n",
            "Write a mystery that leaves\n",
            "the reader unable to predict\n",
            "the ending. The answer is:\n",
            "\n",
            "Language model\n",
            "\n",
            "The answer is: Detective James\n",
            "had been working on the case\n",
            "for weeks, but he couldn‘t\n",
            "seem to make any headway ...\n",
            "\n",
            "Text generation\n",
            "\n",
            "[ICLR 2024]\n",
            "\n",
            "Self-evaluation task\n",
            "\n",
            "Is the following article a\n",
            "mystery that leaves the\n",
            "reader unable to predict the\n",
            "ending? [TEXT]. Answer:\n",
            "\n",
            "Language model\n",
            "\n",
            "Text evaluation; Caption: Reinforcement Learning Contemplation (RLC) [ICLR 2024]\n",
            "Caption for /content/drive/MyDrive/pages/page_434.png-- OCR: [ICLR 2024]\n",
            "\n",
            "YJ Reinforcement Learning Contemplation (RLC)\n",
            "mA\n",
            "\n",
            " \n",
            "\n",
            "m™ Given an unlabelled question set, the language model\n",
            "generates answers to unlabeled samples and directly\n",
            "evaluates the quality of these answers.\n",
            "\n",
            "= RLC employs self-evaluation results as rewards and\n",
            "utilizes reinforcement learning to optimize the language\n",
            "model.\n",
            "\n",
            "149; Caption: The image is a slide from a presentation, specifically slide number 149. It is titled 'Reinforcement Learning Contemplation (RLC)' and is associated with the 'ICLR 2024' conference. The slide contains two bullet points explaining the concept of RLC. The first bullet point states that given an unlabeled question set, the language model generates answers to unlabeled samples and directly evaluates the quality of these answers. The second bullet point explains that RLC employs self-evaluation results as rewards and utilizes reinforcement learning to optimize the language model. The slide has a blue header with the title and a white background for the content. There is a watermark or logo in the top left corner that appears to be a profile picture of a person with the letters 'A' and 'I' visible.\n",
            "Caption for /content/drive/MyDrive/pages/page_435.png-- OCR: YJ Potential for Self-Improvement C\n",
            "mA\n",
            "\n",
            "m™ Experiments conducted on NLP benchmarks\n",
            "\n",
            "= Benchmark#1 (reasoning):\n",
            "e Bigbench-hard (2023)\n",
            "\n",
            "* including multiple reasoning tasks consisting of multiple-choice and\n",
            "direct-generation tasks.\n",
            "\n",
            "BIG-bench fF,\n",
            "\n",
            "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large\n",
            "language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench are\n",
            "\n",
            "https://github.com/google/BIG-bench?tab=readme-ov-file\n",
            "\n",
            "150; Caption: The image is a slide from a presentation, titled 'Potential for Self-Improvement'. It discusses experiments conducted on NLP benchmarks, specifically focusing on Benchmark#1 (reasoning). The slide mentions 'Bigbench-hard (2023)' which includes multiple reasoning tasks consisting of multiple-choice and direct-generation tasks. There is also a mention of 'BIG-bench', which is described as a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The slide includes a URL link to the BIG-bench repository on GitHub.\n",
            "Caption for /content/drive/MyDrive/pages/page_436.png-- OCR: AN\n",
            "\n",
            "m This task asks models to answer questions about the\n",
            "colors of objects on a surface.\n",
            "\n",
            "= Question types:\n",
            "\n",
            "e 1. What color is a particular object?\n",
            "\n",
            "* EX: On the floor, | see a pink cup, a blue crayon, and a purple puzzle.\n",
            "What color is the puzzle?\n",
            "\n",
            "e 2. Is a particular object a particular color?\n",
            "\n",
            "¢ EX: On the desk, there is a red pencil, a gold envelope, a magenta\n",
            "puzzle, a teal mug, and a purple crayon. Is the puzzle black?\n",
            "\n",
            "Sample Task: Reasoning about Colored Objects (1/3) C\n",
            "\n",
            "151; Caption: The image displays a slide from a presentation, specifically slide number 151. It is titled 'Sample Task: Reasoning about Colored Objects (1/3)' and is part of a series, as indicated by the fraction in the title. The slide is divided into two main sections. The first section provides an overview of the task, which involves asking models to answer questions about the colors of objects on a surface. The second section lists two types of questions that models should be able to answer, with examples provided for each. The first type of question asks for the color of a particular object, and the second type asks if a particular object is a particular color. The slide uses a simple layout with bullet points and clear, concise text.\n",
            "Caption for /content/drive/MyDrive/pages/page_437.png-- OCR: AN\n",
            "\n",
            "= Question types (cont.):\n",
            "\n",
            "e 3. How many objects are not particular colors?\n",
            "« EX: On the table, there is a purple plate and a brown textbook. How\n",
            "many objects are neither purple nor brown?\n",
            "e 4. How are the objects ordered on the surface?\n",
            "¢ EX: On the floor, you see the following items arranged in a row: a\n",
            "purple stuffed bear, a silver pair of sunglasses, a burgundy plate, a\n",
            "blue puzzle, and a mauve pencil. What is the color of the item\n",
            "directly to the right of the pair of sunglasses?\n",
            "\n",
            "Sample Task: Reasoning about Colored Objects (2/3) C\n",
            "\n",
            "152; Caption: The image displays a slide from a presentation, specifically slide number 152. It is titled 'Sample Task: Reasoning about Colored Objects (2/3)' and contains a list of question types related to reasoning about colored objects. There are two question types listed, each with an example provided. The first question type is about counting objects that are not a specific color, with an example involving a purple plate and a brown textbook. The second question type is about determining the order of objects on a surface, with an example involving various items arranged in a row. The slide has a blue header with the title and a white background for the content.\n",
            "Caption for /content/drive/MyDrive/pages/page_438.png-- OCR: AN\n",
            "\n",
            "= Question types (cont.):\n",
            "\n",
            "e 5. How many objects of a type remain when some objects are\n",
            "removed?\n",
            "¢ EX: On the nightstand, you see three grey mugs, one burgundy jug,\n",
            "two gold mugs, one burgundy mug, two brown mugs, three brown\n",
            "jugs, and two gold jugs. If | remove all the jugs from the nightstand,\n",
            "how many brown things remain on it?\n",
            "\n",
            "Sample Task: Reasoning about Colored Objects (3/3) C\n",
            "\n",
            "153; Caption: The image displays a slide from a presentation, specifically slide number 153, which is part of a series titled 'Sample Task: Reasoning about Colored Objects (3/3)'. The slide contains a list of question types, with the focus on question type number 5, which asks about the number of objects of a certain type remaining after some have been removed. An example is provided to illustrate this type of question, involving a scenario with various colored mugs and jugs on a nightstand. The slide is text-based with no charts, tables, or diagrams.\n",
            "Caption for /content/drive/MyDrive/pages/page_439.png-- OCR: ‘YJ Potential for Self-Improvement\n",
            "mm\n",
            "\n",
            " \n",
            "\n",
            "m We compare two answer generation strategies:\n",
            "\n",
            "e (1) w/o SE: the answer is a directly deterministic output of the LLM.\n",
            "e (2) w/ SE: LLM generates an answer and evaluates its correctness. If the\n",
            "\n",
            "evaluation indicates the answer is incorrect, LLM re-generates an answer\n",
            "as the final output.\n",
            "\n",
            "Reasoning about Logical Tracking Shuffled § Object\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Tracking Shuffled Geometric\n",
            "Colored Objects Deduction (7) Objects (5) Counting Objects (3) Shapes\n",
            "w/o SE 43.2% 30.3% 13.7% 22.6% 25.7% 9.6%\n",
            "w/ SE 44.7% 34.5% 10.6% 22.5% 28.5% 10.5%\n",
            ". Sports Logical Logical Penguins in .\n",
            "| Web of Lies Understanding Deduction (3) Deduction (5) a Table Navigate\n",
            "w/o SE 50.5% 55.1% 50.7% 29.7% 30.8% 51.1%\n",
            "154 w/ SE 51.0% 55.1% 57.2% 34.8% 34.0% 49.9%; Caption: Slide 154 titled 'Potential for Self-Improvement' compares two answer generation strategies.\n",
            "Caption for /content/drive/MyDrive/pages/page_440.png-- OCR: ‘YJ Potential for Self-Improvement C\n",
            "mm\n",
            "\n",
            "m Experiments conducted on NLP benchmarks\n",
            "\n",
            "m Benchmark#2 (Summarization):\n",
            "\n",
            "e CNN/Daily Mail (2016)\n",
            "* including 286,817 training pairs, 13,368 validation pairs and 11,487\n",
            "test pairs, as defined by their scripts from new-stories in CNN and\n",
            "Daily Mail websites. The source documents in the training set have\n",
            "766 words spanning 29.74 sentences on an average while the\n",
            "summaries consist of 53 words and 3.72 sentences.\n",
            "\n",
            "e BBC (2021)\n",
            "\n",
            "* including 2225 BBC news website documents in five topical areas\n",
            "from 2004-2005\n",
            "\n",
            "155 Reference#1: “Abstractive text summarization using sequence-to-sequence RNNs and beyond,” CoNLL 2016\n",
            "Reference#2: “Automated News Summarization Using Transformers,” ICSAC 2021.; Caption: The image is a slide from a presentation, titled 'Potential for Self-Improvement'. It discusses experiments conducted on NLP benchmarks, specifically focusing on a benchmark related to summarization. The slide details two datasets used in the experiments: one from CNN/Daily Mail (2016) and another from BBC (2021). The CNN/Daily Mail dataset includes 286,817 training pairs, 13,368 validation pairs, and 11,487 test pairs, with source documents spanning an average of 29.74 sentences. The BBC dataset includes 2225 news website documents from five topical areas from 2004-2005. The slide also references two studies related to the topic.\n",
            "Caption for /content/drive/MyDrive/pages/page_441.png-- OCR: yN Sample Data of CNN/Daily Mail\n",
            "yi\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "| Source Document\n",
            "( @entityO ) wanted : film director , must be eager to shoot footage of golden lassos and invisible\n",
            "jets . <eos> @entity0 confirms that @entity5 is leaving the upcoming \" @entity9 \" movie ( the\n",
            "hollywood reporter first broke the story ) . <eos> @entity5 was announced as director of the movie\n",
            "in november . <eos> @entity0 obtained a statement from @entity13 that says ,\"\" given creative\n",
            "differences , @entity13 and @entityS have decided not to move forward with plans to develop and\n",
            "direct ’ @entity9 ’ together . <eos> \" ( @entityO and @entity13 are both owned by @entity16\n",
            ". <eos> ) the movie , starring @entity18 in the title role of the @entity21 princess , is still set\n",
            "for release on june 00 , 0000 . <eos> it ’s the first theatrical movie centering around the most\n",
            "popular female superhero . <eos> @entity18 will appear beforehand in \" @entity25 v. @entity26\n",
            ": @entity27 , \" due out march 00 , 0000 . <eos> in the meantime , @entity13 will need to find\n",
            "someone new for the director ’s chair . <eos>\n",
            "\n",
            "| Ground truth Summary\n",
            "\n",
            "@entity5 is no longer set to direct the first \" @entity9 \" theatrical movie <eos> @entity5 left the\n",
            "\n",
            "project over \" creative differences \" <eos> movie is currently set for 0000\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "156; Caption: Sample Data of CNN/Daily Mail\n",
            "Caption for /content/drive/MyDrive/pages/page_442.png-- OCR: Evaluation Metrics for Text Summarization\n",
            "\n",
            " \n",
            "\n",
            "AN\n",
            "\n",
            "= BLEU: Word-Level Similarity Measurement\n",
            "= ROUGE: Word-Level Similarity Measurement\n",
            "= BERTScore: Measuring Semantic Similarity\n",
            "\n",
            "Reference#1: “BLEU: a method for automatic evaluation of machine translation,” ACL 2002.\n",
            "Reference#2: “ROUGE: A package for automatic evaluation of summaries,” TSBO 2004.\n",
            "Reference#3: “Bertscore: Evaluating text generation with BERT,” ICLR 2020.\n",
            "\n",
            "157; Caption: The image displays a slide titled 'Evaluation Metrics for Text Summarization' with a list of three bullet points. The first two bullet points are 'BLEU: Word-Level Similarity Measurement' and 'ROUGE: Word-Level Similarity Measurement,' both in black text. The third bullet point is 'BERTScore: Measuring Semantic Similarity,' also in black text. Below the bullet points, there are three references listed in blue text, each providing a source for the respective evaluation metric. The slide number '157' is visible at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_443.png-- OCR: \\ CD €\n",
            "\n",
            "= Computing unigrams precision\n",
            "e To count how many words in a candidate text C are present in the\n",
            "reference text R, and divide the result by the number of words in C to get a\n",
            "percentage.\n",
            "¢ Therefore, a perfect score is 1.0, whereas the worst score is 0.0.\n",
            "e Example:\n",
            "« R: The cat is on the mat.\n",
            "¢ C: The cat and the dog.\n",
            "* In C, there are three words (“the’, “cat”, “the”) that appear on the reference R, thus:\n",
            "BLEU*(C) = 3/5 = 0.6\n",
            "e The problem with repeating unigrams\n",
            "* Consider C: “The The The The The”, BLEU*(C) = 5/5 = 1!\n",
            "\n",
            "158; Caption: The image is a slide from a presentation, specifically slide number 158, discussing the concept of BLEU (Bilingual Evaluation Understudy) in the context of computing unigram precision. The slide is titled 'BLEU (1/4)' and is part of a larger series. It explains how to calculate the precision of unigrams by comparing the number of words in a candidate text (C) that are present in a reference text (R) and dividing the result by the number of words in C to get a percentage. The slide provides an example where the reference text R is 'The cat is on the mat.' and the candidate text C is 'The cat and the dog.' It calculates the BLEU score as 0.6, indicating that 60% of the words in the candidate text are present in the reference text. The slide also highlights a problem with repeating unigrams, where a candidate text with repeated words can artificially inflate the BLEU score, as demonstrated with an example where the score is 1.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_444.png-- OCR: \\ CD @\n",
            "\n",
            "= Computing n-grams precision\n",
            "e Away to get higher scores for well-formed sentences is to consider\n",
            "matching 2-grams or 3-grams instead of 1-grams only. Let’s call BLEU**,\n",
            "the score that considers only 1-grams and BLEU**, the score that\n",
            "considers only 2-grams.\n",
            "e Example:\n",
            "¢ R: There is a cat on the mat.\n",
            "¢ C1: There is a cat on the mat.\n",
            "* C2: Mat the cat is on a there.\n",
            "* C7 has six 2-grams and they all appear on the reference R, thus:\n",
            "» BLEU**,(C1) = 7/7 = 1.0 & BLEU**,(C1) = 6/6 = 1.0\n",
            "* Instead, in C2 all the 2-grams don’t appear in the reference, thus:\n",
            "» BLEU**,(C2) = 7/7 = 1.0 & BLEU**,(C2) = 0/6 = 0.0\n",
            "\n",
            "159; Caption: The image is a slide from a presentation, specifically slide number 159, discussing the concept of 'BLEU' in the context of natural language processing. The slide is titled 'BLEU (2/4)', indicating it is the second part of a four-part series on BLEU. The main content of the slide is about 'Computing n-grams precision' and suggests a method to achieve higher scores for well-formed sentences by considering matching 2-grams or 3-grams instead of 1-grams only. The slide introduces two types of BLEU scores: BLEU**1, which considers only 1-grams, and BLEU**2, which considers only 2-grams. An example is provided to illustrate the difference between the two scores. The slide is text-heavy with no images, charts, or diagrams.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_445.png-- OCR: \\ CD @\n",
            "\n",
            "m In the literature, it has been found that the geometric mean of the\n",
            "BLEU**, scores with n between one and four has the best\n",
            "correlation with human evaluation, therefore it’s the score more\n",
            "commonly adopted, called MEAN_BLEU™**.\n",
            "\n",
            "= Penalizing short candidate translations\n",
            "\n",
            "e Consider\n",
            "¢ R: There is a cat on the mat.\n",
            "\n",
            "¢ C: There is a cat.\n",
            "¢ The scores are: BLEU**,(C) = 4/4 = 1.0 & BLEU**,(C) = 3/3= 1.0\n",
            "\n",
            "¢ Looks like C achieves a perfect BLEU**, score for each n, even though the\n",
            "candidate is missing a piece of text with respect to the reference.\n",
            "\n",
            "160; Caption: The image is a slide from a presentation, specifically slide number 160. It is titled 'BLEU (3/4)' and contains text discussing the BLEU score, which is a metric used to evaluate the quality of machine translation. The slide explains that the geometric mean of the BLEU scores with n between one and four has the best correlation with human evaluation and is the most commonly adopted score, referred to as MEAN_BLEU. The slide also discusses the concept of penalizing short candidate translations, providing an example with two sentences in English and their corresponding translations in another language. The example demonstrates how BLEU scores are calculated for each sentence, resulting in a perfect score despite one sentence being missing a piece of text. The slide is part of a larger series, as indicated by the title 'BLEU (3/4)', suggesting there are at least three other slides in this series.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_446.png-- OCR: \\ CD @\n",
            "\n",
            "= Brevity Penalty (BP)\n",
            "e To add a penalty for a candidate whose length is less than the reference\n",
            "\n",
            "= Therefore, the final BLEU score is:\n",
            "\n",
            "BLEU = BP * MEAN_BLEU**\n",
            "\n",
            "BP = 1 ifc>r  ¢: the length of the candidate\n",
            "el-r/e) if e<r___rthe length of the reference\n",
            "\n",
            "161; Caption: The image is a slide from a presentation, specifically slide number 161, discussing the BLEU (Brevity Penalty) score in the context of machine translation evaluation. The slide is titled 'BLEU (4/4)' and is part of a series, as indicated by the '(4/4)' in the title. The slide explains the concept of Brevity Penalty (BP), which is a penalty added to a candidate's length that is less than the reference length. It then presents the formula for calculating the final BLEU score, which is BLEU = BP * MEAN_BLEU**. Below the formula, there is a conditional expression for BP, which is '1' if the candidate length 'c' is greater than the reference length 'r', and 'e^(1-r/c)' if 'c' is less than or equal to 'r'. The slide uses a simple color scheme with a white background and black text, and the formula is highlighted in blue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_447.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BertScore is a method used to measure the quality of text\n",
            "summarization. This method measures how similar the text\n",
            "summary is to the original text.\n",
            "\n",
            "m BertScore addresses two common issues that n-gram-based\n",
            "metrics often encounter.\n",
            "\n",
            "e First, n-gram models tend to incorrectly match paraphrases because\n",
            "semantically accurate expressions may differ from the surface form of the\n",
            "reference text, which can lead to incorrect performance estimation.\n",
            "\n",
            "e Second, n-gram models cannot capture long-range dependencies and\n",
            "penalize semantically significant reordering.\n",
            "\n",
            "162; Caption: The image is a slide from a presentation, featuring the title 'BERTScore' in large, bold letters at the top. Below the title, there are two bullet points explaining what BERTScore is and the issues it addresses. The first bullet point states that BERTScore is a method used to measure the quality of text summarization by comparing the similarity of the text summary to the original text. The second bullet point lists two common issues that n-gram-based metrics often encounter, which BERTScore addresses. The first issue is that n-gram models tend to incorrectly match paraphrases because semantically accurate expressions may differ from the surface form of the reference text, leading to incorrect performance estimation. The second issue is that n-gram models cannot capture long-range dependencies and penalize semantically significant reordering. The slide number '162' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_448.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding (Optional)\n",
            "Reference @\n",
            "the weather is _ ee) _>\n",
            "cog — > Rowse = OR ae Pei\n",
            "\n",
            "Candidate\n",
            "> —\n",
            "it is freezing today &\n",
            "\n",
            "163\n",
            "\n",
            " \n",
            "\n",
            "Candidate; Caption: Page showing BERTScore Architecture.\n",
            "Caption for /content/drive/MyDrive/pages/page_449.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Step 1: Contextual Embeddings:\n",
            "\n",
            "Reference and candidate sentences are represented using\n",
            "contextual embeddings based on surrounding words, computed\n",
            "by models like BERT, Roberta, XLNET, and XLM.\n",
            "\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding Similarity (Optional)\n",
            "\n",
            "Reference\n",
            "the weather is _ oe) >\n",
            "cold today =\n",
            "Candidate 7\n",
            "\n",
            "> —\n",
            "it is freezing today\n",
            "\n",
            "164\n",
            "\n",
            "> — (0.713% 1.27)+(0.515x7.94)+...\n",
            "ReeERT = “73747.9441.82+7.90+8.88\n",
            "\n",
            "   \n",
            "\n",
            "idf\n",
            "weights\n",
            "Candidate; Caption: Slide 164 of a presentation on BERTScore.\n",
            "Caption for /content/drive/MyDrive/pages/page_450.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Reference\n",
            "the weather is\n",
            "cold today\n",
            "\n",
            "Candidate Z\n",
            "\n",
            "it is freezing today\n",
            "\n",
            "165\n",
            "\n",
            "Step 2: Cosine Similarity:\n",
            "\n",
            "The similarity between contextual embeddings of\n",
            "reference and candidate sentences is measured using\n",
            "cosine similarity.\n",
            "\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding (Optional)\n",
            "\n",
            "— | |\n",
            "—\n",
            "\n",
            "&)\\-\n",
            "\n",
            "> — (0.713% 1.27)+(0.515x7.94)+...\n",
            "ReeERT = “73747.9441.82+7.90+8.88\n",
            "\n",
            "Reference\n",
            "\n",
            "  \n",
            "\n",
            "we © weights\n",
            "\n",
            "Candidate; Caption: Page showing BERTScore Architecture.\n",
            "Caption for /content/drive/MyDrive/pages/page_451.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Step 3: Token Matching for Precision and Recall:\n",
            "Each token in the candidate sentence is matched to the most similar token in the reference sentence,\n",
            "and vice versa, to compute Recall and Precision, which are then combined to calculate the F1 score.\n",
            "\n",
            "Reference\n",
            "the weather is\n",
            "cold today\n",
            "\n",
            "Candidate\n",
            "it is freezing today\n",
            "\n",
            "166\n",
            "\n",
            "1 Ts 1 Ts Peert + Reser\n",
            "= — Tg. PB —— Tg. F, -2-ee err\n",
            "Reert ia x max x; Xj > BERT ia dpe Xj; BERT Papert + Reet\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding Similarity (Optional)\n",
            "\n",
            "— | |\n",
            "\n",
            "> — (0.713% 1.27)+(0.515x7.94)+...\n",
            "ReeERT = “73747.9441.82+7.90+8.88\n",
            "\n",
            "Reference\n",
            "\n",
            "  \n",
            "\n",
            "idf\n",
            "we © weights\n",
            "\n",
            "Candidate; Caption: Page showing information about BERTScore.\n",
            "Caption for /content/drive/MyDrive/pages/page_452.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Step 4: Importance Weighting:\n",
            "\n",
            "Rare words’ importance is considered using Inverse Document\n",
            "Frequency (IDF), which can be incorporated into BERTScore\n",
            "equations, though it’s optional and domain-dependent.\n",
            "\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding Similarity (Optional)\n",
            "\n",
            "Reference\n",
            "the weather is _ ee) >\n",
            "cold today =\n",
            "Candidate 7\n",
            "\n",
            "> —\n",
            "it is freezing today\n",
            "\n",
            "167\n",
            "\n",
            "> — (0.713% 1.27)+(0.515x7.94)+...\n",
            "ReeERT = “73747.9441.82+7.90+8.88\n",
            "\n",
            "   \n",
            "\n",
            "idf\n",
            "weights\n",
            "Candidate; Caption: Page showing BERTScore Architecture.\n",
            "Caption for /content/drive/MyDrive/pages/page_453.png-- OCR: YJ BERTScore C\n",
            "ys\n",
            "\n",
            "= BERTScore Architecture\n",
            "\n",
            "Step 5: Baseline Rescaling:\n",
            "\n",
            "BERTScore values are linearly rescaled to Rgert — 0\n",
            "improve human readability, ensuring they Rgert = 1b\n",
            "fall within a more intuitive range.\n",
            "\n",
            "Contextual Pairwise Cosine Maximum Similarity Importance Weighting\n",
            "Embedding Similarity (Optional)\n",
            "\n",
            "Reference\n",
            "the weather is _ ee) _>\n",
            "cold today =\n",
            "Candidate 7\n",
            "\n",
            "> —\n",
            "it is freezing today\n",
            "\n",
            "168\n",
            "\n",
            "> — (0.713% 1.27)+(0.515x7.94)+...\n",
            "ReeERT = “73747.9441.82+7.90+8.88\n",
            "\n",
            "   \n",
            "\n",
            "idf\n",
            "weights\n",
            "Candidate; Caption: Page showing BERTScore Architecture.\n",
            "Caption for /content/drive/MyDrive/pages/page_454.png-- OCR: yN Overall Training Procedure of RLC\n",
            "yi\n",
            "\n",
            " \n",
            "\n",
            "m= Three steps:\n",
            "e (1) Answer generation to the unlabeled questions.\n",
            "e (2) Self-evaluation by asking LM using evaluation prompt, with the evaluation results as the reward.\n",
            "e (3) Update the language model to maximize the reward using reinforcement learning algorithms.\n",
            "e Note that the solid lines represent the data flow, while the dashed line represents the update of LLM parameters.\n",
            "\n",
            "Unlabeled questions Evaluation prompt\n",
            "(1) Generate\n",
            "\n",
            "Q1: Given a movie review, your > Is the answer to the\n",
            "task is to classify it as positive, question correct? The\n",
            "negative, or neutral based on the question is: [Q]. The\n",
            "text: The movie is so interesting ... answer is: [A].\n",
            "\n",
            "Q2: Sort the following list of words\n",
            "alphabetically: The word list: oven\n",
            "costume counterpart. answer to the question\n",
            "Q3: Today is Christmas Eve of 1937. and give me an\n",
            "\n",
            "What is the date 10 days ago? (2) Evaluate evaluation score from 1\n",
            "\n",
            "Options: (A) 12/14/2026 (B) to 10. The question is:\n",
            "12/14/2007 (C) 12/14/1937. €---=+ [Q]. The answer is [A].\n",
            "\n",
            "(3) Improve\n",
            "\n",
            "   \n",
            "      \n",
            " \n",
            "\n",
            "Evaluation\n",
            "prompt\n",
            "\n",
            "Language\n",
            "\n",
            "Please evaluate the\n",
            "\n",
            "model\n",
            "\n",
            " \n",
            "\n",
            "169; Caption: Overall Training Procedure of RLC\n",
            "Caption for /content/drive/MyDrive/pages/page_455.png-- OCR: YJ Problem Formulation (1/3) C\n",
            "mm\n",
            "\n",
            "m Given a vocabulary 2, and\n",
            "\n",
            "m an LLM M which takes a token sequence of the question\n",
            "q = {o: --- Yj} aS input and predicts the next token using\n",
            "autoregressive modeling:\n",
            "\n",
            "On, = M(q, {09, .. - , OF), Where g € 2” ando,E€ 2\n",
            "\n",
            "170; Caption: The image displays a slide titled 'Problem Formulation (1/3)' with a list of bullet points explaining the concept of an LLM (Language Model) that takes a token sequence of a question as input and predicts the next token using autoregressive modeling. The slide includes a mathematical expression that represents the autoregressive model. The slide is part of a larger presentation, as indicated by the slide number 170.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_456.png-- OCR: YJ Problem Formulation (2/3) C\n",
            "mm\n",
            "\n",
            "= To fine-tune LLM with reinforcement learning, we can view this\n",
            "problem by MDP, which is described as a tuple (S, A, P, R, y, do):\n",
            "e State space S: the space of input token sequences:\n",
            "* GU {0o, O7,..., OF.\n",
            "* S,is defined as the token sequence of question q.\n",
            "e Action space A: the space of tokens o,.\n",
            "\n",
            "e Reward function R(q, {0o, . . . , 0;}): a score that reflects the quality of the\n",
            "generated answer to the question.\n",
            "\n",
            "e Transition P: Sj4 = S;U Ops4-\n",
            "e Initial distribution d,: the distribution of question gq.\n",
            "\n",
            "171; Caption: The image is a slide from a presentation, specifically slide number 171, discussing the problem formulation for fine-tuning a Language Model (LLM) with reinforcement learning. The slide is titled 'Problem Formulation (2/3)' and contains bullet points explaining the components of the Markov Decision Process (MDP) used in this context. The components listed are the State space (S), Action space (A), Reward function (R), Transition (P), and Initial distribution (d0). Each component is briefly described, with the State space being the space of input token sequences, the Action space being the space of tokens, the Reward function reflecting the quality of the generated answer, the Transition describing the state transition, and the Initial distribution being the distribution of the question. The slide is part of a larger presentation, as indicated by the title, and it is the second part of a three-part series on problem formulation.\n",
            "Caption for /content/drive/MyDrive/pages/page_457.png-- OCR: YJ Self-evaluation as the Reward C\n",
            "ys\n",
            "\n",
            "=m RLC uses two types of prompts for evaluating the self-\n",
            "\n",
            "generated text:\n",
            "\n",
            "e (1) Correctness Evaluation Prompt (CEP)\n",
            "* \"Is the answer to the question correct? The question is: [Q]. The\n",
            "\n",
            "answer is: [A]”\n",
            "* The reward is a 0-1 value based on the evaluation result.\n",
            "\n",
            "e (2) Quality Evaluation Prompt (QEP)\n",
            "* \"Please evaluate the answer to the question and give me an\n",
            "evaluation score from 1 to 10. The question is: [Q]. The answer is\n",
            "\n",
            "AT\".\n",
            "\n",
            "172; Caption: The image is a slide from a presentation, titled 'Self-evaluation as the Reward'. It discusses the use of two types of prompts by RLC for evaluating self-generated text. The first type is the Correctness Evaluation Prompt (CEP), which asks if the answer to a question is correct and provides a reward based on the evaluation result. The second type is the Quality Evaluation Prompt (QEP), which requests an evaluation score from 1 to 10 for the answer to a question. The slide is numbered 172.\n",
            "Caption for /content/drive/MyDrive/pages/page_458.png-- OCR: YJ Problem Formulation (3/3) C\n",
            "mm\n",
            "\n",
            "m™ Here, the LLM M acts as a policy mapping from state\n",
            "space to the probability space over action space. The\n",
            "objective of reinforcement learning is to train the policy to\n",
            "maximize the expected return:\n",
            "\n",
            "| So1'R(ao)|a~ dovor~ MCs)\n",
            "\n",
            "t=0\n",
            "\n",
            "m With the evaluation reward, the LLM M can be updated\n",
            "using any RL algorithm to maximize the reward.\n",
            "\n",
            "173; Caption: The image is a slide from a presentation, specifically slide number 173, discussing the problem formulation of a Learning Logic Machine (LLM) M as a policy mapping from state space to probability space over action space. It explains the objective of reinforcement learning, which is to train the policy to maximize the expected return, represented by a mathematical equation. The slide also mentions that the LLM M can be updated using any Reinforcement Learning (RL) algorithm to maximize the reward.\n",
            "Caption for /content/drive/MyDrive/pages/page_459.png-- OCR: ‘| SD S\n",
            "mm\n",
            "\n",
            "= Model output examples on BigBench-hard\n",
            "\n",
            "Question Before training After training\n",
            "\n",
            "(Object Counting) I have two fish, four cats, two | First we add all the animals: two | I have 2 fish, 4 cats,\n",
            "\n",
            "chickens, a mouse, and a goat. How many animals | fish, 4 cats, 2 chickens, mouse, | 2 chickens, a mouse,\n",
            "\n",
            "do I have? Let’s think step by step. and goat= 11. Now we add up | and a goat, so i have\n",
            "all the animals together: 2x 4 | 2 fish + 4 cats + 2\n",
            "= 8. Now divide this by 4 and | chickens + 1 mouse\n",
            "multiply it by 2 to get the num- | + 1 goat = 10 ani-\n",
            "ber of animals: 8 x 4=48. So, | mals. So, the an-\n",
            "the answer is 48. swer is 10.\n",
            "\n",
            "(Logical Deduction Objects (3)) The following para- | The falcon is second from the | The blue jay is the\n",
            "graphs each describe a set of three objects arranged | left. The answer: (C). second from the left.\n",
            "in a fixed order. The statements are logically consis- The answer: (A).\n",
            "tent within each paragraph. On a branch, there are\n",
            "three birds: a blue jay, a quail, and a falcon. The\n",
            "falcon is to the right of the blue jay. The blue jay is\n",
            "to the right of the quail. Options: (A) The blue jay is\n",
            "the second from the left (B) The quail is the second\n",
            "from the left (C) The falcon is the second from the\n",
            "left. Let’s think step by step.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "174; Caption: The image is a slide from a presentation titled 'Evaluations' with a blue header that reads 'AI'. It contains a table with three columns and two rows, excluding the header row. The first column is titled 'Question' and contains two examples of questions related to object counting and logical deduction. The second column is titled 'Before training' and shows the model's initial response to the questions before being trained. The third column is titled 'After training' and displays the model's improved response after training. The slide number '174' is visible at the bottom.\n",
            "Caption for /content/drive/MyDrive/pages/page_460.png-- OCR: ‘| SD S\n",
            "mm\n",
            "\n",
            "= Model output examples on summarization task\n",
            "\n",
            "175\n",
            "\n",
            "Question\n",
            "\n",
            "Please give a summary of the following text. Text: Kyle\n",
            "Naughton has been ruled out for the rest of the season\n",
            "after David Meyler’s red card challenge on Saturday. The\n",
            "Hull midfielder was shown a straight red for the hefty\n",
            "challenge that left Naughton in hospital at the weekend,\n",
            "though neither Garry Monk nor Steve Bruce condemned\n",
            "Meyler at the time. A scan on Monday revealed Naughton\n",
            "suffered ligament damage and the right-back will face a six-\n",
            "week lay-off. David Meyler (left) lunges in high on Kyle\n",
            "Naughton and was sent off the poor challenge on Saturday.\n",
            "The full back writhes in pain following the incident and\n",
            "faces a six-week lay off with ankle ligament damage. A\n",
            "club statement said: ’ An initial scan on Saturday ruled out a\n",
            "fracture, but a follow up scan last night confirmed ligament\n",
            "damage that will keep the January signing from Spurs out\n",
            "for around six weeks. ’ With just seven league games left,\n",
            "it’s a blow for Naughton and the Swans with the right-\n",
            "back continuing to impress with every appearance.’ Angel\n",
            "Rangel will return to the side to face Everton on Saturday.\n",
            "Rangel said: \"It’s not good because Kyle has an injury and\n",
            "I don’t wish bad on anyone, but from my point of view I\n",
            "always try and be ready so if the manager needs me I’ll try\n",
            "my best.’ Naughton will miss the rest of the season and\n",
            "had to helped off the pitch by the Swansea team physios.\n",
            "Meyler looks shocked as referee Andre Marriner shows\n",
            "him a straight red, a decision that was justified. Answer:\n",
            "\n",
            " \n",
            "\n",
            "Before training\n",
            "\n",
            "[BELU score: 0.887]\n",
            "David Meyler lunged in\n",
            "high on Kyle Naughton\n",
            "and was sent off. The\n",
            "Hull midfielder was\n",
            "shown a straight red\n",
            "\n",
            "for the hefty challenge.\n",
            "\n",
            "The Hull midfielder was\n",
            "shown a straight red\n",
            "\n",
            "for the hefty challenge.\n",
            "\n",
            "The Hull midfielder was\n",
            "shown a straight red\n",
            "\n",
            "for the hefty challenge.\n",
            "\n",
            "The Hull midfielder was\n",
            "shown a straight red\n",
            "\n",
            "for the hefty challenge.\n",
            "\n",
            "The Swansea.\n",
            "\n",
            " \n",
            "\n",
            "After training\n",
            "\n",
            "[BLEU score: 0.904]\n",
            "David Meyler was\n",
            "sent off for the tackle\n",
            "on Kyle Naughton.\n",
            "The Hull midfielder\n",
            "was shown a straight\n",
            "red for the tackle.; Caption: The image displays a slide titled 'Evaluations' with a subtitle 'Model output examples on summarization task'. It contains a table with three columns labeled 'Question', 'Before training', and 'After training'. The table compares the model's performance on a summarization task before and after training, with scores indicated in brackets. The 'Question' column contains a text summarization task, and the 'Before training' and 'After training' columns show the model's output before and after training, respectively. The slide number '175' is visible at the bottom.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-4-multimodal-instruct.0af439b3adb8c23fda473c4f86001dbf9a226021.modeling_phi4mm:If you are not using the generate method, you may encounter nonsensical outputs after the 4096th token, as the KV cache needs to be recomputed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption for /content/drive/MyDrive/pages/page_461.png-- OCR: | SD @\n",
            "mm\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Reasoning about Logical Tracking Shuffled Object Tracking Shuffled Geometric\n",
            "Colored Objects Deduction (7) Objects (5) Counting Objects (3) Shapes\n",
            "RLFT 32.1% 45.7% 12.4% 42.6% 33.6% 18.9%\n",
            "DG 32.0% 35.2% 12.4% 31.9% 31.2% 5.2%\n",
            "sc 39.6% 27.6% 12.4% 24.0% 33.6% 15.6%\n",
            "Self-train 19.5% 13.1% 15.5% 11.7% 33.1% 12.4%\n",
            "Self-refine 25.2% 13.2% 8.0% 18.0% 25.2% 10.0%\n",
            "Best-of-N 26.8% 12.8% 12.1% 14.0% 30.0% 8.4%\n",
            "RLAIF 30.4% 36.9% 11.4% 32.5% 32.8% 14.0%\n",
            "RLC 35.0% 39.2% 12.2% 35.4% 33.6% 17.8%\n",
            ": Sports Logical Logical Penguins in :\n",
            "Web of Lies Understanding Deduction (3) Deduction (5) a Table Navigate\n",
            "RLFT 72.2% 68.8% 58.6% 41.9% 44.2% 55.6%\n",
            "DG 43.6% 53.2% 39.6% 28.4% 15.7% 46.4%\n",
            "sc 48.4% 53.6% 42.8% 30.8% 35.2% 62.8%\n",
            "Self-train 51.1% 51.1% 34.0% 18.4% 19.7% 48.7%\n",
            "Self-refine 47.2% 50.0% 28.4% 17.2% 17.8% 46.0%\n",
            "Best-of-N 50.0% 59.2% 42.0% 22.0% 17.8% 45.2%\n",
            "RLAIF 52.1% 56.1% 22.0% 33.7% 19.8% 48.8%\n",
            "RLC 52.9% 53.5% 44.0% 34.6% 29.8% 57.1%\n",
            "\n",
            "176\n",
            "\n",
            " \n",
            "\n",
            "Performance on BigBench-hard\n",
            "\n",
            "Performance on summarization task\n",
            "\n",
            "1.00\n",
            "Mm DG\n",
            "0.95 \"88 Self-train\n",
            "\n",
            "Mm RLC\n",
            "0.899\n",
            "\n",
            "0.90 0.886 0.886 i\n",
            "0.70 i f\n",
            "\n",
            "Summarization\n",
            "\n",
            "BERTScore\n",
            "coo «\n",
            "2a &\n",
            "\n",
            "So a\n",
            "\n",
            "oO\n",
            "N\n",
            "a; Caption: This image is a slide from a presentation, titled 'Evaluations'. It contains a table and a bar chart. The table is divided into two main sections. The first section lists various evaluation metrics such as 'Reasoning about Colored Objects', 'Logical Deduction', 'Tracking Shuffled Objects', 'Object Counting', 'Tracking Shuffled Geometric Shapes', and 'Performance on BigBench-hard'. Each metric has a corresponding percentage value for different methods like RLFT, DG, SC, Self-train, Self-refine, Best-of-N, RLAIF, and RLC. The second section of the table lists similar metrics but for different tasks like 'Web of Lies', 'Sports Understanding', 'Logical Deduction', 'Penguins in a Table', and 'Navigate'. The bar chart on the right side of the slide shows the performance of three methods (DG, Self-train, RLC) on a summarization task, with RLC having the highest performance score.\n",
            "Caption for /content/drive/MyDrive/pages/page_462.png-- OCR: | SD\n",
            "mm\n",
            "\n",
            "= Reinforcement learning is an Al area concerned with how an intelligent agent ought to take\n",
            "actions in a dynamic environment in order to maximize the cumulative reward.\n",
            "\n",
            "= Different from supervised learning in not needing labelled input/output pairs to be presented,\n",
            "reinforcement learning focuses on finding a balance between exploration (of uncharted\n",
            "territory) and exploitation (of current knowledge) with the goal of maximizing the long term\n",
            "reward.\n",
            "\n",
            "machine learning\n",
            "\n",
            "unsupervised supervised reinforcement\n",
            "learning learning learning\n",
            "\n",
            " \n",
            "\n",
            "177; Caption: The image is a slide from a presentation, with the title 'Remarks' at the top. It contains text and three diagrams representing different types of machine learning: unsupervised learning, supervised learning, and reinforcement learning. The text explains that reinforcement learning is an AI area focused on how an intelligent agent should take actions in a dynamic environment to maximize cumulative reward, and it differs from supervised learning in not needing labeled input/output pairs. The diagrams visually represent the concepts of unsupervised learning as clustering of data points, supervised learning as a decision boundary separating classes, and reinforcement learning as a state-action diagram with rewards.\n",
            "Caption for /content/drive/MyDrive/pages/page_463.png-- OCR: SINvIDIA. Q\n",
            "\n",
            "Home Al DataCenter Gaming ProGraphics Robotics Healthcare Startups AlPodcast NVIDIA Life\n",
            "\n",
            " \n",
            "\n",
            "What Is Agentic Al?\n",
            "\n",
            "Agentic Al uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\n",
            "\n",
            " \n",
            "\n",
            "2nd MEIS Workshop @CVPR2025\n",
            "\n",
            "a Multi-Agent Embodied Intelligent Systems Meet Generative-Al Era:\n",
            "User Opportunities, Challenges and Futures\n",
            "\n",
            "  \n",
            "   \n",
            "\n",
            "PCR Ree\n",
            "\n",
            "wesw |owards Agentic Al for Science: Hypothesis Generation,\n",
            "\n",
            " \n",
            "\n",
            "3 —_ Comprehension, Quantification, and Validation (ICLR,\n",
            "= 2025)\n",
            "woe Al Agent for Information Retrieval (Agent4IR)\n",
            "- & ‘ PP PUN PLE Co ae MTR Ao Xeon ese PAUed\n",
            "ng\n",
            "cm WMAC 2025: AAAI 2025 Workshop on Advancing\n",
            "\n",
            "403 vesoeniat LLM-Based Multi-Agent Collaboration; Caption: Page showing information about AI.\n",
            "Page 333 snippet:\n",
            " === OCR TEXT ===\n",
            "Values of States\n",
            "\n",
            "m Recursive definition of value (similar to expectimax):\n",
            "\n",
            "V*(s) = max Q*(s,a)\n",
            "\n",
            "sisa\n",
            "State\n",
            "\n",
            "i _ (s,a)isa\n",
            "Q*(s,a) = VIT(s,a, 8!) [RCs 4,8\") + 9V*(s')) g-state\n",
            "(s,a,s’))isa\n",
            "transition\n",
            "\n",
            "V*(s) = max) | T(s,4, s') [R(s, a, s')+ yV*(s')|\n",
            "\n",
            "=== Φ-4 CAPTION ===\n",
            "Values of States\n",
            "Page 445 snippet:\n",
            " === OCR TEXT ===\n",
            "\\ CD @\n",
            "\n",
            "m In the literature, it has been found that the geometric mean of the\n",
            "BLEU**, scores with n between one and four has the best\n",
            "correlation with human evaluation, therefore it’s\n",
            "Page 463 snippet:\n",
            " === OCR TEXT ===\n",
            "SINvIDIA. Q\n",
            "\n",
            "Home Al DataCenter Gaming ProGraphics Robotics Healthcare Startups AlPodcast NVIDIA Life\n",
            "\n",
            " \n",
            "\n",
            "What Is Agentic Al?\n",
            "\n",
            "Agentic Al uses sophisticated reasoning and iterative pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Build RAG Index with Sentence‐Transformers: Embeddings + FAISS index on the combined text**"
      ],
      "metadata": {
        "id": "KTsLIT70e-c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Use a small embedding model\n",
        "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Build embeddings\n",
        "page_ids = sorted(page_texts)\n",
        "embs = embedder.encode([page_texts[p] for p in page_ids], show_progress_bar=True)\n",
        "\n",
        "# FAISS index\n",
        "d = embs.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "faiss.normalize_L2(embs)\n",
        "index.add(embs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1311d7765b3b4e35b13a3060ff3e0899",
            "28a3539d510243a8a235dd0c7cbeea9a",
            "f83bd28976cb4446b7329aa9c073778e",
            "2866579d69a9419fa77a46bf86bcbfdf",
            "dbc0836c279a4d50bbd6953988085070",
            "97cb2d5cbf714395b88e5c48e4401283",
            "2b5977e1f9f148c6a6569cb957125747",
            "58fb68561984413db2ceaeccc9fa3128",
            "9204a336ca8f4b9a99ad74229617b067",
            "baacd1c1c7064663ab99b5b381959b65",
            "44f875c8cd2a43d7bfa406a6b68bb1eb"
          ]
        },
        "id": "LU3-A1F9fHUH",
        "outputId": "9cfea8c6-6167-41e6-b864-51f08a95675a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1311d7765b3b4e35b13a3060ff3e0899"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Load Queries & Retrieve Best Page:  Read queries, retrieve & fill submission.csv**"
      ],
      "metadata": {
        "id": "SIcv4muefCo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Read with ID as the index\n",
        "queries_df  = pd.read_csv(\n",
        "    QUERY_CSV,\n",
        "    index_col='ID'\n",
        ")\n",
        "template_df = pd.read_csv(\n",
        "    TEMPLATE,\n",
        "    index_col='ID'\n",
        ")\n",
        "\n",
        "# 2. Verify columns\n",
        "print(\"Queries columns:\", queries_df.columns.tolist())    # should show ['Question']\n",
        "print(\"Template columns:\", template_df.columns.tolist())  # whatever your template has\n",
        "\n",
        "# 3. Embed the 'Question' texts\n",
        "texts = queries_df['Question'].astype(str).tolist()\n",
        "q_embs = embedder.encode(texts, show_progress_bar=True)\n",
        "faiss.normalize_L2(q_embs)\n",
        "\n",
        "# 4. Retrieve top-1 page\n",
        "D, I = index.search(q_embs, 1)\n",
        "pred_pages = [ page_ids[idx] for idx in I.flatten() ]\n",
        "\n",
        "# 5. Fill your template (aligned by the same ID order)\n",
        "template_df['Answer'] = pred_pages\n",
        "\n",
        "# 6. Save\n",
        "template_df.to_csv('/content/drive/MyDrive/Hw2_task2/r13922a22.csv')\n",
        "print(\"✔️ submission.csv written with ID as index.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "cca516b24b5b4b92b53b251ca648171e",
            "11244acecb8547c4b998be89b8b43ea3",
            "bc6663412b5243a1a481682917b42cb0",
            "40dea74936784ec6aa545f97ad75bd4f",
            "c4a4bee89891423d851dbdcf427ad20f",
            "63b18a103d6d4cc5a342d73a744a6e4c",
            "15b49d63f6844421a880a68eb9410d42",
            "12b92bc5aa544bf296c0c901794b7ffc",
            "e3341a3c77184f8b93aae9d22456cbc4",
            "d046ece6ad6d411c9564504fa1f59dd1",
            "8f0eb3ed2b6e402c8fbb82dcbe631188"
          ]
        },
        "id": "oD-lOrcLGbfI",
        "outputId": "33985819-67c7-4e30-a448-4d2514e45c89"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries columns: ['Question']\n",
            "Template columns: ['Answer']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cca516b24b5b4b92b53b251ca648171e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ submission.csv written with ID as index.\n"
          ]
        }
      ]
    }
  ]
}